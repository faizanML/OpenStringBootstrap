{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4OXSRB1Aa0I"
      },
      "source": [
        "# With some null constraints and 3 derivatives (Final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0jdL25QAoXp",
        "outputId": "e8f39731-a1c0-4299-8d77-db300471ddbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.5000, -0.1000],\n",
              "        [-1.5000,  0.0818],\n",
              "        [-0.4000, -0.1000],\n",
              "        [-0.4000,  0.0818],\n",
              "        [ 0.7000, -0.1000],\n",
              "        [ 0.7000,  0.0818],\n",
              "        [ 1.8000, -0.1000],\n",
              "        [ 1.8000,  0.0818]], dtype=torch.float64)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#THIS SECTION IS JUST TO TEST THE EXPRESSIONS FOR THE OPEN STRING AMPLITUDE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.special import legendre\n",
        "from scipy import integrate\n",
        "from scipy.special import hyp2f1 as scipy_hyp2f1\n",
        "from scipy.special import gamma as scipy_gamma\n",
        "from scipy.special import zeta\n",
        "#This is all for to check formulaes using the Open String Amplitude\n",
        "#Some definitions\n",
        "d = 10;\n",
        "alpha = (d-3)/2;\n",
        "lm =146/10;\n",
        "dlm =10**(-1)\n",
        "lmp = lm + dlm\n",
        "lmm = lm - dlm\n",
        "ellmax = 20;\n",
        "w00 = -(np.pi)**2/6\n",
        "w10 = -zeta(3)\n",
        "w01 = 7*np.pi**4/360\n",
        "\n",
        "def hyp2f1(a, b, c, z):\n",
        "    if isinstance(z, torch.Tensor):\n",
        "        z_np = z.detach().numpy()\n",
        "        result = scipy_hyp2f1(a,b,c,z_np)\n",
        "        return torch.tensor(result, dtype=z.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_hyp2f1(a,b,c,z))\n",
        "\n",
        "def gamma(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x_np = x.detach().numpy()\n",
        "        result = scipy_gamma(x_np)\n",
        "        return torch.tensor(result, dtype=x.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_gamma(x))\n",
        "\n",
        "def ker(n,s1,s2):\n",
        "    return (1/(s1 - n) + 1/(s2 - n) + 1/(n + lm))\n",
        "\n",
        "def geg(ell,beta, z):\n",
        "    return gamma(ell + 2*beta) / (gamma(2*beta) * gamma(ell + 1)) * \\\n",
        "           hyp2f1(-ell, ell + 2*beta, beta + 1/2, (1-z)/2)\n",
        "\n",
        "#ell,n tensor and Null grid tensor\n",
        "ell = torch.arange(ellmax + 1, dtype = torch.float64).unsqueeze(1).repeat(1, ellmax + 1)\n",
        "n = torch.arange(1, ellmax + 2, dtype = torch.float64).unsqueeze(0).repeat(ellmax + 1, 1)\n",
        "mask = (n > ell) & (n <= ellmax + 1) & ((n - ell - 1) % 2 == 0)\n",
        "ell_n_tensor = torch.stack([ell[mask], n[mask]], dim=1)\n",
        "\n",
        "NullPoints = torch.stack(torch.meshgrid(\n",
        "    torch.arange(-2 +1/2, 2+1/2 + 1e-6, 11/10, dtype=torch.float64),\n",
        "    torch.arange(-0.1, 0.1 + 1e-6, 2/11, dtype=torch.float64),\n",
        "    indexing='ij'\n",
        ")).permute(1, 2, 0).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def w00comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    result = (-1/n)*geg(ell,alpha,1)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def w10comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-((2 * ell * (-3 + d + ell) * lm * (n + 2 * lm) * \\\n",
        "            hyp2f1(1 - ell, -2 + d + ell, d / 2, lm / (n + lm))) / ((-2 + d) * (n + lm)**2)) - \\\n",
        "            hyp2f1(-ell, -3 + d + ell, 1/2 * (-2 + d), lm / (n + lm)))) / (n**2 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def w01comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-(1/((d - 2) * (n + lm)**2)) *\\\n",
        "             2 * ell * (-3 + d + ell) * n * (n + 2*lm) *  hyp2f1(1 - ell, -2 + d + ell, d/2, lm/(n + lm)) + \\\n",
        "             2 * hyp2f1(-ell, -3 + d + ell, (d - 2)/2, lm/(n + lm)))) / (n**3 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcomp(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(n, s1, s2) * geg(ell, alpha, 1 + 2 * (((s1 + lm) * (s2 + lm)) / (n + lm) - lm) / n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1P(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmp)*(s2 + lmp))/(n + lmp)-lmp)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1M(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmm)*(s2 + lmm))/(n + lmm)-lmm)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD1(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ((2 * (-3 + d) * (n**2 + 2 * n * lm - s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) /  (n * (n + lm))) / n - (n + lm) * geg(ell, 1/2 * (-3 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) / (n * (n + lm)))) / (n + lm)**3)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD2(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = (1 / (n + lm)**4 * 2 * (-((2 * (-3 + d) * (-1 + d) * (n - s1) * (n - s2) *\n",
        "               (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "               geg(-2 + ell,(d+1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) /\n",
        "              (n**2 * (n + lm))) + ((-3 + d) * (n - s1) * (n - s2) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            ((-3 + d) * (2*n - s1 - s2) * (n + lm) *\n",
        "             geg(-1 + ell, (d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n -\n",
        "            (3 * (-3 + d) * (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            (n + lm) * geg(ell,(d-3)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))))\n",
        "\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD3(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result =  (2 / (n**3 * (n + lm)**7)) * (4 * (-3 + d) * (-1 + d) * (1 + d) * (n - s1)**2 * (n - s2)**2 * (n**2 + 2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-3 + ell, (3 + d)/2, 1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * (-1 + d) * n * (n - s1)**2 * (n -s2)**2 * (n + lm) * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     4 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (2 * n - s1 -s2) * (n + lm)**2 * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     8 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (n + lm) * (n**2 +2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (n - s1) * (n -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (2 * n - s1 - s2) * (n + lm)**3 * geg(-1 + ell, 1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     n**3 * (n + lm)**3 * geg(ell, 1/2 * (-3 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +\n",
        "     4 * n * (n + lm) * (2 * (-3 + d) * (-1 + d) * (n - s1) * (n -s2) * (n**2 + 2 * n * lm - s2 * lm -\n",
        "     s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))\n",
        "     - (-3 + d) * n * (n - s1) * (n -s2) * (n + lm) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) - (-3 + d) * n * (2 * n - s1 -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +3 * (-3 + d) * n * (n + lm) * (n**2 + 2 * n * lm -\n",
        "      s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell,1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "        n**2 * (n + lm)**2 * geg(ell, 1/2 * (-3 + d),  1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "#First, lets calculate cc(ell,n) for open string\n",
        "\n",
        "def AbsOpString(n,s2):\n",
        "    return (-1)**(1 + n)*gamma(-s2)/(gamma(n+1)*gamma(1 - n - s2))\n",
        "\n",
        "\n",
        "\n",
        "def cc(ell_n_tensor):\n",
        "    z = torch.linspace(-1 + 10**(-6), 1 - 10**(-6), 10**6).view(1, -1)\n",
        "\n",
        "    # Directly use ell and n from ell_n_tensor\n",
        "    ell_values = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n_values = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "\n",
        "    # Compute normfac for all combinations of ell and n\n",
        "    normfac = ((np.pi * 2**(1 - 2 * alpha) * gamma(2 * alpha + ell_values)) /\n",
        "                (gamma(ell_values + 1) * (ell_values + alpha) * gamma(alpha)**2))**-1\n",
        "\n",
        "    # Compute integrand\n",
        "    integrand = normfac * (1 - z**2)**(alpha - 1/2) * \\\n",
        "                AbsOpString(n_values, n_values * (z - 1) / 2) * \\\n",
        "                geg(ell_values, alpha, z)\n",
        "\n",
        "    # Integrate across z\n",
        "    return torch.trapz(integrand, z, dim=1)\n",
        "\n",
        "NullPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El3jwbdgAoX1",
        "outputId": "c1a816f1-a8dd-44c9-dde5-615879e048d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-740bb5865f5a>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(result, dtype=z.dtype)\n",
            "<ipython-input-2-740bb5865f5a>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(scipy_hyp2f1(a,b,c,z))\n"
          ]
        }
      ],
      "source": [
        "#Store all the values\n",
        "coeff_vals = cc(ell_n_tensor)\n",
        "w00comp_vals = w00comp(ell_n_tensor)\n",
        "w01comp_vals = w01comp(ell_n_tensor)\n",
        "w10comp_vals = w10comp(ell_n_tensor)\n",
        "\n",
        "ampcomp_vals = ampcomp(ell_n_tensor, NullPoints)\n",
        "ampcompD1P_vals = ampcompD1P(ell_n_tensor, NullPoints)\n",
        "ampcompD1M_vals = ampcompD1M(ell_n_tensor, NullPoints)\n",
        "ampcompD1_vals = ampcompD1(ell_n_tensor, NullPoints)\n",
        "ampcompD2_vals = ampcompD2(ell_n_tensor, NullPoints)\n",
        "ampcompD3_vals = ampcompD3(ell_n_tensor, NullPoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRfOjCPYAoX1",
        "outputId": "6b5543db-a556-4881-a9be-19aa0dd0f75d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-1.6449340668482264,\n",
              " -1.5984308179417641,\n",
              " -1.2020569031595942,\n",
              " -1.202056902362787,\n",
              " 1.8940656589944915,\n",
              " 1.894065658958795,\n",
              " -0.9368392530438512,\n",
              " 1.4470927928167399e-11,\n",
              " 7.384788822117905e-10,\n",
              " -3.6782704044614235e-10,\n",
              " -0.9366382712809055,\n",
              " -0.9370430272245681,\n",
              " 0.9995680497779686)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#CHECKS FOR OPEN STRING\n",
        "def w00fullX():\n",
        "    return torch.dot(coeff_vals,w00comp_vals)\n",
        "\n",
        "def w10fullX():\n",
        "    return torch.dot(coeff_vals,w10comp_vals)\n",
        "\n",
        "def w01fullX():\n",
        "    return torch.dot(coeff_vals,w01comp_vals)\n",
        "\n",
        "def ampfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1PfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1MfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD3_vals, dim = 0)\n",
        "\n",
        "(w00, w00fullX().item(),w10, w10fullX().item(),w01, w01fullX().item(),ampfullX()[1].item(),\n",
        " ampD1fullX()[1].item(),ampD2fullX()[1].item(),ampD3fullX()[1].item(),\n",
        " ampD1PfullX()[1].item(),ampD1MfullX()[1].item(),(ampD1PfullX()/ampD1MfullX())[1].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4oqzSsPAoX2"
      },
      "outputs": [],
      "source": [
        "# Same definitions for the PINN\n",
        "def w00full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w00comp_vals)\n",
        "\n",
        "def w01full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w01comp_vals)\n",
        "\n",
        "def w10full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w10comp_vals)\n",
        "\n",
        "def ampfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1Pfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1Mfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD3_vals, dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiddBNzuAoX2",
        "outputId": "4c953174-9034-42cc-e864-d4f7417716cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 3032880943.758614, Const: (1.5600017283286223, 6.706810379854307, 5.433765075937293, 4.5648051004082735, 3.45460394646817), W00: -114008.81565555999\n",
            "Epoch 5000, Loss_Tot: 9683.746061569747, Const: (2.0157613816795816e-05, 0.0006048677075822795, 0.007980506770757356, 0.018938274624205928, 0.01923553068864023), W00: -2585.867071870814\n",
            "Epoch 10000, Loss_Tot: 1005.5874288836735, Const: (-3.3224187916403025e-05, 0.00016980997199067538, 0.0020254186292869543, 0.00704965418344302, 0.0068216996199992564), W00: -499.0922174125252\n",
            "Epoch 15000, Loss_Tot: 22.623251916459118, Const: (0.00011848952604021079, 5.276037207369022e-05, 0.00033697343887477394, 0.0001229042439029586, 6.385920810902182e-05), W00: -11.23213518947776\n",
            "Epoch 20000, Loss_Tot: 8.441014955692493, Const: (-0.0014983734133482418, 0.0010571909200212115, 0.0001268891661327194, 5.1696699540969436e-05, 3.788216323509383e-05), W00: -3.464045773267852\n",
            "Epoch 25000, Loss_Tot: 2.1528507967245933, Const: (1.3487150787971913e-05, 3.827235226738779e-06, 2.7917761996888416e-05, 4.3300502197106805e-05, 1.8742968511063213e-05), W00: -2.0724878699081164\n",
            "Epoch 30000, Loss_Tot: 4.476135049465904, Const: (0.0013447384174762878, -0.0009659715899896781, 1.555542139282808e-05, 2.2486292725159695e-05, 9.400532360942625e-06), W00: -1.7099214085220504\n",
            "Epoch 35000, Loss_Tot: 2.683649860057608, Const: (0.0008270009508137832, -0.0006209197725124405, 8.58849486796301e-06, 1.459583009100285e-05, 4.261169397153061e-06), W00: -1.6065705032834263\n",
            "Epoch 40000, Loss_Tot: 1.5799080536022476, Const: (-9.468283279567835e-05, 7.223106040421534e-05, 4.734265002345856e-06, 1.0033102384804331e-05, 1.8682845912972034e-06), W00: -1.563380408546894\n",
            "Epoch 45000, Loss_Tot: 2.1996184611772245, Const: (-0.000654352295688776, 0.0004841483604065555, 4.616049030092079e-06, 6.780787990046681e-06, 1.5689929284712304e-06), W00: -1.5348626677303003\n",
            "Epoch 50000, Loss_Tot: 1.582062375031369, Const: (-0.00020452602630793848, 0.00015156113430458973, 5.136558505251242e-06, 5.5387955472879e-06, 1.9080939693835357e-06), W00: -1.5145879597558618\n",
            "Epoch 55000, Loss_Tot: 1.5058817885418743, Const: (-1.9073042589745626e-07, -8.838795497734253e-07, 5.045451256826669e-06, 4.511671673141235e-06, 2.1214038754220607e-06), W00: -1.50331045754653\n",
            "Epoch 60000, Loss_Tot: 1.4971114837712298, Const: (-8.841801948822336e-06, 5.716013816936538e-06, 4.874881793003005e-06, 3.966255856944938e-06, 2.281679761453451e-06), W00: -1.4946032489979402\n",
            "Epoch 65000, Loss_Tot: 1.4891964075744464, Const: (-1.3690606315286402e-07, -6.934132983360541e-07, 4.887362677276407e-06, 3.5074056272808794e-06, 2.644910946415825e-06), W00: -1.4867879791671017\n",
            "Epoch 70000, Loss_Tot: 1.482044501010897, Const: (-2.1521975090976753e-08, -6.647334209741729e-07, 4.981264538851806e-06, 4.036116527119542e-06, 3.1023392825238347e-06), W00: -1.479536844290934\n",
            "Epoch 75000, Loss_Tot: 1.4745018148141134, Const: (9.691956552870806e-07, -1.3932220064294398e-06, 5.2363939127464405e-06, 5.050605188357074e-06, 3.7997449607953824e-06), W00: -1.4717170056108553\n",
            "Epoch 80000, Loss_Tot: 1.6172926587511858, Const: (0.0003075457116388236, -0.00023674820251717854, 6.670716884942128e-06, 7.400796918041851e-06, 5.701014159575599e-06), W00: -1.4621214628752655\n",
            "Epoch 85000, Loss_Tot: 1.4628657931055133, Const: (-2.4779141034603214e-06, 6.935283347608845e-07, 7.14864158907086e-06, 9.373550421702843e-06, 7.0803412174157e-06), W00: -1.4576108697294954\n",
            "Epoch 90000, Loss_Tot: 1.4594740647166013, Const: (2.67430896316867e-07, -9.437484247420969e-07, 7.819062897271849e-06, 1.081158377422646e-05, 8.12081814811586e-06), W00: -1.4531764900459765\n",
            "Final values:\n",
            "-1.4531764900459765 2.67430896316867e-07 -9.437484247420969e-07 7.819062897271849e-06 1.081158377422646e-05 8.12081814811586e-06\n",
            "\n",
            "Minimum Cons: 0.000004419878552367862959 at epoch 59683\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4959695484\n",
            "W10 diff: 0.0000012536\n",
            "W01 diff: -0.0000012741\n",
            "Comp3: 0.0000042690726472\n",
            "Comp4: 0.0000038353693856\n",
            "Comp4: 0.0000021966492878\n",
            "Coeffs: tensor([9.970221656970217338766815e-01, 4.975818980467485580510001e-02,\n",
            "        1.705843914057409912987673e-03, 6.046082199688397829312816e-05,\n",
            "        2.134781542846427362258464e-06, 7.646712491089148136049917e-08,\n",
            "        2.745698109394550731346833e-09, 9.866528814297416302144238e-11,\n",
            "        3.555298072975796656162472e-12, 1.284378866048727386005801e-13,\n",
            "        4.639917772546054365278491e-15, 6.930184938905854796775685e-02,\n",
            "        5.573188864055051219836834e-03, 1.559326539007061082804662e-04,\n",
            "        4.922309574170027119570988e-06, 1.813263360436173989307583e-07,\n",
            "        6.585449512774530792690599e-09, 2.383367958699617402681438e-10,\n",
            "        8.612845810512093546505194e-12, 3.095642020727557794207098e-13,\n",
            "        1.108564962561892750316439e-14, 1.143942698678337456519305e-02,\n",
            "        8.213375400389521423585859e-04, 2.495974138459090146697356e-05,\n",
            "        7.012794039311629018350466e-07, 1.867140784384353363399879e-08,\n",
            "        5.067558947353105067907780e-10, 1.997484091788689808084248e-11,\n",
            "        7.013799958023858367094697e-13, 2.494690787810529348212089e-14,\n",
            "        8.985358573290015395870715e-16, 2.635514918068754090796357e-03,\n",
            "        2.129750357496587261953158e-04, 7.344763292730874028075173e-06,\n",
            "        2.176548331503225952112838e-07, 5.638668659417198480249207e-09,\n",
            "        1.372066118835454738714500e-10, 3.349186673055001807365470e-12,\n",
            "        8.618784916453572204272209e-14, 2.447215285090221643211682e-15,\n",
            "        7.111775645558108573107692e-04, 5.846315595769616514990052e-05,\n",
            "        2.281708934236107587698690e-06, 7.219069370207199248213687e-08,\n",
            "        2.089719082990341597339894e-09, 5.306056366147117546780218e-11,\n",
            "        1.277015080737636667722940e-12, 3.073407826563088891519653e-14,\n",
            "        7.513606147501584163549296e-16, 2.018823079457286934820098e-04,\n",
            "        1.562378195044070719943105e-05, 7.333075844654462099417064e-07,\n",
            "        2.194325570440498868693198e-08, 6.823431883737413951810608e-10,\n",
            "        2.023606675185077013853641e-11, 5.125376972068886900869793e-13,\n",
            "        1.233530749765134821261729e-14, 5.937402606827771534480850e-05,\n",
            "        4.198781565476647754215581e-06, 2.344395294159612960640751e-07,\n",
            "        7.140497985761317101972168e-09, 2.090838865544903624997804e-10,\n",
            "        6.445914593141157882391669e-12, 1.960022453596248152681526e-13,\n",
            "        4.950849989595156954112386e-15, 9.507406338313218949198635e-06,\n",
            "        1.095611640826197864006836e-06, 7.174796192333608963662128e-08,\n",
            "        2.336793962066124716984742e-09, 6.769991974430470965057919e-11,\n",
            "        1.986563003585275509400019e-12, 6.090348409940489459392741e-14,\n",
            "        1.787664719030387598241050e-06, 2.716940437539863462167514e-07,\n",
            "        2.006739782227637892643416e-08, 7.468958568896552501278402e-10,\n",
            "        2.215542440176361958167340e-11, 6.430782022656743481061180e-13,\n",
            "        1.880038721825062748147825e-14, 3.364116743684817953427937e-07,\n",
            "        6.746455225832716037272647e-08, 5.258687454032466379221489e-09,\n",
            "        2.054961934319247200854723e-10, 7.068601301510753234004215e-12,\n",
            "        2.100582412604136102784835e-13, 6.330759161906927492838651e-08,\n",
            "        1.690076980596901769227431e-08, 1.378957490415091379325792e-09,\n",
            "        5.586581685658536958267293e-11, 2.063890462563551409966095e-12,\n",
            "        6.478168837818062645456240e-14, 1.191353040836080334989708e-08,\n",
            "        3.876123464761750144898049e-09, 3.436179300960307372762234e-10,\n",
            "        1.514859314965373886143190e-11, 5.637527882885742290003263e-13,\n",
            "        2.241946032066455568499367e-09, 8.330075251839839291844373e-10,\n",
            "        8.437779303774848680494607e-11, 4.083338529494701744554056e-12,\n",
            "        1.533727080414288279733991e-13, 3.651133667419510607087144e-10,\n",
            "        1.778091624953211520832813e-10, 2.071955894512007666992749e-11,\n",
            "        1.076298579825919876734629e-12, 5.716521788107702557832036e-11,\n",
            "        3.576506448739213940619876e-11, 5.087972482596659121579140e-12,\n",
            "        2.822071611233712732189211e-13, 8.965445597719665217751002e-12,\n",
            "        6.847280576522517813773184e-12, 1.227269186077375377873220e-12,\n",
            "        1.404786644696130634774630e-12, 1.306358470807145297030220e-12,\n",
            "        2.611637677420564669756843e-13, 2.201173680064429558435581e-13,\n",
            "        2.474848437509811064903913e-13, 3.450029003399910626325401e-14,\n",
            "        3.971289254082987717885631e-14, 5.407433421587804010472401e-15,\n",
            "        8.475388519948375089034907e-16], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network. Now lets impose null constraints as well\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 90000\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**3\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VlCJ16Rzx-Y",
        "outputId": "b6af2cbe-88e0-4b76-bfb0-64fb7bc74b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 6672447252.684245, Const: (1.7070715852463794, 10.291343524741169, 8.056814229510731, 6.796830708592248, 5.10366399717221), W00: -151560.98090172617\n",
            "Epoch 5000, Loss_Tot: 11745.056475890831, Const: (0.002379821309393959, 0.004921376177963177, 0.009183834000379861, 0.022072911877896194, 0.021711960580590453), W00: -2322.2696172653627\n",
            "Epoch 10000, Loss_Tot: 6505.084722087959, Const: (0.0005555528903644635, 0.000942948735810134, 0.006325100666942398, 0.020087877551203278, 0.017871993292239978), W00: -1780.2661173100755\n",
            "Epoch 15000, Loss_Tot: 975.9246002206447, Const: (0.0001304430495527864, 0.0004791198738032243, 0.001415742000931321, 0.007420508013487918, 0.006045162020426481), W00: -683.6375646264478\n",
            "Epoch 20000, Loss_Tot: 81.14986055528874, Const: (-3.863445436014246e-06, -2.1447005924324358e-05, 0.000481959315125809, 0.0007328225744465594, 0.00015894405288302354), W00: -57.35861537379573\n",
            "Epoch 25000, Loss_Tot: 15.150938415522852, Const: (-0.000888815245845942, 0.0006510316306909125, 0.00019671733666419788, 0.00028601969871939456, 0.00012607596488095656), W00: -9.969630218712867\n",
            "Epoch 30000, Loss_Tot: 4.820169516008471, Const: (9.027155995156022e-05, -8.33062103062776e-05, 9.499711517371526e-05, 0.00016890710233301195, 8.400559683987575e-05), W00: -3.8670488981450184\n",
            "Epoch 35000, Loss_Tot: 3.669432415569418, Const: (-0.0008788264538037716, 0.0007007324914487523, 4.4223033217943804e-05, 7.724043787393931e-05, 4.3451402397601386e-05), W00: -2.2026486787793176\n",
            "Epoch 40000, Loss_Tot: 1.7376532776010272, Const: (-4.6765798271897197e-05, 3.901630839298065e-05, 1.5899355171932597e-05, 2.903287922007924e-05, 1.814306734888277e-05), W00: -1.7074929369354435\n",
            "Epoch 45000, Loss_Tot: 3.8849019567312935, Const: (-0.0012142687070100244, 0.0008970064534181699, 7.440674349714396e-06, 1.5089114633463541e-05, 9.489970343756668e-06), W00: -1.5999787820382565\n",
            "Epoch 50000, Loss_Tot: 1.5455936741343772, Const: (-2.3571575514580445e-05, 1.8508211497403337e-05, 3.1578283888860093e-06, 5.885446798845334e-06, 4.245946893901322e-06), W00: -1.5436456465068917\n",
            "Epoch 55000, Loss_Tot: 1.5050028941512201, Const: (5.063564425533684e-07, 8.788198857523355e-08, 2.729776278003162e-06, 2.3894368470479983e-06, 2.7946737310836707e-07), W00: -1.5042516746678758\n",
            "Epoch 60000, Loss_Tot: 1.4868240418314287, Const: (-6.180113024889522e-06, 3.7189749899635416e-06, 4.081254404616273e-06, 4.214914652387255e-06, 1.3746128506804495e-06), W00: -1.4850866984419162\n",
            "Epoch 65000, Loss_Tot: 1.4952231426251703, Const: (-0.00010600527510540303, 7.558524654238674e-05, 5.619209019087121e-06, 4.43769681394205e-06, 2.0504017017144367e-06), W00: -1.4750914464799547\n",
            "Epoch 70000, Loss_Tot: 1.473967465925993, Const: (-2.0412000334424363e-07, -8.16077607490584e-07, 6.458580667503671e-06, 4.671605255762259e-06, 2.442492961100769e-06), W00: -1.4697676421869599\n",
            "Epoch 75000, Loss_Tot: 1.47099262349468, Const: (5.6248072026754414e-06, -4.834917721519005e-06, 6.607260222624302e-06, 5.301992353597611e-06, 2.839478542436123e-06), W00: -1.466535846082981\n",
            "Epoch 80000, Loss_Tot: 1.4906024457296574, Const: (-0.00012038373250544687, 8.70075162817674e-05, 6.656697609286915e-06, 5.935755877991122e-06, 3.1396089533594407e-06), W00: -1.4640636421398208\n",
            "Epoch 85000, Loss_Tot: 1.6412663876962896, Const: (-0.00033808420405234685, 0.0002466262044518164, 6.7635580909365035e-06, 6.507043756332209e-06, 3.485491275279911e-06), W00: -1.4615119118714475\n",
            "Epoch 90000, Loss_Tot: 1.5734369933688683, Const: (0.000267319769060137, -0.0001973554558223345, 6.878995641104095e-06, 6.95840893128015e-06, 3.805936158486876e-06), W00: -1.4582329957877316\n",
            "Final values:\n",
            "-1.4582329957877316 0.000267319769060137 -0.0001973554558223345 6.878995641104095e-06 6.95840893128015e-06 3.805936158486876e-06\n",
            "\n",
            "Minimum Cons: 0.000002285402612870635853 at epoch 53847\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5118181851\n",
            "W10 diff: 0.0000006890\n",
            "W01 diff: 0.0000004421\n",
            "Comp3: 0.0000022847899544\n",
            "Comp4: 0.0000020846321734\n",
            "Comp4: 0.0000009366823393\n",
            "Coeffs: tensor([9.947324289863284096924190e-01, 8.345246675835316096936367e-02,\n",
            "        7.877427469442891891371339e-03, 7.889330637111634236813273e-04,\n",
            "        7.873349898020698628495900e-05, 8.042867752493001570193373e-06,\n",
            "        9.309631089096985269109685e-07, 1.050061066089072181159683e-07,\n",
            "        1.171019153826614124053565e-08, 1.351221364675535917050443e-09,\n",
            "        1.565711132922031263643531e-10, 6.730137483984342805509726e-02,\n",
            "        1.009874821285090991385136e-02, 7.443839446913952514681134e-04,\n",
            "        5.954346160481172278667802e-05, 5.356625480855487803500802e-06,\n",
            "        5.175672503720857169060596e-07, 5.518585708401655553110321e-08,\n",
            "        5.860394928154166009962887e-09, 6.223374931836991978213285e-10,\n",
            "        6.638613018822157986393093e-11, 1.093872487009967843507585e-02,\n",
            "        1.854028025754345023606851e-03, 2.662705144629794163334269e-04,\n",
            "        2.224281081241414546314714e-05, 1.878647382989866667269037e-06,\n",
            "        1.670255633767660576590797e-07, 1.477385814798573655020715e-08,\n",
            "        1.306787206808618054308120e-09, 1.155888175107325432290363e-10,\n",
            "        1.012415579646379111555952e-11, 2.551201226647708316525032e-03,\n",
            "        4.096552003671665776775301e-04, 7.107825079480687780267290e-05,\n",
            "        8.678993854633743466973211e-06, 7.346090827195654374194588e-07,\n",
            "        6.260733245704836329694533e-08, 5.558584981576168570719967e-09,\n",
            "        4.952453975934763123791714e-10, 4.412417982001571679778711e-11,\n",
            "        6.857405104403736751869647e-04, 1.068949766144064884160203e-04,\n",
            "        1.765923135635397091432186e-05, 2.790148188441154318361432e-06,\n",
            "        2.972693604046953176611983e-07, 2.496020574904208687428977e-08,\n",
            "        2.133769478196818564764776e-09, 1.878159090231732203085629e-10,\n",
            "        1.673356881640419866178281e-11, 1.871318385185649884727005e-04,\n",
            "        3.023069220754444767274884e-05, 5.122576164583362837081238e-06,\n",
            "        7.937229384967162202418000e-07, 1.028465660777488316429687e-07,\n",
            "        1.129319727752560430469258e-08, 8.709761273735808405656979e-10,\n",
            "        7.285499347190080660701132e-11, 4.753111910687956084961212e-05,\n",
            "        7.457854494593729105316685e-06, 1.326574109250276136293622e-06,\n",
            "        2.010144396990101613701850e-07, 2.791057591671842501920565e-08,\n",
            "        3.579048878198355892471744e-09, 4.083581412863346230151381e-10,\n",
            "        3.213557445631447819066437e-11, 9.930863893790508177441927e-06,\n",
            "        1.657884788022522735929584e-06, 2.752194867975451403330212e-07,\n",
            "        4.658202470823669338592708e-08, 7.080654885640773903646240e-09,\n",
            "        9.486584207140470194544344e-10, 1.218125743570115822417680e-10,\n",
            "        2.027052487121959779034323e-06, 3.502555666833150017397305e-07,\n",
            "        5.781978231109746769039715e-08, 9.727088849145702138698330e-09,\n",
            "        1.583340619639522035613167e-09, 2.384214350049918510927424e-10,\n",
            "        3.232184431526890712681914e-11, 4.143851281645334523753106e-07,\n",
            "        7.441791099805827604704816e-08, 1.219857337531919724326347e-08,\n",
            "        2.031982269356863586654533e-09, 3.437101597980550559696447e-10,\n",
            "        5.259300125027682931695371e-11, 8.473316793554089435948980e-08,\n",
            "        1.580956162111625327802424e-08, 2.576198279192380601956948e-09,\n",
            "        4.264661574135509117030406e-10, 7.162488094835041665967370e-11,\n",
            "        1.162525284619568569183410e-11, 1.732910603596512802967850e-08,\n",
            "        3.332913746420087397166387e-09, 5.441227801884263211967661e-10,\n",
            "        8.980673440438217011889118e-11, 1.500285497942025374760607e-11,\n",
            "        3.544042023583189247711036e-09, 7.033116664496519086162599e-10,\n",
            "        1.152215986012274739236099e-10, 1.895336140472976858942662e-11,\n",
            "        3.152748731973257309097833e-12, 7.248056402903401035609504e-10,\n",
            "        1.479763260163183100687324e-10, 2.450012660570824233127469e-11,\n",
            "        4.003168457316658198162915e-12, 1.482327839545377329588025e-10,\n",
            "        3.087886659628409075326714e-11, 5.206249321084364844229394e-12,\n",
            "        8.457608586559836672815104e-13, 3.031565569091272428355484e-11,\n",
            "        6.452770856873852578415781e-12, 1.106200004598472389012272e-12,\n",
            "        6.199971122506410455226005e-12, 1.348438473319115666634994e-12,\n",
            "        2.350403091945700202444768e-13, 1.267979894990938490035319e-12,\n",
            "        2.796999895190031817864037e-13, 2.593194359021635522813514e-13,\n",
            "        5.707737631468538772805160e-14, 5.303441332330385327667859e-14,\n",
            "        1.084627145960617378030138e-14], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network. Now lets impose null constraints as well\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 90000\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**3\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjoeV35j_88J",
        "outputId": "14e07076-9828-43e4-91d6-3bb4c737a7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 768551637.4797472, Const: (1.3272933235919409, 3.0856182041727465, 2.7367375368724276, 2.2692530229285723, 1.7560684329366525), W00: -62370.015090035384\n",
            "Epoch 5000, Loss_Tot: 13032.274312853548, Const: (-6.134538322566385e-05, -6.321023002464621e-05, 0.0092026657463705, 0.027314109887890337, 0.027146578657429383), W00: -3080.363538350486\n",
            "Epoch 10000, Loss_Tot: 4782.507036818766, Const: (0.0004655979108625363, -0.001670882107531968, 0.005152612934648971, 0.014656881454170418, 0.012724919334826737), W00: -1747.808656916256\n",
            "Epoch 15000, Loss_Tot: 407.4519211868233, Const: (4.9221057696557935e-05, 0.00016003455790425392, 0.0009937696797547885, 0.002303515532016292, 0.0010399975133889806), W00: -302.27829114048984\n",
            "Epoch 20000, Loss_Tot: 32.1156297809065, Const: (-0.00022413371065832521, 8.837252845306232e-05, 0.0003324835672660296, 0.0007896645093842569, 0.0003547205518148531), W00: -20.25365519941249\n",
            "Epoch 25000, Loss_Tot: 12.271117791884098, Const: (-0.0008335482401777838, 0.0005195098460462066, 0.00017278917618791264, 0.0005435425300100588, 0.00023956005720249918), W00: -7.967987199335313\n",
            "Epoch 30000, Loss_Tot: 5.476724767813909, Const: (0.00015568240958097057, -0.00011615939928621977, 8.647607739246812e-05, 0.00040403099637612326, 0.00016553165518649554), W00: -4.500541778098159\n",
            "Epoch 35000, Loss_Tot: 3.4598007638998336, Const: (-2.8037127785651705e-05, 1.2172427433787547e-05, 4.115596097576747e-05, 0.0003280859063759159, 0.00013239335324447603), W00: -3.164316841046021\n",
            "Epoch 40000, Loss_Tot: 5.046094784427005, Const: (0.0012635344447267105, -0.0008649860999634384, 2.3479463350389693e-05, 0.0002672035618408902, 0.000110059975889973), W00: -2.562735076610619\n",
            "Epoch 45000, Loss_Tot: 2.3042726049192317, Const: (4.051069731225709e-06, -3.561435493848464e-06, 1.1496161221103375e-05, 0.0002182385374820025, 9.38663798521002e-05), W00: -2.2345883811395053\n",
            "Epoch 50000, Loss_Tot: 2.054806008577921, Const: (6.738639348746744e-07, 3.099907464054752e-08, 6.083107227133831e-06, 0.00017810377532325648, 8.098100133433958e-05), W00: -2.0128262568091855\n",
            "Epoch 55000, Loss_Tot: 1.9009597221430257, Const: (5.734469359364169e-07, -2.594554868728949e-07, 3.6110903500529243e-06, 0.00014372493529399638, 6.603135572477072e-05), W00: -1.8746383316687485\n",
            "Epoch 60000, Loss_Tot: 1.794251957833723, Const: (-1.4103573484414156e-05, 1.0450876754219962e-05, 2.4609812087399534e-06, 0.00011844704259435687, 5.3402152503647285e-05), W00: -1.7764566915814115\n",
            "Epoch 65000, Loss_Tot: 1.7376003729498553, Const: (-9.713862745353019e-05, 6.639240553951709e-05, 2.0465404554963873e-06, 9.873559036186513e-05, 4.517576064246107e-05), W00: -1.7115481095556464\n",
            "Epoch 70000, Loss_Tot: 1.6570153527250333, Const: (-9.286236566374839e-07, 1.7275813808392826e-06, 3.0190808103813055e-06, 8.43824578181711e-05, 3.744125157126912e-05), W00: -1.6475777744450903\n",
            "Epoch 75000, Loss_Tot: 1.6114563108459967, Const: (-1.2184385500546568e-08, 1.4504502112711037e-06, 3.942735377632102e-06, 7.20544631807127e-05, 3.2463544899769455e-05), W00: -1.6036539632541988\n",
            "Epoch 80000, Loss_Tot: 1.8275776167981745, Const: (0.0004135225501811224, -0.00027934726220557593, 4.2813571327807985e-06, 6.198544755399236e-05, 2.730048935934345e-05), W00: -1.5721213100705551\n",
            "Epoch 85000, Loss_Tot: 2.3419408002176683, Const: (0.0007395651759338762, -0.0004885381163732827, 5.0309465132619806e-06, 5.451356312090295e-05, 2.4221138007297678e-05), W00: -1.5502252252415722\n",
            "Epoch 90000, Loss_Tot: 1.5364226956438019, Const: (-5.683045908089213e-06, 5.319492704547457e-06, 4.983338923306684e-06, 4.736000087971998e-05, 2.1126251212000092e-05), W00: -1.5311894467743206\n",
            "Final values:\n",
            "-1.5311894467743206 -5.683045908089213e-06 5.319492704547457e-06 4.983338923306684e-06 4.736000087971998e-05 2.1126251212000092e-05\n",
            "\n",
            "Minimum Cons: 0.000051858570541601001127 at epoch 89999\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5311739895\n",
            "W10 diff: 0.0000062286\n",
            "W01 diff: -0.0000030309\n",
            "Comp3: 0.0000049949470393\n",
            "Comp4: 0.0000473592149168\n",
            "Comp4: 0.0000211285612638\n",
            "Coeffs: tensor([9.964381031365342478167690e-01, 7.440151741783650185535492e-02,\n",
            "        3.676583451386498252455581e-03, 3.261025936182812556829447e-04,\n",
            "        2.948964036969817782110788e-05, 2.729833607720805703630504e-06,\n",
            "        2.704325528896619579962356e-07, 2.512189655864167249711799e-08,\n",
            "        2.470376105949652790719584e-09, 2.596951214359258500844515e-10,\n",
            "        4.182272028780455382559701e-11, 7.045815438647663742077754e-02,\n",
            "        2.512724300959873376115628e-03, 1.983281620552967560063007e-04,\n",
            "        1.810847989782483788380116e-05, 1.698627632967737256851435e-06,\n",
            "        1.773816519800691572751264e-07, 3.062482484521832615427189e-08,\n",
            "        4.820887845689906373033280e-09, 7.737496855969044733524807e-10,\n",
            "        1.261288372692734254048857e-10, 1.177483416007150257565073e-02,\n",
            "        8.253390674799202181635693e-04, 7.921593306730680182723564e-05,\n",
            "        1.060678581507659182204487e-05, 1.877722929385792689058903e-06,\n",
            "        3.303277525121506020965468e-07, 5.063178472785464758017376e-08,\n",
            "        7.575792654294813115531341e-09, 1.133529730359557783027373e-09,\n",
            "        1.787953216867403623467103e-10, 2.422552776351054315551181e-03,\n",
            "        2.862485401436033619418375e-04, 3.194045987677803866491844e-04,\n",
            "        4.156203493604557208662698e-05, 5.565592510881886509105810e-06,\n",
            "        9.671736702008793918288300e-07, 1.419341322100935338341657e-07,\n",
            "        2.082903144949114002690539e-08, 3.056689197678766459613033e-09,\n",
            "        4.297398953475909354204898e-04, 1.270274888905538967186215e-04,\n",
            "        2.193056933234442174886214e-04, 3.367857858369313161887859e-04,\n",
            "        4.238875346880398585535168e-05, 3.197453401191770519100847e-06,\n",
            "        4.271768376513324485633865e-07, 6.268880310998808826101353e-08,\n",
            "        9.199668797294121782296847e-09, 4.880003473105275088811741e-05,\n",
            "        2.668575808848438845168397e-05, 5.073872669722321080516478e-05,\n",
            "        2.484500231010702050310490e-04, 3.524405350507532489259810e-04,\n",
            "        4.197415093119679578136799e-05, 2.332839242949288220457717e-06,\n",
            "        1.841275881824383307631130e-07, 5.063512004710797670067619e-06,\n",
            "        3.699132577889248422547920e-06, 5.105232258728728458180290e-06,\n",
            "        2.040137973024830345320814e-05, 7.771509459684867548827369e-05,\n",
            "        9.919871417807072115548767e-05, 1.970736403869617359976493e-05,\n",
            "        1.086982645933754267357409e-06, 5.217926423429516934396756e-07,\n",
            "        5.325737673987008601340638e-07, 4.758253731465641661497752e-07,\n",
            "        1.652168909246863322086429e-06, 6.779583350144155920826740e-06,\n",
            "        1.814319035804291932630072e-05, 2.270859874937677312352682e-05,\n",
            "        5.384741344991370120718491e-08, 8.003941305668889317434669e-08,\n",
            "        4.421706713556597844366124e-08, 1.520320993516566960231692e-07,\n",
            "        5.094483089221765775950440e-07, 1.916100696253029224457379e-06,\n",
            "        4.207733794941142487281132e-06, 5.531854008134682030796406e-09,\n",
            "        1.206977008680508557543984e-08, 4.283170590457088579114543e-09,\n",
            "        1.248197538772600717260317e-08, 4.040343517043367404985654e-08,\n",
            "        1.534902373789417546340997e-07, 5.692978641132087101351831e-10,\n",
            "        1.728815946542435894123084e-09, 6.009923789124440894618225e-10,\n",
            "        1.077037953612214606398522e-09, 3.542416664846364629441399e-09,\n",
            "        1.200364540492442186091297e-08, 5.860983068120582406366966e-11,\n",
            "        1.946007649204929255958978e-10, 8.717829304721943060519114e-11,\n",
            "        9.519887548365679021743518e-11, 2.898088286387910818795874e-10,\n",
            "        6.033945440922486384934855e-12, 1.995903932470768066884740e-11,\n",
            "        1.263968084337015486602403e-11, 8.501850663758025133578730e-12,\n",
            "        2.445698379886719476263410e-11, 6.212012073743356633380886e-13,\n",
            "        2.016473036852781583241352e-12, 1.835525151404088181130431e-12,\n",
            "        7.863776721819575129988825e-13, 6.395333597570650493641302e-14,\n",
            "        2.022554900036601178329756e-13, 2.673318324689502232512607e-13,\n",
            "        8.805885983841210119144592e-14, 6.584065088521140419474800e-15,\n",
            "        2.022028213448431382460764e-14, 3.908320356703698624946127e-14,\n",
            "        6.681133296686058600356948e-16, 2.021501664012759813388467e-15,\n",
            "        5.217258494926379726597662e-15, 6.772770651713046398735657e-17,\n",
            "        2.029080180911615546682753e-16, 6.865664890035543635817519e-18,\n",
            "        2.045167624403344900102592e-17, 6.959833250863773876981367e-19,\n",
            "        7.055293209858111942332299e-20], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network. Now lets impose null constraints as well\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 90000\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**3\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGqzi-N0BS7N",
        "outputId": "8dd0c0b6-e24e-473c-fad5-e18f83f96433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 68233129.22517483, Const: (1.1674487328320668, 0.5917249049853968, 0.8110668219121712, 0.6580096732159172, 0.5299065671685811), W00: -23337.65755859312\n",
            "Epoch 5000, Loss_Tot: 5442.365726379367, Const: (0.0002938426462983923, -9.49816292672434e-05, 0.005764585489332381, 0.020506633165800307, 0.013287399366147293), W00: -1522.1487892743478\n",
            "Epoch 10000, Loss_Tot: 385.9998899716486, Const: (7.941014181378492e-05, 0.00014836161681475168, 0.0009059989199992557, 0.01118867514337525, 0.004244600715673072), W00: -160.68508182822453\n",
            "Epoch 15000, Loss_Tot: 19.664419031718353, Const: (-2.9529890634316658e-05, 4.171397195662507e-05, 0.00020253505386771172, 0.0019594665740422837, 0.0006666271465410856), W00: -11.275861150006877\n",
            "Epoch 20000, Loss_Tot: 3.8479521883993355, Const: (-5.899845013002114e-05, 5.776731337947538e-05, 7.22768648134789e-05, 0.00031928309370844894, 5.3520328623472234e-05), W00: -3.213933670555915\n",
            "Epoch 25000, Loss_Tot: 5.084232279753786, Const: (-0.0012264415041449706, 0.0012579378598325253, 2.0940103518633962e-05, 6.391801019179135e-05, 1.29410937366854e-05), W00: -1.9495640799932843\n",
            "Epoch 30000, Loss_Tot: 2.548831940638833, Const: (-0.0006731812941505044, 0.000609212730694253, 8.134309723585563e-06, 2.9290098732419822e-05, 1.4300258181892117e-05), W00: -1.7168396278689908\n",
            "Epoch 35000, Loss_Tot: 1.6466635298439105, Const: (-2.2797050311940126e-05, 1.7609046061961564e-05, 5.526952479252621e-06, 2.824429456328027e-05, 1.6964430510886653e-05), W00: -1.6416934933890457\n",
            "Epoch 40000, Loss_Tot: 1.6001415716535936, Const: (1.0628020719050113e-06, -4.636073569486143e-06, 4.412039703116675e-06, 2.791842565727228e-05, 1.6614337252453707e-05), W00: -1.597116864799501\n",
            "Epoch 45000, Loss_Tot: 1.895583200076106, Const: (0.0004162083935586036, -0.0003924596534432556, 3.83425647505368e-06, 2.5559019994257694e-05, 1.4639534319288157e-05), W00: -1.5659914618868704\n",
            "Epoch 50000, Loss_Tot: 1.5378013373682504, Const: (-1.4566589329634283e-05, 1.2128437193892339e-05, 2.857525294789652e-06, 3.2777595939812466e-05, 1.492875113204454e-05), W00: -1.5353282693677937\n",
            "Epoch 55000, Loss_Tot: 1.5156802023709532, Const: (-7.801316814770587e-07, -1.5862347730877246e-06, 2.430712234543341e-06, 3.5074437822276734e-05, 1.338454969146715e-05), W00: -1.5136768790690494\n",
            "Epoch 60000, Loss_Tot: 1.5484947395040656, Const: (0.00016618225009601417, -0.00015740935827368574, 3.7692138784322693e-06, 3.145803968759271e-05, 1.0624340239696386e-05), W00: -1.493577310992306\n",
            "Epoch 65000, Loss_Tot: 5.5598616479836265, Const: (0.0014589439913959001, -0.0013939880140363403, 8.745704486583932e-06, 2.5705170796894604e-05, 7.814401731280784e-06), W00: -1.4797709392996652\n",
            "Epoch 70000, Loss_Tot: 1.4751475112752055, Const: (-4.2796069643635803e-07, -1.907670111434001e-06, 5.9805133171538446e-06, 2.388874813036076e-05, 6.283784217270987e-06), W00: -1.4709568767346033\n",
            "Epoch 75000, Loss_Tot: 1.4710729613268887, Const: (-4.2279228673614e-07, -1.7797205267289229e-06, 6.296126774064495e-06, 2.163361275102355e-05, 5.2978011505100736e-06), W00: -1.4666094140352168\n",
            "Epoch 80000, Loss_Tot: 3.3497803296471678, Const: (-0.0009923242374425545, 0.0009445424332978902, 8.689386181969818e-06, 2.1820426055030487e-05, 4.951350255490667e-06), W00: -1.4648613390464045\n",
            "Epoch 85000, Loss_Tot: 1.4650687626736731, Const: (-4.586831185982021e-07, -1.2759909759907373e-06, 6.684099325492872e-06, 2.064758959640792e-05, 4.32943417248856e-06), W00: -1.460154138794798\n",
            "Epoch 90000, Loss_Tot: 1.4630881321300702, Const: (-1.2467294433937681e-05, 1.0115552441458675e-05, 6.63730039223272e-06, 2.0013533783027585e-05, 3.9515430365205475e-06), W00: -1.4580088424218445\n",
            "Final values:\n",
            "-1.4580088424218445 -1.2467294433937681e-05 1.0115552441458675e-05 6.63730039223272e-06 2.0013533783027585e-05 3.9515430365205475e-06\n",
            "\n",
            "Minimum Cons: 0.000020235355592859745327 at epoch 85559\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4604879195\n",
            "W10 diff: 0.0000033676\n",
            "W01 diff: -0.0000074755\n",
            "Comp3: 0.0000065669608366\n",
            "Comp4: 0.0000197875246899\n",
            "Comp4: 0.0000042336134230\n",
            "Coeffs: tensor([9.944935821188718438889964e-01, 7.922610700178102371982192e-02,\n",
            "        6.629617030222002826977956e-03, 7.097576839219542138223473e-04,\n",
            "        8.000642939216530370380442e-05, 1.150000076386526397893492e-05,\n",
            "        1.679985303989974791902544e-06, 2.644411796541931841980253e-07,\n",
            "        4.438617888996671970366700e-08, 7.450173602477830732219073e-09,\n",
            "        1.195958673045180167339571e-09, 6.602259093182898108675971e-02,\n",
            "        1.168020345613180258070862e-02, 2.179804526351125908839590e-03,\n",
            "        4.528857626097428632806763e-04, 7.991942601950834481651376e-05,\n",
            "        1.364132985677456338967969e-05, 2.399962768828051296726971e-06,\n",
            "        4.225637112054437203818370e-07, 7.440113108780584841020413e-08,\n",
            "        1.309986486390257405027523e-08, 9.942308592267259936825496e-03,\n",
            "        1.935737750005433067262972e-03, 8.502741534153328848213982e-04,\n",
            "        1.065304598489780945128058e-03, 2.783744406648212170798939e-04,\n",
            "        5.079034401619353616789965e-05, 9.054492818918991229953062e-06,\n",
            "        1.574917014345042515170562e-06, 2.742597598622822349869355e-07,\n",
            "        4.853802585348371753801237e-08, 1.920959614190699245653282e-03,\n",
            "        3.717833569113210691152449e-04, 1.236229575149692667783430e-04,\n",
            "        1.457867520228119594897370e-04, 2.136519966012596384846017e-04,\n",
            "        1.080208720104692979389199e-04, 2.100118532333829056294242e-05,\n",
            "        4.011966679660420387408103e-06, 7.412596476128779399860811e-07,\n",
            "        3.298281236744438418839742e-04, 7.356141691592938802041202e-05,\n",
            "        1.917216190676446639271600e-05, 1.099286050426471231201628e-05,\n",
            "        1.978644097880756394600837e-05, 2.701326335303474670450699e-05,\n",
            "        2.658664594052905075010837e-05, 6.027870515227320990809395e-06,\n",
            "        1.214579964959899388600982e-06, 2.781130874047605110611220e-05,\n",
            "        1.397146887242361105823090e-05, 3.582524010016991549055663e-06,\n",
            "        1.559108604969259279961202e-06, 1.320067020293768473779283e-06,\n",
            "        2.076649519426071871926144e-06, 3.185374841054062016284165e-06,\n",
            "        4.066332857227515171368547e-06, 2.294413169406015314205974e-06,\n",
            "        2.343803486894369863050014e-06, 6.874279953414872501398208e-07,\n",
            "        2.468298618047654930149999e-07, 1.318286439479163289899575e-07,\n",
            "        1.314332438531729479106846e-07, 2.082825149780588032203397e-07,\n",
            "        3.349825949447256367416453e-07, 1.898346958534523031449911e-07,\n",
            "        3.618142526290532541845507e-07, 1.174672345758554061931448e-07,\n",
            "        4.406105108920285913122399e-08, 1.962040105914244610042123e-08,\n",
            "        9.286845713733672589669693e-09, 1.299683455480772083980599e-08,\n",
            "        1.574442681759974549677219e-08, 4.433645338284914092640500e-08,\n",
            "        1.914841448905436091580466e-08, 7.389856255974766925044023e-09,\n",
            "        3.059796722534590160539786e-09, 1.385615889326180856001459e-09,\n",
            "        8.335294075346648776914252e-10, 1.216862494580588744316523e-09,\n",
            "        4.714065054228773226496988e-09, 3.121396052966758103487842e-09,\n",
            "        1.190526359349326433971359e-09, 5.073103342896068695143525e-10,\n",
            "        1.985607826399836004221980e-10, 9.362203474966636408027582e-11,\n",
            "        4.859166414227331145339272e-10, 4.424548862659425453214254e-10,\n",
            "        1.940685090780003340129926e-10, 8.508535967589688135598116e-11,\n",
            "        3.086621665989560468889807e-11, 7.228862030845828685915357e-12,\n",
            "        4.398356680569313229390369e-11, 5.573678178591610275232625e-11,\n",
            "        3.163523924042792086238283e-11, 1.319813095109480642706793e-11,\n",
            "        5.562568119868187154423973e-13, 3.356095370210765580271835e-12,\n",
            "        6.386233724231489102202598e-12, 5.086517307101087545178219e-12,\n",
            "        2.014889591791507403052140e-12, 4.257852158838252236165786e-14,\n",
            "        2.568507783885761301847788e-13, 6.686263109970254904752148e-13,\n",
            "        6.650504064626669145014299e-13, 3.258741405552052670805607e-15,\n",
            "        1.966218290608686002532960e-14, 6.558176252392986647114321e-14,\n",
            "        7.755621715734276856401229e-14, 2.494073338412158689220286e-16,\n",
            "        1.503553943275621676357978e-15, 6.432543118637531930274493e-15,\n",
            "        1.908835664830737986307204e-17, 1.152789308556149727737621e-16,\n",
            "        6.009952374046028272023683e-16, 1.461044998728688206323197e-18,\n",
            "        8.847279473673322570292216e-18, 1.118608135979961570482864e-19,\n",
            "        6.789996533132257257478745e-19, 8.564309538510947102615965e-21,\n",
            "        6.557023457296176236015578e-22], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network. Now lets impose null constraints as well\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 90000\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**3\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeCqZTnaY-O2",
        "outputId": "6363c334-03bb-4bca-baa5-187c670f89c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 546138476.3959588, Const: (1.1906247815904984, 1.0609012531350908, 1.1778325376330385, 0.9676940884148364, 0.7708685608972735), W00: -32625.087862171327\n",
            "Epoch 5000, Loss_Tot: 45346.46104565066, Const: (7.249594019764771e-05, 0.00022721494670485676, 0.016562452346417633, 0.009331718466331775, 0.008838796831356523), W00: -1388.7601474924957\n",
            "Epoch 10000, Loss_Tot: 9451.509373030225, Const: (8.194434054487054e-05, 8.876347186137501e-05, 0.007300177246865143, 0.005466372832349188, 0.002519376074724458), W00: -497.9424317631191\n",
            "Epoch 15000, Loss_Tot: 1215.7060511412692, Const: (0.0004502728611233131, -0.000387189306626734, 0.002657998862484459, 0.0017661777472336277, 0.0007599385331177603), W00: -104.2550940873325\n",
            "Epoch 20000, Loss_Tot: 293.10385349318835, Const: (0.00021544867144895896, -0.0002006830304317475, 0.0011426929125840342, 0.0007662900760603066, 0.000543767560062746), W00: -65.57159936846065\n",
            "Epoch 25000, Loss_Tot: 143.45778513856843, Const: (0.0002770109091356776, -0.0002158321872431923, 0.000743553315742303, 0.0005399442136948464, 0.00034388434984451824), W00: -34.85915412334232\n",
            "Epoch 30000, Loss_Tot: 65.38022782687473, Const: (5.2422111136429805e-06, -5.650376962540449e-06, 0.0005137416045855561, 0.00036337274553615076, 0.00022123341729551844), W00: -20.882845732269352\n",
            "Epoch 35000, Loss_Tot: 36.206841400695374, Const: (6.312506212502456e-05, -5.238688354047838e-05, 0.0003654180078152867, 0.0002678546354717044, 0.00015149616683221356), W00: -12.711174022735541\n",
            "Epoch 40000, Loss_Tot: 22.754643919719044, Const: (1.536583327843566e-05, -1.0938294414231109e-05, 0.00029441015327738465, 0.00020822422566990932, 0.00010804993582521606), W00: -8.548122893967761\n",
            "Epoch 45000, Loss_Tot: 79.38935573956783, Const: (0.0006484375770112738, -0.0004657986616061205, 0.00024650631016843343, 0.00016426888239857973, 8.257866416314251e-05), W00: -6.188501050907012\n",
            "Epoch 50000, Loss_Tot: 10.989756649399906, Const: (-1.5912886530466963e-05, 1.010897287812007e-05, 0.00018908622440593368, 0.000137522045994352, 6.644200386699373e-05), W00: -5.046170193119159\n",
            "Epoch 55000, Loss_Tot: 8.453757304963753, Const: (7.75576720624116e-08, -1.6375706435312054e-07, 0.00016443558429075997, 0.00011424556989390483, 5.3447358133657555e-05), W00: -4.158980850517475\n",
            "Epoch 60000, Loss_Tot: 6.968098140293401, Const: (3.124857421754612e-05, -2.2706162371655125e-05, 0.00014576491873741348, 9.654401603715445e-05, 4.316520410245626e-05), W00: -3.5757544790342766\n",
            "Epoch 65000, Loss_Tot: 5.723985079905122, Const: (2.3806269997317386e-05, -1.6863235309694247e-05, 0.00012667745798636967, 8.396834952844551e-05, 3.6909606827329036e-05), W00: -3.1928562442188264\n",
            "Epoch 70000, Loss_Tot: 28.383394707217484, Const: (0.00039180951254191854, -0.00028274040173248416, 0.00011925586842606658, 7.190585043162688e-05, 3.3028919764293756e-05), W00: -2.989379516084555\n",
            "Epoch 75000, Loss_Tot: 4.0163397966441226, Const: (-1.8423208070217356e-07, -2.8417833330962594e-07, 9.507596284693411e-05, 6.254920200071462e-05, 2.664868202439958e-05), W00: -2.6501289631821185\n",
            "Epoch 80000, Loss_Tot: 3.632574093419911, Const: (-5.287648168428305e-06, 3.164067694294559e-06, 8.537379837061358e-05, 5.704116422680981e-05, 2.393792607631225e-05), W00: -2.517236621726089\n",
            "Epoch 85000, Loss_Tot: 3.2495403721916425, Const: (-7.826697709933939e-07, 1.3674864596247005e-07, 8.261708836300742e-05, 4.699079564919874e-05, 1.911026140008707e-05), W00: -2.309585219364844\n",
            "Epoch 90000, Loss_Tot: 2.90460977845461, Const: (-1.322713201190595e-07, -2.3442760110903293e-07, 6.837006492918029e-05, 4.315985017815182e-05, 1.7531974694327347e-05), W00: -2.220141675004044\n",
            "Final values:\n",
            "-2.220141675004044 -1.322713201190595e-07 -2.3442760110903293e-07 6.837006492918029e-05 4.315985017815182e-05 1.7531974694327347e-05\n",
            "\n",
            "Minimum Cons: 0.000068109178027744667386 at epoch 89795\n",
            "Values at minimum cons epoch:\n",
            "W00: -2.2261718312\n",
            "W10 diff: 0.0000436474\n",
            "W01 diff: -0.0000314502\n",
            "Comp3: 0.0000681091780277\n",
            "Comp4: 0.0000436283180846\n",
            "Comp4: 0.0000177098054858\n",
            "Coeffs: tensor([9.990926170147829132872630e-01, 4.629864628077202254319289e-02,\n",
            "        2.698013311008713548122540e-03, 5.586089908631764909194284e-04,\n",
            "        1.314094704427630347972483e-04, 3.266006092916038072411847e-05,\n",
            "        8.581958161270973793285401e-06, 2.256641857199043242290402e-06,\n",
            "        5.941917570989238362168488e-07, 1.607543634340471094464357e-07,\n",
            "        4.353455243343207476540535e-08, 7.295986043186811209615428e-02,\n",
            "        1.536085825140651520137602e-02, 4.156135319136007588713966e-03,\n",
            "        1.094032475307755818139110e-03, 2.974366433650744209055139e-04,\n",
            "        8.083734194117471471858344e-05, 2.196824311886599878640425e-05,\n",
            "        5.969931090493584017661870e-06, 1.621746971804561971339221e-06,\n",
            "        4.438451139128503201720238e-07, 1.433884721384184532388950e-02,\n",
            "        5.824418411703083560404526e-03, 4.890037271683531430732650e-03,\n",
            "        3.231550866442571903758285e-03, 9.736994096961838415782431e-04,\n",
            "        2.678705621777606915752401e-04, 7.367395040057827550455255e-05,\n",
            "        2.024406549355377822152317e-05, 5.560104775592182321070859e-06,\n",
            "        1.527100294442245606745044e-06, 4.079667524059750818854475e-03,\n",
            "        3.732174915797911024517575e-03, 2.956189214887920083812967e-03,\n",
            "        2.057951565762291320366151e-03, 1.430870631741589698096573e-03,\n",
            "        9.137603859176447436765689e-04, 2.844448078288316222872467e-04,\n",
            "        7.716828612231513490667817e-05, 2.093374354401941948270079e-05,\n",
            "        1.379738981379372442931386e-03, 1.780945816929087885741034e-03,\n",
            "        1.701723642791376804248027e-03, 1.116358755908323152281070e-03,\n",
            "        7.277022267246742925558833e-04, 4.743230632585188847466506e-04,\n",
            "        3.108412532339199048012079e-04, 1.906950062210546260107330e-04,\n",
            "        6.774302389492022730913684e-05, 4.651251150832844171237324e-04,\n",
            "        5.380321791251868254746249e-04, 7.366667743154959148779715e-04,\n",
            "        5.157542013571717086709012e-04, 3.374192567607629225819033e-04,\n",
            "        2.199180585446019098230103e-04, 1.433319587761213895725498e-04,\n",
            "        9.389787360456542297717369e-05, 1.117562551867793465285375e-04,\n",
            "        1.477942508536078257581903e-04, 2.247997740978982495699962e-04,\n",
            "        2.318865772031544628464794e-04, 1.562489143699186198554346e-04,\n",
            "        1.019572029363056855748304e-04, 6.644941093573949770102055e-05,\n",
            "        4.330735627683381751057076e-05, 3.233815913050485578396007e-05,\n",
            "        3.909942449616505909899475e-05, 6.350110505535270990096902e-05,\n",
            "        7.976344795137005024617677e-05, 7.212811644802687822356640e-05,\n",
            "        4.726734326456571126532161e-05, 3.080564310013250765808365e-05,\n",
            "        1.126380050077265176694443e-05, 1.156811494812223516960841e-05,\n",
            "        1.703197146753179829414479e-05, 2.444915393392283828734139e-05,\n",
            "        2.547029375197671770768813e-05, 2.184799931798765294647426e-05,\n",
            "        1.428121877040875508453784e-05, 3.820181751569241555594077e-06,\n",
            "        3.243192646204720592910137e-06, 4.434753069250782942390537e-06,\n",
            "        7.133608541915629214637089e-06, 8.517689537623784968634923e-06,\n",
            "        8.034419112902060842985007e-06, 1.257133081011840714042902e-06,\n",
            "        9.034081623478758664457823e-07, 1.208766251627464692562359e-06,\n",
            "        1.952331981036976163500045e-06, 2.651858602071498410086749e-06,\n",
            "        2.748868323687186847017116e-06, 4.136929411550647021098091e-07,\n",
            "        2.539759889542753189506071e-07, 3.375220095793585741980657e-07,\n",
            "        5.083417371091790121258726e-07, 7.571938053364047511780842e-07,\n",
            "        1.361365822945172095600694e-07, 7.418197904706256541637572e-08,\n",
            "        9.098648115225666313606083e-08, 1.359753325216767511123889e-07,\n",
            "        2.099860128999026285098246e-07, 4.479933174946270520461070e-08,\n",
            "        2.388848409366763366026468e-08, 2.564649061134944181503712e-08,\n",
            "        3.912025226392469630655357e-08, 1.493861307684711841969732e-08,\n",
            "        7.692699357646946361186722e-09, 7.095050450380461265658497e-09,\n",
            "        1.065251140400247974785774e-08, 4.967037346765676062397554e-09,\n",
            "        2.477244803404399845097849e-09, 1.959765537985493984036063e-09,\n",
            "        1.651522788303241343559010e-09, 7.977358167577740502476704e-10,\n",
            "        5.413183434843690796307560e-10, 5.491256310047224383132803e-10,\n",
            "        2.568912172512478415779460e-10, 1.825823783062590923171715e-10,\n",
            "        8.272550398569635159154375e-11, 6.070801103000242091990276e-11,\n",
            "        2.018520427468133685204802e-11], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network. Lets try all betas = 4\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 90000\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**4\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiCk6Tg7cLTA",
        "outputId": "8d7bc41a-10bb-40bf-a4d4-b4e1cf9c75c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 2.9044372754614356, Const: (-2.3765322487712126e-07, -1.6327027840823405e-07, 6.836333437717173e-05, 4.315735123471734e-05, 1.7530966191779432e-05), W00: -2.2200852389996184\n",
            "Epoch 5000, Loss_Tot: 2.657896223635761, Const: (-3.6276189963047045e-07, -9.545012114031692e-08, 5.984211734423567e-05, 3.8723498263823574e-05, 1.5687344873531558e-05), W00: -2.1252140414225424\n",
            "Epoch 10000, Loss_Tot: 3.5255695403288905, Const: (-8.171555189595558e-05, 5.8884030568417955e-05, 6.0027001155346394e-05, 3.31589972658032e-05, 1.315340791767152e-05), W00: -2.0235162818419745\n",
            "Epoch 15000, Loss_Tot: 2.345904237061884, Const: (-1.1739498351204247e-07, -2.6114448403724566e-07, 5.430488836243722e-05, 3.019888543249214e-05, 1.186488424666531e-05), W00: -1.9457191332986823\n",
            "Epoch 20000, Loss_Tot: 2.1876849646790086, Const: (1.244287812784961e-07, -4.158684077104624e-07, 4.293656565615095e-05, 2.8311674963204598e-05, 1.1262257395435669e-05), W00: -1.9104723166527793\n",
            "Epoch 25000, Loss_Tot: 2.0756325311983628, Const: (-2.2991858616983052e-07, -1.3699668444111524e-07, 3.7234186997615874e-05, 2.617993889330613e-05, 1.0290355128291892e-05), W00: -1.8578588390838782\n",
            "Epoch 30000, Loss_Tot: 15.490917956237986, Const: (-0.00030369816869679767, 0.0002056928883638065, 3.929635858645757e-05, 2.4544339255292882e-05, 9.737600492489446e-06), W00: -1.8125588318160228\n",
            "Epoch 35000, Loss_Tot: 1.9923701582770417, Const: (1.172019427619908e-05, -8.6959030836109e-06, 3.152748889439462e-05, 2.4855076545943393e-05, 9.610910323359822e-06), W00: -1.8006592915124466\n",
            "Epoch 40000, Loss_Tot: 1.9072134754935925, Const: (-1.0633306057883374e-07, -1.8316691252628914e-07, 2.7640250997169622e-05, 2.3334843855212402e-05, 8.840056462685274e-06), W00: -1.7685444886901167\n",
            "Epoch 45000, Loss_Tot: 1.8884927131265654, Const: (-5.111525380741e-08, -2.0987319571119656e-07, 2.764716070146241e-05, 2.338647218162934e-05, 9.033085047096821e-06), W00: -1.7491991270315508\n",
            "Epoch 50000, Loss_Tot: 1.8696427661624773, Const: (-1.4857073120877473e-05, 1.0685933730547603e-05, 2.2530818152555327e-05, 2.2618808221115687e-05, 8.456100457987698e-06), W00: -1.727075197328931\n",
            "Epoch 55000, Loss_Tot: 1.7950454122204202, Const: (-8.916556692994959e-08, -1.4375604950878085e-07, 2.341761892949836e-05, 1.954853980259807e-05, 7.39111656634729e-06), W00: -1.6965266617065813\n",
            "Epoch 60000, Loss_Tot: 1.7603975387358195, Const: (-9.369808695591786e-07, 4.90943506514796e-07, 1.7785372944554613e-05, 1.9363657098035344e-05, 7.043214981620541e-06), W00: -1.6861978844417171\n",
            "Epoch 65000, Loss_Tot: 1.7339105218018618, Const: (-3.8069751728819767e-07, 9.059982697401381e-08, 1.6547576459635403e-05, 1.8943961277140595e-05, 6.794725547524116e-06), W00: -1.6660087828266843\n",
            "Epoch 70000, Loss_Tot: 2.4226396973218, Const: (6.728515891762932e-05, -5.021003854266404e-05, 1.890152728015447e-05, 1.7451680760274652e-05, 6.481610184244618e-06), W00: -1.6474216226730094\n",
            "Epoch 75000, Loss_Tot: 1.7205663668781772, Const: (-1.53875891744093e-05, 1.1156696756442486e-05, 1.3964036537586317e-05, 1.659879924043154e-05, 6.129652648676353e-06), W00: -1.6336326791425537\n",
            "Epoch 80000, Loss_Tot: 1.6632670995921714, Const: (-8.471671764098687e-08, -1.1553747114589896e-07, 1.1509747327757689e-05, 1.6832429424202602e-05, 6.036583468938854e-06), W00: -1.6180405166245175\n",
            "Epoch 85000, Loss_Tot: 1.6530104032312434, Const: (-6.210297764930317e-08, -1.1450817782865386e-07, 9.474647908770442e-06, 1.728969991268502e-05, 6.025067625072529e-06), W00: -1.6105082947457245\n",
            "Epoch 90000, Loss_Tot: 1.6353784880472577, Const: (-6.353851200735505e-08, -1.0269551586894465e-07, 8.796209732162146e-06, 1.7064217043546775e-05, 5.883546833570534e-06), W00: -1.5950593364656815\n",
            "Final values:\n",
            "-1.5950593364656815 -6.353851200735505e-08 -1.0269551586894465e-07 8.796209732162146e-06 1.7064217043546775e-05 5.883546833570534e-06\n",
            "\n",
            "Minimum Cons: 0.000016861278057789128243 at epoch 77806\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.6182169896\n",
            "W10 diff: 0.0000083334\n",
            "W01 diff: -0.0000063749\n",
            "Comp3: 0.0000144143678451\n",
            "Comp4: 0.0000157868430152\n",
            "Comp4: 0.0000059226924077\n",
            "Coeffs: tensor([9.982783401939953060733046e-01, 3.213430467303823867553092e-02,\n",
            "        1.522079506187939404102116e-03, 2.169118898032235045070931e-04,\n",
            "        3.863540829289426379146780e-05, 7.083037794682919758074407e-06,\n",
            "        1.432430580945512606517978e-06, 2.942477095790683887298420e-07,\n",
            "        6.156094422016229902343471e-08, 1.291260975958704817781884e-08,\n",
            "        2.708566951499299337872411e-09, 6.979005407337247890442455e-02,\n",
            "        1.067876013218862357723182e-02, 1.926710563422010099796089e-03,\n",
            "        3.700050202493885024776499e-04, 7.764663853536578536941593e-05,\n",
            "        1.664268180428399445552369e-05, 3.593518373492522587780724e-06,\n",
            "        7.768339561038079696072590e-07, 1.673424733405374856316268e-07,\n",
            "        3.604619825358557828609291e-08, 1.149031698066145870673616e-02,\n",
            "        2.984844705914746747915656e-03, 2.158285469967821407905051e-03,\n",
            "        1.217228565453236420815242e-03, 2.713153486888627771711491e-04,\n",
            "        5.784191376751507456194581e-05, 1.251671744983994162157478e-05,\n",
            "        2.708510547593509445470474e-06, 5.860962528102476268122910e-07,\n",
            "        1.268256208975006507791119e-07, 2.486058144191454974403221e-03,\n",
            "        1.569680016875426567177221e-03, 1.101812488549240532861861e-03,\n",
            "        6.288251051606630120591190e-04, 3.426725526204302935649737e-04,\n",
            "        1.544389637491406196134758e-04, 3.741793756428665539788200e-05,\n",
            "        8.110115409869788448003931e-06, 1.757799101349544184925934e-06,\n",
            "        6.320717495148456960274430e-04, 5.602781305735366748285720e-04,\n",
            "        4.393242392071237240568082e-04, 2.317745116097003445828489e-04,\n",
            "        1.216291614268258695300690e-04, 6.382611110869861200355080e-05,\n",
            "        3.342212336152116590156161e-05, 1.327724678408021486612865e-05,\n",
            "        3.646312101211469705992979e-06, 1.558825252782151938018423e-04,\n",
            "        1.201526833306237857608087e-04, 1.399206040892990118330863e-04,\n",
            "        8.157950602749802022830433e-05, 4.303263601696057821018732e-05,\n",
            "        2.258138163229082853397135e-05, 1.184952551258107562805102e-05,\n",
            "        6.209157147651258508457874e-06, 2.521719501688893849190333e-05,\n",
            "        2.407216055201576979018924e-05, 3.097107273803702785774927e-05,\n",
            "        2.748453618966020703092215e-05, 1.514654976706276298084770e-05,\n",
            "        7.989081463015391092186146e-06, 4.192236679807996620607084e-06,\n",
            "        2.199856467139005361196864e-06, 4.439427116075057187175989e-06,\n",
            "        4.627494405228599883808834e-06, 6.527425898871607167503819e-06,\n",
            "        6.906686171002502348543334e-06, 5.272150686328239822906350e-06,\n",
            "        2.812124949687454815623350e-06, 1.483165286133802947780977e-06,\n",
            "        1.015437776940908415815160e-06, 8.948486045465892102851652e-07,\n",
            "        1.307771730013497197777804e-06, 1.522636524184678777021711e-06,\n",
            "        1.367477728361035647951845e-06, 9.814609917853686278269932e-07,\n",
            "        5.220995631685249865403648e-07, 2.324090237361952491700024e-07,\n",
            "        1.560329230620593944576629e-07, 2.422100116347778559962604e-07,\n",
            "        3.209053062961365873784626e-07, 3.344351386316630290696731e-07,\n",
            "        2.692410476730204378832158e-07, 5.047959961417047487976944e-08,\n",
            "        2.815883758893723891261045e-08, 4.459987986648881774413107e-08,\n",
            "        6.581184105019114861788470e-08, 7.485661042022282040128776e-08,\n",
            "        6.721748743248715343513369e-08, 1.093510187625405273809444e-08,\n",
            "        5.088003083980441370315893e-09, 7.716889579988816936792368e-09,\n",
            "        1.268575019788341523157279e-08, 1.577649648443904810508009e-08,\n",
            "        2.368807441142143674750118e-09, 9.193481462493826369367078e-10,\n",
            "        1.257273738708254546425563e-09, 2.344088271969605820752923e-09,\n",
            "        3.130156037394702443719204e-09, 5.211883779554650800214086e-10,\n",
            "        1.957233758866769097087711e-10, 2.232213346705810152315141e-10,\n",
            "        4.316338717419441532632346e-10, 1.176763684035515112988957e-10,\n",
            "        4.276341997242147481154672e-11, 3.938587245199835796934496e-11,\n",
            "        7.243070476038313642777698e-11, 2.719509030170802453041270e-11,\n",
            "        9.343340208311264583692887e-12, 6.942053287096383288208731e-12,\n",
            "        6.200236597786140250435975e-12, 2.041417788928113528939452e-12,\n",
            "        1.239094114423249324275234e-12, 1.412756696952902947069153e-12,\n",
            "        4.511680738378028934262942e-13, 3.238513506656777885512565e-13,\n",
            "        9.973959081594627169548968e-14, 7.513845729882359292166461e-14,\n",
            "        1.743326916389756563251161e-14], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**4\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOMDkHmeZAdY",
        "outputId": "09c8d86d-ee1c-43a4-d4dd-6aff4209f2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 1.6353631884403284, Const: (-6.347779168969225e-08, -1.0273456196863151e-07, 8.79765899517082e-06, 1.706155164133721e-05, 5.883072051787231e-06), W00: -1.5950511415611177\n",
            "Epoch 5000, Loss_Tot: 1.6245851081622178, Const: (-5.695181548581729e-08, -9.156894065220911e-08, 7.867599122434894e-06, 1.7422709432902703e-05, 5.874831363307423e-06), W00: -1.5845875889760004\n",
            "Epoch 10000, Loss_Tot: 1.607445233294772, Const: (2.4991230707094303e-07, -3.1635544517349956e-07, 6.560231135406822e-06, 1.5374004188634047e-05, 5.368931724074433e-06), W00: -1.5766067730818931\n",
            "Epoch 15000, Loss_Tot: 1.7270021382637615, Const: (2.648154923035051e-05, -2.096572058118973e-05, 8.434091134962738e-06, 1.6032517929290725e-05, 5.7946133313451885e-06), W00: -1.5767434425423927\n",
            "Epoch 20000, Loss_Tot: 1.598310971413185, Const: (-1.124282686948419e-07, -3.1773844666460604e-08, 5.514027373529888e-06, 1.5770670239922264e-05, 5.655748781309454e-06), W00: -1.5671990032469352\n",
            "Epoch 25000, Loss_Tot: 1.5900272029547249, Const: (-4.841174705561002e-08, -8.057618861023741e-08, 5.794073122328613e-06, 1.5627597635666183e-05, 5.645976558884803e-06), W00: -1.5590593050815178\n",
            "Epoch 30000, Loss_Tot: 1.5830296918276743, Const: (-5.945875281021529e-08, -7.428891568572737e-08, 5.3884923950105885e-06, 1.5552585291268897e-05, 5.607073182914406e-06), W00: -1.5527929834878627\n",
            "Epoch 35000, Loss_Tot: 1.5818626776262146, Const: (-4.872400949373912e-08, -8.009671348396807e-08, 5.713987469270473e-06, 1.5794004222316776e-05, 5.837379257667577e-06), W00: -1.5502442767978148\n",
            "Epoch 40000, Loss_Tot: 1.5781858206614332, Const: (3.0800546697573594e-06, -2.4748136950858424e-06, 8.09959871268739e-06, 1.4988659401993192e-05, 5.6873974626271655e-06), W00: -1.5443636867146828\n",
            "Epoch 45000, Loss_Tot: 1.5715339922813345, Const: (-6.03961047840329e-08, -5.4827815398894586e-08, 4.729481070349409e-06, 1.576079143829908e-05, 5.822683459525161e-06), W00: -1.5410659088408347\n",
            "Epoch 50000, Loss_Tot: 1.6735042645691416, Const: (-2.53427842995535e-05, 1.9520115139171068e-05, 4.551783406141181e-06, 1.642930201919577e-05, 5.960521138301157e-06), W00: -1.5385582525336718\n",
            "Epoch 55000, Loss_Tot: 1.5603240587801122, Const: (-1.0540079704579597e-06, 6.787476733993714e-07, 4.08824724540841e-06, 1.4555139270262578e-05, 5.5276051118654795e-06), W00: -1.5342548693606137\n",
            "Epoch 60000, Loss_Tot: 1.5930293353823113, Const: (1.5531015732150877e-05, -1.1743465544222786e-05, 4.52161863425694e-06, 1.4452010546658324e-05, 5.534766064628161e-06), W00: -1.5291232641857906\n",
            "Epoch 65000, Loss_Tot: 1.961857868885549, Const: (5.889460470398511e-05, -2.5757901678025164e-05, 8.193384302251738e-06, 1.3012528329890755e-05, 5.249219731092246e-06), W00: -1.5222522979116129\n",
            "Epoch 70000, Loss_Tot: 1.6646685881343632, Const: (-2.839210416949456e-05, 2.075604850904078e-05, 4.447623491646413e-06, 1.4435550098067524e-05, 5.480651459862711e-06), W00: -1.5151556750680792\n",
            "Epoch 75000, Loss_Tot: 1.531795734867203, Const: (2.6800161978535186e-07, -3.3478340588644073e-07, 4.65704273749434e-06, 1.3894229501290443e-05, 5.449564924079406e-06), W00: -1.5073338025519831\n",
            "Epoch 80000, Loss_Tot: 1.5252040086807424, Const: (-1.3709694846042453e-07, 4.454860258462645e-09, 3.884682014467962e-06, 1.3733512846948242e-05, 5.392522131100693e-06), W00: -1.5019241846983133\n",
            "Epoch 85000, Loss_Tot: 1.523782157939358, Const: (-2.6197889073653613e-06, 1.862362596360967e-06, 4.279584098409921e-06, 1.3812138808371953e-05, 5.49425717913176e-06), W00: -1.4988213010566562\n",
            "Epoch 90000, Loss_Tot: 1.5256236295772712, Const: (-2.6894609850280915e-07, 8.041454813145776e-08, 4.579745094386881e-06, 1.4500389602920297e-05, 5.79472404270759e-06), W00: -1.499134330677209\n",
            "Final values:\n",
            "-1.499134330677209 -2.6894609850280915e-07 8.041454813145776e-08 4.579745094386881e-06 1.4500389602920297e-05 5.79472404270759e-06\n",
            "\n",
            "Minimum Cons: 0.000012960051068807461864 at epoch 64958\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5176843829\n",
            "W10 diff: 0.0000063800\n",
            "W01 diff: -0.0000049019\n",
            "Comp3: 0.0000066182173299\n",
            "Comp4: 0.0000119047436579\n",
            "Comp4: 0.0000051224995993\n",
            "Coeffs: tensor([9.982749345720448586050111e-01, 2.571146655996921215336926e-02,\n",
            "        1.122768171725645972719443e-03, 1.275546759765397908294865e-04,\n",
            "        1.732534083529910931358552e-05, 2.568807081031872616334617e-06,\n",
            "        4.070216720709439638558236e-07, 6.449160470779429537766490e-08,\n",
            "        1.021853813623701285067935e-08, 1.658960955757606494541195e-09,\n",
            "        2.751040979582929193057522e-10, 6.923605067981929150899845e-02,\n",
            "        8.813724071117247718665233e-03, 1.341287526657218088024792e-03,\n",
            "        2.052949058255335219348725e-04, 3.424092331856704740254346e-05,\n",
            "        5.789690167354856229300387e-06, 9.804556253372591293485582e-07,\n",
            "        1.653250522502620084890139e-07, 2.777562178180281920780091e-08,\n",
            "        4.664949699451395219296218e-09, 1.083231027131350242276309e-02,\n",
            "        2.539439497159353721633668e-03, 1.436029738048125994551163e-03,\n",
            "        6.554643304634911234879402e-04, 1.094393903667806961996223e-04,\n",
            "        1.763535607099562185166169e-05, 2.913313152109166855057874e-06,\n",
            "        4.812685002284442887661681e-07, 7.950368604158575180712658e-08,\n",
            "        1.312904560945687729776228e-08, 2.140648130128305036412595e-03,\n",
            "        1.219796406500049754800807e-03, 5.795488112610375566746690e-04,\n",
            "        2.396962482617781770369292e-04, 9.699633998398321792656224e-05,\n",
            "        3.173516839371131524573030e-05, 5.933908918351511480732130e-06,\n",
            "        9.974837103641409136916066e-07, 1.647225645521844761707468e-07,\n",
            "        4.731307699959230793178289e-04, 3.289056596943189703270705e-04,\n",
            "        1.853855028026942972866481e-04, 7.402640459713259746005881e-05,\n",
            "        2.932819111535970769033717e-05, 1.161924844797501258405591e-05,\n",
            "        4.603291493485398998188746e-06, 1.294562783312401053854379e-06,\n",
            "        2.681506078526454477175568e-07, 9.125833936431574180438153e-05,\n",
            "        5.879511956921612965106319e-05, 5.046967518450487776840094e-05,\n",
            "        2.220518038908220998070737e-05, 8.867576772402292834755581e-06,\n",
            "        3.513136584981481660408444e-06, 1.391824286347731127037896e-06,\n",
            "        5.514085651232831786284811e-07, 1.403357326077372962827919e-05,\n",
            "        1.005757631690943991725068e-05, 9.816719809411022455150272e-06,\n",
            "        6.359516547673188586911434e-06, 2.659510146542990758564964e-06,\n",
            "        1.062211021777391200147044e-06, 4.208233775161103567928741e-07,\n",
            "        1.667204323253629363285237e-07, 2.471811477018298678465886e-06,\n",
            "        1.651525545911002786441898e-06, 1.806789865530217195304752e-06,\n",
            "        1.406695820958339105918086e-06, 7.824617890352516165147312e-07,\n",
            "        3.185262741139272193996576e-07, 1.272375089432243442610736e-07,\n",
            "        5.118922508751790802030194e-07, 2.778049036096023170178495e-07,\n",
            "        3.110297322352741813409401e-07, 2.723581109512947204591654e-07,\n",
            "        1.787173748098786195374904e-07, 9.456121276755990587166006e-08,\n",
            "        3.814946256146282480080272e-08, 1.008669256940986655406083e-07,\n",
            "        4.543702612885147267197299e-08, 4.995920154744528058087094e-08,\n",
            "        5.012794172368512591809583e-08, 3.837664706550113401634683e-08,\n",
            "        2.255016989009368794940731e-08, 1.987554021901873295862259e-08,\n",
            "        7.511716839308999552791439e-09, 8.024704030214840489437614e-09,\n",
            "        8.844140244172908495668689e-09, 7.556352617539633527251817e-09,\n",
            "        4.956595686223994595671675e-09, 3.916418423922147183706288e-09,\n",
            "        1.241113279384241374511655e-09, 1.228503300323579887409373e-09,\n",
            "        1.504378491731048957919248e-09, 1.374819604170172629358837e-09,\n",
            "        7.717190578942690091900649e-10, 2.050612664433256216953503e-10,\n",
            "        1.842655237656522119916077e-10, 2.427497942835244647706737e-10,\n",
            "        2.376059313998738205279436e-10, 1.532078308824294349413750e-10,\n",
            "        3.885791217330048272392738e-11, 2.966419675382945051971975e-11,\n",
            "        3.899172027449667790353648e-11, 3.051695104368274182791070e-11,\n",
            "        7.663578903245088226668778e-12, 4.828864727874267629620272e-12,\n",
            "        5.834639009485049958494801e-12, 6.078568540477801315323057e-12,\n",
            "        1.511926604537049972555187e-12, 7.941120067211755699754176e-13,\n",
            "        1.208398139728520814647214e-12, 2.982949409059708062833212e-13,\n",
            "        1.311445526731496496225807e-13, 2.392315539710550317398657e-13,\n",
            "        5.884946907333742980517932e-14, 4.736165551218183757303606e-14,\n",
            "        1.161018688313938556601971e-14, 9.375264771907220279968699e-15,\n",
            "        1.855759216969382323953530e-15], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**4\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXsdSBzrk7bI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o2Jm5b1fNrp",
        "outputId": "b736b86f-0606-4668-ea55-7aa2212dd171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 1.5255999482912919, Const: (-9.941996337303749e-09, -1.125098814114267e-07, 4.573168881872401e-06, 1.4497608835855543e-05, 5.7941093688580954e-06), W00: -1.499132048663832\n",
            "Epoch 5000, Loss_Tot: 1.51981244738841, Const: (-3.8937995050503105e-08, -5.292278815183238e-08, 3.786477851097411e-06, 1.383409136431575e-05, 5.613933616132475e-06), W00: -1.4960884407855706\n",
            "Epoch 10000, Loss_Tot: 1.5184729837350925, Const: (-3.9261234929810485e-08, -6.76763485252252e-08, 4.48131963194703e-06, 1.375355786553402e-05, 5.590032334202444e-06), W00: -1.494423267471651\n",
            "Epoch 15000, Loss_Tot: 58.244408822925884, Const: (0.000612362544751166, -0.00043844733767151034, 7.950459424584181e-06, 1.3904632464543505e-05, 5.563834622677725e-06), W00: -1.4932629239155142\n",
            "Epoch 20000, Loss_Tot: 1.5109957344395082, Const: (-2.9838154169681275e-08, -5.150773074724668e-08, 4.1768589510934375e-06, 1.2583236819036555e-05, 5.264050198380217e-06), W00: -1.490645957700096\n",
            "Epoch 25000, Loss_Tot: 1.5091910491949785, Const: (-4.531580932720658e-08, -7.875232554965805e-08, 5.562436375882295e-06, 1.243679862482037e-05, 5.232451107861587e-06), W00: -1.4878909033432195\n",
            "Epoch 30000, Loss_Tot: 1.509079122831756, Const: (5.746060875999603e-07, -4.786354113051061e-07, 6.2818775627167056e-06, 1.2728045398980876e-05, 5.337688062967677e-06), W00: -1.4860275925055668\n",
            "Final values:\n",
            "-1.4860275925055668 5.746060875999603e-07 -4.786354113051061e-07 6.2818775627167056e-06 1.2728045398980876e-05 5.337688062967677e-06\n",
            "\n",
            "Minimum Cons: 0.000012960051068807461864 at epoch 64958\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5176843829\n",
            "W10 diff: 0.0000063800\n",
            "W01 diff: -0.0000049019\n",
            "Comp3: 0.0000066182173299\n",
            "Comp4: 0.0000119047436579\n",
            "Comp4: 0.0000051224995993\n",
            "Coeffs: tensor([9.982749345720448586050111e-01, 2.571146655996921215336926e-02,\n",
            "        1.122768171725645972719443e-03, 1.275546759765397908294865e-04,\n",
            "        1.732534083529910931358552e-05, 2.568807081031872616334617e-06,\n",
            "        4.070216720709439638558236e-07, 6.449160470779429537766490e-08,\n",
            "        1.021853813623701285067935e-08, 1.658960955757606494541195e-09,\n",
            "        2.751040979582929193057522e-10, 6.923605067981929150899845e-02,\n",
            "        8.813724071117247718665233e-03, 1.341287526657218088024792e-03,\n",
            "        2.052949058255335219348725e-04, 3.424092331856704740254346e-05,\n",
            "        5.789690167354856229300387e-06, 9.804556253372591293485582e-07,\n",
            "        1.653250522502620084890139e-07, 2.777562178180281920780091e-08,\n",
            "        4.664949699451395219296218e-09, 1.083231027131350242276309e-02,\n",
            "        2.539439497159353721633668e-03, 1.436029738048125994551163e-03,\n",
            "        6.554643304634911234879402e-04, 1.094393903667806961996223e-04,\n",
            "        1.763535607099562185166169e-05, 2.913313152109166855057874e-06,\n",
            "        4.812685002284442887661681e-07, 7.950368604158575180712658e-08,\n",
            "        1.312904560945687729776228e-08, 2.140648130128305036412595e-03,\n",
            "        1.219796406500049754800807e-03, 5.795488112610375566746690e-04,\n",
            "        2.396962482617781770369292e-04, 9.699633998398321792656224e-05,\n",
            "        3.173516839371131524573030e-05, 5.933908918351511480732130e-06,\n",
            "        9.974837103641409136916066e-07, 1.647225645521844761707468e-07,\n",
            "        4.731307699959230793178289e-04, 3.289056596943189703270705e-04,\n",
            "        1.853855028026942972866481e-04, 7.402640459713259746005881e-05,\n",
            "        2.932819111535970769033717e-05, 1.161924844797501258405591e-05,\n",
            "        4.603291493485398998188746e-06, 1.294562783312401053854379e-06,\n",
            "        2.681506078526454477175568e-07, 9.125833936431574180438153e-05,\n",
            "        5.879511956921612965106319e-05, 5.046967518450487776840094e-05,\n",
            "        2.220518038908220998070737e-05, 8.867576772402292834755581e-06,\n",
            "        3.513136584981481660408444e-06, 1.391824286347731127037896e-06,\n",
            "        5.514085651232831786284811e-07, 1.403357326077372962827919e-05,\n",
            "        1.005757631690943991725068e-05, 9.816719809411022455150272e-06,\n",
            "        6.359516547673188586911434e-06, 2.659510146542990758564964e-06,\n",
            "        1.062211021777391200147044e-06, 4.208233775161103567928741e-07,\n",
            "        1.667204323253629363285237e-07, 2.471811477018298678465886e-06,\n",
            "        1.651525545911002786441898e-06, 1.806789865530217195304752e-06,\n",
            "        1.406695820958339105918086e-06, 7.824617890352516165147312e-07,\n",
            "        3.185262741139272193996576e-07, 1.272375089432243442610736e-07,\n",
            "        5.118922508751790802030194e-07, 2.778049036096023170178495e-07,\n",
            "        3.110297322352741813409401e-07, 2.723581109512947204591654e-07,\n",
            "        1.787173748098786195374904e-07, 9.456121276755990587166006e-08,\n",
            "        3.814946256146282480080272e-08, 1.008669256940986655406083e-07,\n",
            "        4.543702612885147267197299e-08, 4.995920154744528058087094e-08,\n",
            "        5.012794172368512591809583e-08, 3.837664706550113401634683e-08,\n",
            "        2.255016989009368794940731e-08, 1.987554021901873295862259e-08,\n",
            "        7.511716839308999552791439e-09, 8.024704030214840489437614e-09,\n",
            "        8.844140244172908495668689e-09, 7.556352617539633527251817e-09,\n",
            "        4.956595686223994595671675e-09, 3.916418423922147183706288e-09,\n",
            "        1.241113279384241374511655e-09, 1.228503300323579887409373e-09,\n",
            "        1.504378491731048957919248e-09, 1.374819604170172629358837e-09,\n",
            "        7.717190578942690091900649e-10, 2.050612664433256216953503e-10,\n",
            "        1.842655237656522119916077e-10, 2.427497942835244647706737e-10,\n",
            "        2.376059313998738205279436e-10, 1.532078308824294349413750e-10,\n",
            "        3.885791217330048272392738e-11, 2.966419675382945051971975e-11,\n",
            "        3.899172027449667790353648e-11, 3.051695104368274182791070e-11,\n",
            "        7.663578903245088226668778e-12, 4.828864727874267629620272e-12,\n",
            "        5.834639009485049958494801e-12, 6.078568540477801315323057e-12,\n",
            "        1.511926604537049972555187e-12, 7.941120067211755699754176e-13,\n",
            "        1.208398139728520814647214e-12, 2.982949409059708062833212e-13,\n",
            "        1.311445526731496496225807e-13, 2.392315539710550317398657e-13,\n",
            "        5.884946907333742980517932e-14, 4.736165551218183757303606e-14,\n",
            "        1.161018688313938556601971e-14, 9.375264771907220279968699e-15,\n",
            "        1.855759216969382323953530e-15], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(30000+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**4\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhFX5M4-i4Zy",
        "outputId": "43273286-d4ba-4361-f171-1aed2148a5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 2941396017.6441875, Const: (1.4342534011555697, 4.366445105599574, 3.7198591671493593, 3.1109470039432052, 2.3846511289179024), W00: -82642.82347095115\n",
            "Epoch 5000, Loss_Tot: 38784.09907207144, Const: (0.0008743957773795774, 0.0021475232716710035, 0.012218998428155405, 0.009869164063193504, 0.010777598519938493), W00: -2492.6274728460958\n",
            "Epoch 10000, Loss_Tot: 3064.9544264895294, Const: (3.087353856745523e-05, 0.0005276334701524998, 0.0038354063336861768, 0.0017645192658357345, 0.0013191642201610023), W00: -1108.2686538614698\n",
            "Epoch 15000, Loss_Tot: 573.2697700101106, Const: (9.473901280387942e-05, 0.00018091525850461032, 0.0010550441956894184, 0.0007634475945098721, 0.00034312845245362887), W00: -391.85730226839735\n",
            "Epoch 20000, Loss_Tot: 84.79332316229124, Const: (-0.0003776721584214382, 0.0002912179491370015, 0.0004894430555841283, 0.00039448450004367136, 0.0001469367022212299), W00: -42.88958701975658\n",
            "Epoch 25000, Loss_Tot: 15.892790352022708, Const: (0.0003805595709911458, -0.00015288709168737036, 0.00020260768657472713, 0.0001761628258492422, 7.700318891523663e-05), W00: -7.923319604798229\n",
            "Epoch 30000, Loss_Tot: 5.541182145099761, Const: (0.00024597565206629923, -0.00015533063433603722, 9.870299720382347e-05, 8.861512724652915e-05, 4.4058256718660365e-05), W00: -3.5029452758246915\n",
            "Epoch 35000, Loss_Tot: 3.0315154563789575, Const: (-0.00017515080289598473, 0.00013808623971267586, 5.098486942583858e-05, 5.180075759709834e-05, 2.5187189191939812e-05), W00: -2.3900528532864906\n",
            "Epoch 40000, Loss_Tot: 2.1917891987790976, Const: (-0.00013199561010224237, 0.00012033815564782557, 2.581629179282267e-05, 3.4666522911213926e-05, 1.617919408012349e-05), W00: -1.9468835806112956\n",
            "Epoch 45000, Loss_Tot: 4.604701425460418, Const: (-0.0013337909882751298, 0.0009900373744449187, 1.4425678801776392e-05, 2.5034513797041586e-05, 1.0978068667557343e-05), W00: -1.7499945140970938\n",
            "Epoch 50000, Loss_Tot: 24.004462170508656, Const: (0.0037863506105471068, -0.0028245587329640554, 8.856145065252743e-06, 1.9387882597379966e-05, 8.293161862412342e-06), W00: -1.6375694054844228\n",
            "Epoch 55000, Loss_Tot: 1.6222176154227526, Const: (1.4091214203482139e-05, -8.066647062099364e-06, 2.0824415847487994e-06, 1.5564650937191148e-05, 6.814822182053289e-06), W00: -1.5926503099978495\n",
            "Epoch 60000, Loss_Tot: 1.5844434087930097, Const: (-7.691384678709667e-06, 6.928693790264617e-06, 3.268047333813765e-06, 1.3855837998331656e-05, 6.135758941729818e-06), W00: -1.560305052816787\n",
            "Epoch 65000, Loss_Tot: 2.9082550677297982, Const: (-0.0009201779008001232, 0.0007005566928615092, 4.195280332407443e-06, 1.3215461816583806e-05, 6.009536081777439e-06), W00: -1.547911685494697\n",
            "Epoch 70000, Loss_Tot: 1.5582242952183654, Const: (1.3041374724576116e-06, -1.6455095663836516e-07, 4.4221909142292915e-06, 1.276024394393483e-05, 5.882105473243661e-06), W00: -1.536524691087903\n",
            "Epoch 75000, Loss_Tot: 1.5489310681732222, Const: (-1.745453999113522e-05, 1.3937915368522624e-05, 4.936326221844247e-06, 1.2240317244411732e-05, 5.7838498895962684e-06), W00: -1.5276675814863308\n",
            "Epoch 80000, Loss_Tot: 1.540628462601508, Const: (-1.7645807838695404e-05, 1.3729959283503845e-05, 5.709543956142083e-06, 1.1640185873242325e-05, 5.640801454005629e-06), W00: -1.520137430245875\n",
            "Epoch 85000, Loss_Tot: 1.5341867240099827, Const: (-2.097567861847338e-06, 1.7328204058308927e-06, 6.271270456832727e-06, 1.1124485608256359e-05, 5.483304428046357e-06), W00: -1.514864357488321\n",
            "Epoch 90000, Loss_Tot: 1.5340344882543047, Const: (4.092337389782763e-05, -3.082519570085651e-05, 6.5130560331252165e-06, 1.1137284222796876e-05, 5.571673157015271e-06), W00: -1.511659318981268\n",
            "Final values:\n",
            "-1.511659318981268 4.092337389782763e-05 -3.082519570085651e-05 6.5130560331252165e-06 1.1137284222796876e-05 5.571673157015271e-06\n",
            "\n",
            "Minimum Cons: 0.000012149462358540478522 at epoch 88211\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5112291069\n",
            "W10 diff: 0.0000073293\n",
            "W01 diff: -0.0000054770\n",
            "Comp3: 0.0000065792392911\n",
            "Comp4: 0.0000108674367635\n",
            "Comp4: 0.0000054321500157\n",
            "Coeffs: tensor([9.888647177968603596553976e-01, 1.756184665184544468896632e-01,\n",
            "        4.457446920852600746876249e-02, 1.632332355960857198096114e-02,\n",
            "        5.980718131366696485584633e-03, 2.184102536886234954133235e-03,\n",
            "        7.966527910103320120338566e-04, 2.905339689292956752153518e-04,\n",
            "        1.059739881743047227582663e-04, 3.865237472447996324798405e-05,\n",
            "        1.409755497996106113876586e-05, 6.371575222984859898645027e-02,\n",
            "        1.549464950415282288309626e-02, 4.698384968959210504502177e-03,\n",
            "        1.718395858652311744552144e-03, 6.281676750336665556589955e-04,\n",
            "        2.295502285999105357861688e-04, 8.414710328924516417447438e-05,\n",
            "        3.242237186390727721836333e-05, 1.248567681429922763853158e-05,\n",
            "        4.802361416848333750180024e-06, 1.031809253318402216925698e-02,\n",
            "        1.922541446547047869902736e-03, 5.633869425408252577802681e-04,\n",
            "        1.752653523266595430706721e-04, 5.641675498092437948541647e-05,\n",
            "        2.072345314097910374010955e-05, 8.827635557692224232799461e-06,\n",
            "        3.866095719407212318496827e-06, 1.562962594115769446298839e-06,\n",
            "        6.318649542229411552175626e-07, 2.087942028627254396205837e-03,\n",
            "        3.844632830705569932587540e-04, 1.381964905834812986067800e-04,\n",
            "        4.679834315914416844337312e-05, 1.467797183858407794869883e-05,\n",
            "        4.603592091562567585529431e-06, 1.443863437484658792807520e-06,\n",
            "        4.528505836387172962784783e-07, 1.428708636396096549838250e-07,\n",
            "        3.880983973544556236111303e-04, 1.485351692661124247286236e-04,\n",
            "        3.827103850963897946303380e-05, 1.295040236078750665914619e-05,\n",
            "        4.389633904931277606992560e-06, 1.313464601751777128534906e-06,\n",
            "        3.876884188340972500057561e-07, 1.209098032706050456201746e-07,\n",
            "        3.792190362205638332345712e-08, 5.196881383604635236059255e-05,\n",
            "        3.726792224030330431252742e-05, 2.161029952920098047737529e-05,\n",
            "        4.404579764316426954010378e-06, 1.417480082601728755590468e-06,\n",
            "        4.645323912104105369976046e-07, 1.316381473544316536196798e-07,\n",
            "        3.832363607681040616965282e-08, 7.100785851606955305056253e-06,\n",
            "        7.062175136330655752440848e-06, 5.364224327277920906730489e-06,\n",
            "        2.819249433492631930794729e-06, 5.206793547802381660316562e-07,\n",
            "        1.645151865569030142947903e-07, 5.209632908225028838676424e-08,\n",
            "        1.408239120936751993804256e-08, 8.834362084731099719201374e-07,\n",
            "        1.284122525827048251443880e-06, 8.748788492671116447609481e-07,\n",
            "        6.232801187478431768055785e-07, 2.439062431728642260497363e-07,\n",
            "        5.774255392018382704308059e-08, 1.928759674825309718077555e-08,\n",
            "        1.086911301383681643291739e-07, 1.974944852826604206239534e-07,\n",
            "        1.341550656560462050262254e-07, 1.182165430922484944318939e-07,\n",
            "        6.305163806312789464329175e-08, 2.082182093427171430382760e-08,\n",
            "        5.640518728126228917290620e-09, 1.302027315299126575898533e-08,\n",
            "        2.659603475137949809168799e-08, 2.286177615405264943730785e-08,\n",
            "        2.017862764454691718692535e-08, 1.186431325964979240199841e-08,\n",
            "        6.034493476987950122081968e-09, 1.493423633511418755337360e-09,\n",
            "        3.583027183811632994053924e-09, 3.598635462095594184562458e-09,\n",
            "        3.175846867560560273548148e-09, 2.130413572702785632231460e-09,\n",
            "        1.179852941120419532147077e-09, 1.722241337753465047743949e-10,\n",
            "        4.695037024028692300355531e-10, 5.589507703774055340127978e-10,\n",
            "        4.744761939810754124324680e-10, 3.825473816566507572673193e-10,\n",
            "        1.986117774739725308668799e-11, 5.634186985986998493960144e-11,\n",
            "        7.876874362118230048776083e-11, 6.843684208094083561159714e-11,\n",
            "        6.072741955774808275079582e-11, 2.290424534819683230729934e-12,\n",
            "        6.768250416469379447514466e-12, 1.034227557582328311225181e-11,\n",
            "        9.962991326057150653766212e-12, 2.641356225891814707794972e-13,\n",
            "        8.131719795177886696860836e-13, 1.378312066047660910735386e-12,\n",
            "        1.483416167558250199153968e-12, 3.046056574223914011589454e-14,\n",
            "        9.769861154399827236326349e-14, 1.856866856194020961889478e-13,\n",
            "        3.512763845489616783950283e-15, 1.175647384169817535793978e-14,\n",
            "        2.501577550225714603783090e-14, 4.050978546688002566175106e-16,\n",
            "        1.418259082489924730391845e-15, 4.671656822816991574279607e-17,\n",
            "        1.629558675639297254902165e-16, 5.387433485179870608479655e-18,\n",
            "        6.208571266108810674858460e-19], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network. Lets try betas = 3,4,4\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 90000\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKd3CyHllH2n"
      },
      "outputs": [],
      "source": [
        "#Old best with betas = 4,6,4 and a lot of pain\n",
        "Values at minimum cons epoch:\n",
        "W00: -1.5212958279\n",
        "W10 diff: 0.0000005004\n",
        "W01 diff: -0.0000006783\n",
        "Comp3: 0.0000005012250899\n",
        "Comp4: 0.0000009682926051\n",
        "Comp4: 0.0000003606152386\n",
        "\n",
        "\n",
        "\n",
        "#New ones with betas = 3,4,3\n",
        "Values at minimum cons epoch:\n",
        "W00: -1.4959695484\n",
        "W10 diff: 0.0000012536\n",
        "W01 diff: -0.0000012741\n",
        "Comp3: 0.0000042690726472\n",
        "Comp4: 0.0000038353693856\n",
        "Comp4: 0.0000021966492878\n",
        "\n",
        "\n",
        "Values at minimum cons epoch:\n",
        "W00: -1.5118181851\n",
        "W10 diff: 0.0000006890\n",
        "W01 diff: 0.0000004421\n",
        "Comp3: 0.0000022847899544\n",
        "Comp4: 0.0000020846321734\n",
        "Comp4: 0.0000009366823393\n",
        "\n",
        "\n",
        "Values at minimum cons epoch:\n",
        "W00: -1.5311739895\n",
        "W10 diff: 0.0000062286\n",
        "W01 diff: -0.0000030309\n",
        "Comp3: 0.0000049949470393\n",
        "Comp4: 0.0000473592149168\n",
        "Comp4: 0.0000211285612638\n",
        "\n",
        "\n",
        "Values at minimum cons epoch:\n",
        "W00: -1.4604879195\n",
        "W10 diff: 0.0000033676\n",
        "W01 diff: -0.0000074755\n",
        "Comp3: 0.0000065669608366\n",
        "Comp4: 0.0000197875246899\n",
        "Comp4: 0.0000042336134230\n",
        "\n",
        "\n",
        "#For all betas =4\n",
        "Values at minimum cons epoch:\n",
        "W00: -1.5176843829\n",
        "W10 diff: 0.0000063800\n",
        "W01 diff: -0.0000049019\n",
        "Comp3: 0.0000066182173299\n",
        "Comp4: 0.0000119047436579\n",
        "Comp4: 0.0000051224995993\n",
        "\n",
        "\n",
        "\n",
        "#For betas = 3,4,4\n",
        "Values at minimum cons epoch:\n",
        "W00: -1.5112291069\n",
        "W10 diff: 0.0000073293\n",
        "W01 diff: -0.0000054770\n",
        "Comp3: 0.0000065792392911\n",
        "Comp4: 0.0000108674367635\n",
        "Comp4: 0.0000054321500157"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuu2NSyKrXsJ",
        "outputId": "e8f39731-a1c0-4299-8d77-db300471ddbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.5000, -0.1000],\n",
              "        [-1.5000,  0.0818],\n",
              "        [-0.4000, -0.1000],\n",
              "        [-0.4000,  0.0818],\n",
              "        [ 0.7000, -0.1000],\n",
              "        [ 0.7000,  0.0818],\n",
              "        [ 1.8000, -0.1000],\n",
              "        [ 1.8000,  0.0818]], dtype=torch.float64)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#THIS SECTION IS JUST TO TEST THE EXPRESSIONS FOR THE OPEN STRING AMPLITUDE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.special import legendre\n",
        "from scipy import integrate\n",
        "from scipy.special import hyp2f1 as scipy_hyp2f1\n",
        "from scipy.special import gamma as scipy_gamma\n",
        "from scipy.special import zeta\n",
        "#This is all for to check formulaes using the Open String Amplitude\n",
        "#Some definitions\n",
        "d = 10;\n",
        "alpha = (d-3)/2;\n",
        "lm =146/10;\n",
        "dlm =10**(-1)\n",
        "lmp = lm + dlm\n",
        "lmm = lm - dlm\n",
        "ellmax = 20;\n",
        "w00 = -(np.pi)**2/6\n",
        "w10 = -zeta(3)\n",
        "w01 = 7*np.pi**4/360\n",
        "\n",
        "def hyp2f1(a, b, c, z):\n",
        "    if isinstance(z, torch.Tensor):\n",
        "        z_np = z.detach().numpy()\n",
        "        result = scipy_hyp2f1(a,b,c,z_np)\n",
        "        return torch.tensor(result, dtype=z.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_hyp2f1(a,b,c,z))\n",
        "\n",
        "def gamma(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x_np = x.detach().numpy()\n",
        "        result = scipy_gamma(x_np)\n",
        "        return torch.tensor(result, dtype=x.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_gamma(x))\n",
        "\n",
        "def ker(n,s1,s2):\n",
        "    return (1/(s1 - n) + 1/(s2 - n) + 1/(n + lm))\n",
        "\n",
        "def geg(ell,beta, z):\n",
        "    return gamma(ell + 2*beta) / (gamma(2*beta) * gamma(ell + 1)) * \\\n",
        "           hyp2f1(-ell, ell + 2*beta, beta + 1/2, (1-z)/2)\n",
        "\n",
        "#ell,n tensor and Null grid tensor\n",
        "ell = torch.arange(ellmax + 1, dtype = torch.float64).unsqueeze(1).repeat(1, ellmax + 1)\n",
        "n = torch.arange(1, ellmax + 2, dtype = torch.float64).unsqueeze(0).repeat(ellmax + 1, 1)\n",
        "mask = (n > ell) & (n <= ellmax + 1) & ((n - ell - 1) % 2 == 0)\n",
        "ell_n_tensor = torch.stack([ell[mask], n[mask]], dim=1)\n",
        "\n",
        "NullPoints = torch.stack(torch.meshgrid(\n",
        "    torch.arange(-2 +1/2, 2+1/2 + 1e-6, 11/10, dtype=torch.float64),\n",
        "    torch.arange(-0.1, 0.1 + 1e-6, 2/11, dtype=torch.float64),\n",
        "    indexing='ij'\n",
        ")).permute(1, 2, 0).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def w00comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    result = (-1/n)*geg(ell,alpha,1)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def w10comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-((2 * ell * (-3 + d + ell) * lm * (n + 2 * lm) * \\\n",
        "            hyp2f1(1 - ell, -2 + d + ell, d / 2, lm / (n + lm))) / ((-2 + d) * (n + lm)**2)) - \\\n",
        "            hyp2f1(-ell, -3 + d + ell, 1/2 * (-2 + d), lm / (n + lm)))) / (n**2 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def w01comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-(1/((d - 2) * (n + lm)**2)) *\\\n",
        "             2 * ell * (-3 + d + ell) * n * (n + 2*lm) *  hyp2f1(1 - ell, -2 + d + ell, d/2, lm/(n + lm)) + \\\n",
        "             2 * hyp2f1(-ell, -3 + d + ell, (d - 2)/2, lm/(n + lm)))) / (n**3 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcomp(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(n, s1, s2) * geg(ell, alpha, 1 + 2 * (((s1 + lm) * (s2 + lm)) / (n + lm) - lm) / n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1P(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmp)*(s2 + lmp))/(n + lmp)-lmp)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1M(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmm)*(s2 + lmm))/(n + lmm)-lmm)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD1(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ((2 * (-3 + d) * (n**2 + 2 * n * lm - s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) /  (n * (n + lm))) / n - (n + lm) * geg(ell, 1/2 * (-3 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) / (n * (n + lm)))) / (n + lm)**3)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD2(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = (1 / (n + lm)**4 * 2 * (-((2 * (-3 + d) * (-1 + d) * (n - s1) * (n - s2) *\n",
        "               (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "               geg(-2 + ell,(d+1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) /\n",
        "              (n**2 * (n + lm))) + ((-3 + d) * (n - s1) * (n - s2) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            ((-3 + d) * (2*n - s1 - s2) * (n + lm) *\n",
        "             geg(-1 + ell, (d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n -\n",
        "            (3 * (-3 + d) * (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            (n + lm) * geg(ell,(d-3)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))))\n",
        "\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD3(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result =  (2 / (n**3 * (n + lm)**7)) * (4 * (-3 + d) * (-1 + d) * (1 + d) * (n - s1)**2 * (n - s2)**2 * (n**2 + 2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-3 + ell, (3 + d)/2, 1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * (-1 + d) * n * (n - s1)**2 * (n -s2)**2 * (n + lm) * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     4 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (2 * n - s1 -s2) * (n + lm)**2 * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     8 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (n + lm) * (n**2 +2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (n - s1) * (n -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (2 * n - s1 - s2) * (n + lm)**3 * geg(-1 + ell, 1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     n**3 * (n + lm)**3 * geg(ell, 1/2 * (-3 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +\n",
        "     4 * n * (n + lm) * (2 * (-3 + d) * (-1 + d) * (n - s1) * (n -s2) * (n**2 + 2 * n * lm - s2 * lm -\n",
        "     s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))\n",
        "     - (-3 + d) * n * (n - s1) * (n -s2) * (n + lm) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) - (-3 + d) * n * (2 * n - s1 -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +3 * (-3 + d) * n * (n + lm) * (n**2 + 2 * n * lm -\n",
        "      s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell,1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "        n**2 * (n + lm)**2 * geg(ell, 1/2 * (-3 + d),  1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "#First, lets calculate cc(ell,n) for open string\n",
        "\n",
        "def AbsOpString(n,s2):\n",
        "    return (-1)**(1 + n)*gamma(-s2)/(gamma(n+1)*gamma(1 - n - s2))\n",
        "\n",
        "\n",
        "\n",
        "def cc(ell_n_tensor):\n",
        "    z = torch.linspace(-1 + 10**(-6), 1 - 10**(-6), 10**6).view(1, -1)\n",
        "\n",
        "    # Directly use ell and n from ell_n_tensor\n",
        "    ell_values = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n_values = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "\n",
        "    # Compute normfac for all combinations of ell and n\n",
        "    normfac = ((np.pi * 2**(1 - 2 * alpha) * gamma(2 * alpha + ell_values)) /\n",
        "                (gamma(ell_values + 1) * (ell_values + alpha) * gamma(alpha)**2))**-1\n",
        "\n",
        "    # Compute integrand\n",
        "    integrand = normfac * (1 - z**2)**(alpha - 1/2) * \\\n",
        "                AbsOpString(n_values, n_values * (z - 1) / 2) * \\\n",
        "                geg(ell_values, alpha, z)\n",
        "\n",
        "    # Integrate across z\n",
        "    return torch.trapz(integrand, z, dim=1)\n",
        "\n",
        "NullPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2TB1oFwrXsU",
        "outputId": "c1a816f1-a8dd-44c9-dde5-615879e048d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-740bb5865f5a>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(result, dtype=z.dtype)\n",
            "<ipython-input-2-740bb5865f5a>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(scipy_hyp2f1(a,b,c,z))\n"
          ]
        }
      ],
      "source": [
        "#Store all the values\n",
        "coeff_vals = cc(ell_n_tensor)\n",
        "w00comp_vals = w00comp(ell_n_tensor)\n",
        "w01comp_vals = w01comp(ell_n_tensor)\n",
        "w10comp_vals = w10comp(ell_n_tensor)\n",
        "\n",
        "ampcomp_vals = ampcomp(ell_n_tensor, NullPoints)\n",
        "ampcompD1P_vals = ampcompD1P(ell_n_tensor, NullPoints)\n",
        "ampcompD1M_vals = ampcompD1M(ell_n_tensor, NullPoints)\n",
        "ampcompD1_vals = ampcompD1(ell_n_tensor, NullPoints)\n",
        "ampcompD2_vals = ampcompD2(ell_n_tensor, NullPoints)\n",
        "ampcompD3_vals = ampcompD3(ell_n_tensor, NullPoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRCUgaowrXsU",
        "outputId": "6b5543db-a556-4881-a9be-19aa0dd0f75d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-1.6449340668482264,\n",
              " -1.5984308179417641,\n",
              " -1.2020569031595942,\n",
              " -1.202056902362787,\n",
              " 1.8940656589944915,\n",
              " 1.894065658958795,\n",
              " -0.9368392530438512,\n",
              " 1.4470927928167399e-11,\n",
              " 7.384788822117905e-10,\n",
              " -3.6782704044614235e-10,\n",
              " -0.9366382712809055,\n",
              " -0.9370430272245681,\n",
              " 0.9995680497779686)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#CHECKS FOR OPEN STRING\n",
        "def w00fullX():\n",
        "    return torch.dot(coeff_vals,w00comp_vals)\n",
        "\n",
        "def w10fullX():\n",
        "    return torch.dot(coeff_vals,w10comp_vals)\n",
        "\n",
        "def w01fullX():\n",
        "    return torch.dot(coeff_vals,w01comp_vals)\n",
        "\n",
        "def ampfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1PfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1MfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD3_vals, dim = 0)\n",
        "\n",
        "(w00, w00fullX().item(),w10, w10fullX().item(),w01, w01fullX().item(),ampfullX()[1].item(),\n",
        " ampD1fullX()[1].item(),ampD2fullX()[1].item(),ampD3fullX()[1].item(),\n",
        " ampD1PfullX()[1].item(),ampD1MfullX()[1].item(),(ampD1PfullX()/ampD1MfullX())[1].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-8_zMDarXsV"
      },
      "outputs": [],
      "source": [
        "# Same definitions for the PINN\n",
        "def w00full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w00comp_vals)\n",
        "\n",
        "def w01full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w01comp_vals)\n",
        "\n",
        "def w10full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w10comp_vals)\n",
        "\n",
        "def ampfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1Pfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1Mfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD3_vals, dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4NE3uTMrXsV",
        "outputId": "4c953174-9034-42cc-e864-d4f7417716cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss_Tot: 3032880943.758614, Const: (1.5600017283286223, 6.706810379854307, 5.433765075937293, 4.5648051004082735, 3.45460394646817), W00: -114008.81565555999\n",
            "Epoch 5000, Loss_Tot: 9683.746061569747, Const: (2.0157613816795816e-05, 0.0006048677075822795, 0.007980506770757356, 0.018938274624205928, 0.01923553068864023), W00: -2585.867071870814\n",
            "Epoch 10000, Loss_Tot: 1005.5874288836735, Const: (-3.3224187916403025e-05, 0.00016980997199067538, 0.0020254186292869543, 0.00704965418344302, 0.0068216996199992564), W00: -499.0922174125252\n",
            "Epoch 15000, Loss_Tot: 22.623251916459118, Const: (0.00011848952604021079, 5.276037207369022e-05, 0.00033697343887477394, 0.0001229042439029586, 6.385920810902182e-05), W00: -11.23213518947776\n",
            "Epoch 20000, Loss_Tot: 8.441014955692493, Const: (-0.0014983734133482418, 0.0010571909200212115, 0.0001268891661327194, 5.1696699540969436e-05, 3.788216323509383e-05), W00: -3.464045773267852\n",
            "Epoch 25000, Loss_Tot: 2.1528507967245933, Const: (1.3487150787971913e-05, 3.827235226738779e-06, 2.7917761996888416e-05, 4.3300502197106805e-05, 1.8742968511063213e-05), W00: -2.0724878699081164\n",
            "Epoch 30000, Loss_Tot: 4.476135049465904, Const: (0.0013447384174762878, -0.0009659715899896781, 1.555542139282808e-05, 2.2486292725159695e-05, 9.400532360942625e-06), W00: -1.7099214085220504\n",
            "Epoch 35000, Loss_Tot: 2.683649860057608, Const: (0.0008270009508137832, -0.0006209197725124405, 8.58849486796301e-06, 1.459583009100285e-05, 4.261169397153061e-06), W00: -1.6065705032834263\n",
            "Epoch 40000, Loss_Tot: 1.5799080536022476, Const: (-9.468283279567835e-05, 7.223106040421534e-05, 4.734265002345856e-06, 1.0033102384804331e-05, 1.8682845912972034e-06), W00: -1.563380408546894\n",
            "Epoch 45000, Loss_Tot: 2.1996184611772245, Const: (-0.000654352295688776, 0.0004841483604065555, 4.616049030092079e-06, 6.780787990046681e-06, 1.5689929284712304e-06), W00: -1.5348626677303003\n",
            "Epoch 50000, Loss_Tot: 1.582062375031369, Const: (-0.00020452602630793848, 0.00015156113430458973, 5.136558505251242e-06, 5.5387955472879e-06, 1.9080939693835357e-06), W00: -1.5145879597558618\n",
            "Epoch 55000, Loss_Tot: 1.5058817885418743, Const: (-1.9073042589745626e-07, -8.838795497734253e-07, 5.045451256826669e-06, 4.511671673141235e-06, 2.1214038754220607e-06), W00: -1.50331045754653\n",
            "Epoch 60000, Loss_Tot: 1.4971114837712298, Const: (-8.841801948822336e-06, 5.716013816936538e-06, 4.874881793003005e-06, 3.966255856944938e-06, 2.281679761453451e-06), W00: -1.4946032489979402\n",
            "Epoch 65000, Loss_Tot: 1.4891964075744464, Const: (-1.3690606315286402e-07, -6.934132983360541e-07, 4.887362677276407e-06, 3.5074056272808794e-06, 2.644910946415825e-06), W00: -1.4867879791671017\n",
            "Epoch 70000, Loss_Tot: 1.482044501010897, Const: (-2.1521975090976753e-08, -6.647334209741729e-07, 4.981264538851806e-06, 4.036116527119542e-06, 3.1023392825238347e-06), W00: -1.479536844290934\n",
            "Epoch 75000, Loss_Tot: 1.4745018148141134, Const: (9.691956552870806e-07, -1.3932220064294398e-06, 5.2363939127464405e-06, 5.050605188357074e-06, 3.7997449607953824e-06), W00: -1.4717170056108553\n",
            "Epoch 80000, Loss_Tot: 1.6172926587511858, Const: (0.0003075457116388236, -0.00023674820251717854, 6.670716884942128e-06, 7.400796918041851e-06, 5.701014159575599e-06), W00: -1.4621214628752655\n",
            "Epoch 85000, Loss_Tot: 1.4628657931055133, Const: (-2.4779141034603214e-06, 6.935283347608845e-07, 7.14864158907086e-06, 9.373550421702843e-06, 7.0803412174157e-06), W00: -1.4576108697294954\n",
            "Epoch 90000, Loss_Tot: 1.4594740647166013, Const: (2.67430896316867e-07, -9.437484247420969e-07, 7.819062897271849e-06, 1.081158377422646e-05, 8.12081814811586e-06), W00: -1.4531764900459765\n",
            "Final values:\n",
            "-1.4531764900459765 2.67430896316867e-07 -9.437484247420969e-07 7.819062897271849e-06 1.081158377422646e-05 8.12081814811586e-06\n",
            "\n",
            "Minimum Cons: 0.000004419878552367862959 at epoch 59683\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4959695484\n",
            "W10 diff: 0.0000012536\n",
            "W01 diff: -0.0000012741\n",
            "Comp3: 0.0000042690726472\n",
            "Comp4: 0.0000038353693856\n",
            "Comp4: 0.0000021966492878\n",
            "Coeffs: tensor([9.970221656970217338766815e-01, 4.975818980467485580510001e-02,\n",
            "        1.705843914057409912987673e-03, 6.046082199688397829312816e-05,\n",
            "        2.134781542846427362258464e-06, 7.646712491089148136049917e-08,\n",
            "        2.745698109394550731346833e-09, 9.866528814297416302144238e-11,\n",
            "        3.555298072975796656162472e-12, 1.284378866048727386005801e-13,\n",
            "        4.639917772546054365278491e-15, 6.930184938905854796775685e-02,\n",
            "        5.573188864055051219836834e-03, 1.559326539007061082804662e-04,\n",
            "        4.922309574170027119570988e-06, 1.813263360436173989307583e-07,\n",
            "        6.585449512774530792690599e-09, 2.383367958699617402681438e-10,\n",
            "        8.612845810512093546505194e-12, 3.095642020727557794207098e-13,\n",
            "        1.108564962561892750316439e-14, 1.143942698678337456519305e-02,\n",
            "        8.213375400389521423585859e-04, 2.495974138459090146697356e-05,\n",
            "        7.012794039311629018350466e-07, 1.867140784384353363399879e-08,\n",
            "        5.067558947353105067907780e-10, 1.997484091788689808084248e-11,\n",
            "        7.013799958023858367094697e-13, 2.494690787810529348212089e-14,\n",
            "        8.985358573290015395870715e-16, 2.635514918068754090796357e-03,\n",
            "        2.129750357496587261953158e-04, 7.344763292730874028075173e-06,\n",
            "        2.176548331503225952112838e-07, 5.638668659417198480249207e-09,\n",
            "        1.372066118835454738714500e-10, 3.349186673055001807365470e-12,\n",
            "        8.618784916453572204272209e-14, 2.447215285090221643211682e-15,\n",
            "        7.111775645558108573107692e-04, 5.846315595769616514990052e-05,\n",
            "        2.281708934236107587698690e-06, 7.219069370207199248213687e-08,\n",
            "        2.089719082990341597339894e-09, 5.306056366147117546780218e-11,\n",
            "        1.277015080737636667722940e-12, 3.073407826563088891519653e-14,\n",
            "        7.513606147501584163549296e-16, 2.018823079457286934820098e-04,\n",
            "        1.562378195044070719943105e-05, 7.333075844654462099417064e-07,\n",
            "        2.194325570440498868693198e-08, 6.823431883737413951810608e-10,\n",
            "        2.023606675185077013853641e-11, 5.125376972068886900869793e-13,\n",
            "        1.233530749765134821261729e-14, 5.937402606827771534480850e-05,\n",
            "        4.198781565476647754215581e-06, 2.344395294159612960640751e-07,\n",
            "        7.140497985761317101972168e-09, 2.090838865544903624997804e-10,\n",
            "        6.445914593141157882391669e-12, 1.960022453596248152681526e-13,\n",
            "        4.950849989595156954112386e-15, 9.507406338313218949198635e-06,\n",
            "        1.095611640826197864006836e-06, 7.174796192333608963662128e-08,\n",
            "        2.336793962066124716984742e-09, 6.769991974430470965057919e-11,\n",
            "        1.986563003585275509400019e-12, 6.090348409940489459392741e-14,\n",
            "        1.787664719030387598241050e-06, 2.716940437539863462167514e-07,\n",
            "        2.006739782227637892643416e-08, 7.468958568896552501278402e-10,\n",
            "        2.215542440176361958167340e-11, 6.430782022656743481061180e-13,\n",
            "        1.880038721825062748147825e-14, 3.364116743684817953427937e-07,\n",
            "        6.746455225832716037272647e-08, 5.258687454032466379221489e-09,\n",
            "        2.054961934319247200854723e-10, 7.068601301510753234004215e-12,\n",
            "        2.100582412604136102784835e-13, 6.330759161906927492838651e-08,\n",
            "        1.690076980596901769227431e-08, 1.378957490415091379325792e-09,\n",
            "        5.586581685658536958267293e-11, 2.063890462563551409966095e-12,\n",
            "        6.478168837818062645456240e-14, 1.191353040836080334989708e-08,\n",
            "        3.876123464761750144898049e-09, 3.436179300960307372762234e-10,\n",
            "        1.514859314965373886143190e-11, 5.637527882885742290003263e-13,\n",
            "        2.241946032066455568499367e-09, 8.330075251839839291844373e-10,\n",
            "        8.437779303774848680494607e-11, 4.083338529494701744554056e-12,\n",
            "        1.533727080414288279733991e-13, 3.651133667419510607087144e-10,\n",
            "        1.778091624953211520832813e-10, 2.071955894512007666992749e-11,\n",
            "        1.076298579825919876734629e-12, 5.716521788107702557832036e-11,\n",
            "        3.576506448739213940619876e-11, 5.087972482596659121579140e-12,\n",
            "        2.822071611233712732189211e-13, 8.965445597719665217751002e-12,\n",
            "        6.847280576522517813773184e-12, 1.227269186077375377873220e-12,\n",
            "        1.404786644696130634774630e-12, 1.306358470807145297030220e-12,\n",
            "        2.611637677420564669756843e-13, 2.201173680064429558435581e-13,\n",
            "        2.474848437509811064903913e-13, 3.450029003399910626325401e-14,\n",
            "        3.971289254082987717885631e-14, 5.407433421587804010472401e-15,\n",
            "        8.475388519948375089034907e-16], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network. Now lets impose null constraints as well\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 90000\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta2 = 10**4\n",
        "    beta3 = 10**3\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons = torch.max(torch.tensor([cons1,cons2,cons3]))**(1/2)\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta2**2)*cons2 + (beta3**2)*cons3\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "\n",
        "    if epoch % 20000 == 0:\n",
        "        lr = lr\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "torch.set_printoptions(precision=24)\n",
        "# Print final values\n",
        "print(\"Final values:\")\n",
        "print(w00vals.item(),  (losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(),(losscomp4**(1/2)).item(),(losscomp5**(1/2)).item())\n",
        "\n",
        "# Print the minimum loss and the epoch it occurred\n",
        "print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "print(\"Values at minimum cons epoch:\")\n",
        "print(f\"W00: {max_w00:.10f}\")\n",
        "print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "print(f\"Comp4: {min_cons_D3:.16f}\")\n",
        "print(f\"Coeffs: {min_cons_coeffs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With some null constraints, and multi-lambda equality constraints (Using product of cons as the strategy), FINAL"
      ],
      "metadata": {
        "id": "Cvn_WVJT9zso"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79574895-c085-4cfc-d46f-262c929a4983",
        "id": "IMSdVA9q956M"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.500000000000, -0.200000000000],\n",
              "        [-5.500000000000,  0.163636363636],\n",
              "        [-4.490000000000, -0.200000000000],\n",
              "        [-4.490000000000,  0.163636363636],\n",
              "        [-3.480000000000, -0.200000000000],\n",
              "        [-3.480000000000,  0.163636363636],\n",
              "        [-2.470000000000, -0.200000000000],\n",
              "        [-2.470000000000,  0.163636363636],\n",
              "        [-1.460000000000, -0.200000000000],\n",
              "        [-1.460000000000,  0.163636363636],\n",
              "        [-0.450000000000, -0.200000000000],\n",
              "        [-0.450000000000,  0.163636363636],\n",
              "        [ 0.560000000000, -0.200000000000],\n",
              "        [ 0.560000000000,  0.163636363636],\n",
              "        [ 1.570000000000, -0.200000000000],\n",
              "        [ 1.570000000000,  0.163636363636],\n",
              "        [ 2.580000000000, -0.200000000000],\n",
              "        [ 2.580000000000,  0.163636363636],\n",
              "        [ 3.590000000000, -0.200000000000],\n",
              "        [ 3.590000000000,  0.163636363636],\n",
              "        [ 4.600000000000, -0.200000000000],\n",
              "        [ 4.600000000000,  0.163636363636]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "#THIS SECTION IS JUST TO TEST THE EXPRESSIONS FOR THE OPEN STRING AMPLITUDE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.special import legendre\n",
        "from scipy import integrate\n",
        "from scipy.special import hyp2f1 as scipy_hyp2f1\n",
        "from scipy.special import gamma as scipy_gamma\n",
        "from scipy.special import zeta\n",
        "#This is all for to check formulaes using the Open String Amplitude\n",
        "#Some definitions\n",
        "d = 10;\n",
        "alpha = (d-3)/2;\n",
        "lm = 56/10;\n",
        "dlm = 1/2\n",
        "lmp = lm + dlm\n",
        "lmm = lm - dlm\n",
        "ellmax = 12;\n",
        "w00 = -(np.pi)**2/6\n",
        "w10 = -zeta(3)\n",
        "w01 = 7*np.pi**4/360\n",
        "\n",
        "torch.set_printoptions(precision=12)\n",
        "\n",
        "def hyp2f1(a, b, c, z):\n",
        "    if isinstance(z, torch.Tensor):\n",
        "        z_np = z.detach().numpy()\n",
        "        result = scipy_hyp2f1(a,b,c,z_np)\n",
        "        return torch.tensor(result, dtype=z.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_hyp2f1(a,b,c,z))\n",
        "\n",
        "def gamma(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x_np = x.detach().numpy()\n",
        "        result = scipy_gamma(x_np)\n",
        "        return torch.tensor(result, dtype=x.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_gamma(x))\n",
        "\n",
        "def ker(lm,n,s1,s2):\n",
        "    return (1/(s1 - n) + 1/(s2 - n) + 1/(n + lm))\n",
        "\n",
        "def geg(ell,beta, z):\n",
        "    return gamma(ell + 2*beta) / (gamma(2*beta) * gamma(ell + 1)) * \\\n",
        "           hyp2f1(-ell, ell + 2*beta, beta + 1/2, (1-z)/2)\n",
        "\n",
        "#ell,n tensor and Null grid tensor\n",
        "ell = torch.arange(ellmax + 1, dtype = torch.float64).unsqueeze(1).repeat(1, ellmax + 1)\n",
        "n = torch.arange(1, ellmax + 2, dtype = torch.float64).unsqueeze(0).repeat(ellmax + 1, 1)\n",
        "mask = (n > ell) & (n <= ellmax + 1) & ((n - ell - 1) % 2 == 0)\n",
        "ell_n_tensor = torch.stack([ell[mask], n[mask]], dim=1)\n",
        "\n",
        "NullPoints = torch.stack(torch.meshgrid(\n",
        "    torch.arange(-6 +1/2, 5+1/2 + 1e-6, 101/100, dtype=torch.float64),\n",
        "    torch.arange(-0.2, 0.2 + 1e-6, 4/11, dtype=torch.float64),\n",
        "    indexing='ij'\n",
        ")).permute(1, 2, 0).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def w00comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    result = (-1/n)*geg(ell,alpha,1)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def w10comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-((2 * ell * (-3 + d + ell) * lm * (n + 2 * lm) * \\\n",
        "            hyp2f1(1 - ell, -2 + d + ell, d / 2, lm / (n + lm))) / ((-2 + d) * (n + lm)**2)) - \\\n",
        "            hyp2f1(-ell, -3 + d + ell, 1/2 * (-2 + d), lm / (n + lm)))) / (n**2 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def w01comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-(1/((d - 2) * (n + lm)**2)) *\\\n",
        "             2 * ell * (-3 + d + ell) * n * (n + 2*lm) *  hyp2f1(1 - ell, -2 + d + ell, d/2, lm/(n + lm)) + \\\n",
        "             2 * hyp2f1(-ell, -3 + d + ell, (d - 2)/2, lm/(n + lm)))) / (n**3 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcomp(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lm,n, s1, s2) * geg(ell, alpha, 1 + 2 * (((s1 + lm) * (s2 + lm)) / (n + lm) - lm) / n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1P(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lmp,n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmp)*(s2 + lmp))/(n + lmp)-lmp)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1M(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lmm,n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmm)*(s2 + lmm))/(n + lmm)-lmm)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD1(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ((2 * (-3 + d) * (n**2 + 2 * n * lm - s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) /  (n * (n + lm))) / n - (n + lm) * geg(ell, 1/2 * (-3 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) / (n * (n + lm)))) / (n + lm)**3)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD2(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = (1 / (n + lm)**4 * 2 * (-((2 * (-3 + d) * (-1 + d) * (n - s1) * (n - s2) *\n",
        "               (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "               geg(-2 + ell,(d+1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) /\n",
        "              (n**2 * (n + lm))) + ((-3 + d) * (n - s1) * (n - s2) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            ((-3 + d) * (2*n - s1 - s2) * (n + lm) *\n",
        "             geg(-1 + ell, (d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n -\n",
        "            (3 * (-3 + d) * (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            (n + lm) * geg(ell,(d-3)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))))\n",
        "\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD3(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result =  (2 / (n**3 * (n + lm)**7)) * (4 * (-3 + d) * (-1 + d) * (1 + d) * (n - s1)**2 * (n - s2)**2 * (n**2 + 2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-3 + ell, (3 + d)/2, 1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * (-1 + d) * n * (n - s1)**2 * (n -s2)**2 * (n + lm) * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     4 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (2 * n - s1 -s2) * (n + lm)**2 * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     8 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (n + lm) * (n**2 +2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (n - s1) * (n -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (2 * n - s1 - s2) * (n + lm)**3 * geg(-1 + ell, 1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     n**3 * (n + lm)**3 * geg(ell, 1/2 * (-3 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +\n",
        "     4 * n * (n + lm) * (2 * (-3 + d) * (-1 + d) * (n - s1) * (n -s2) * (n**2 + 2 * n * lm - s2 * lm -\n",
        "     s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))\n",
        "     - (-3 + d) * n * (n - s1) * (n -s2) * (n + lm) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) - (-3 + d) * n * (2 * n - s1 -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +3 * (-3 + d) * n * (n + lm) * (n**2 + 2 * n * lm -\n",
        "      s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell,1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "        n**2 * (n + lm)**2 * geg(ell, 1/2 * (-3 + d),  1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "#First, lets calculate cc(ell,n) for open string\n",
        "\n",
        "def AbsOpString(n,s2):\n",
        "    return (-1)**(1 + n)*gamma(-s2)/(gamma(n+1)*gamma(1 - n - s2))\n",
        "\n",
        "\n",
        "\n",
        "def cc(ell_n_tensor):\n",
        "    z = torch.linspace(-1 + 10**(-6), 1 - 10**(-6), 10**6).view(1, -1)\n",
        "\n",
        "    # Directly use ell and n from ell_n_tensor\n",
        "    ell_values = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n_values = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "\n",
        "    # Compute normfac for all combinations of ell and n\n",
        "    normfac = ((np.pi * 2**(1 - 2 * alpha) * gamma(2 * alpha + ell_values)) /\n",
        "                (gamma(ell_values + 1) * (ell_values + alpha) * gamma(alpha)**2))**-1\n",
        "\n",
        "    # Compute integrand\n",
        "    integrand = normfac * (1 - z**2)**(alpha - 1/2) * \\\n",
        "                AbsOpString(n_values, n_values * (z - 1) / 2) * \\\n",
        "                geg(ell_values, alpha, z)\n",
        "\n",
        "    # Integrate across z\n",
        "    return torch.trapz(integrand, z, dim=1)\n",
        "\n",
        "NullPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60a0bf7-5677-411d-e84b-752eca051f41",
        "id": "8SelCZ8m956N"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-1ea80a7cb81f>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(result, dtype=z.dtype)\n",
            "<ipython-input-28-1ea80a7cb81f>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(scipy_hyp2f1(a,b,c,z))\n"
          ]
        }
      ],
      "source": [
        "#Store all the values\n",
        "coeff_vals = cc(ell_n_tensor)\n",
        "w00comp_vals = w00comp(ell_n_tensor)\n",
        "w01comp_vals = w01comp(ell_n_tensor)\n",
        "w10comp_vals = w10comp(ell_n_tensor)\n",
        "\n",
        "ampcomp_vals = ampcomp(ell_n_tensor, NullPoints)\n",
        "ampcompD1P_vals = ampcompD1P(ell_n_tensor, NullPoints)\n",
        "ampcompD1M_vals = ampcompD1M(ell_n_tensor, NullPoints)\n",
        "ampcompD1_vals = ampcompD1(ell_n_tensor, NullPoints)\n",
        "ampcompD2_vals = ampcompD2(ell_n_tensor, NullPoints)\n",
        "ampcompD3_vals = ampcompD3(ell_n_tensor, NullPoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90043fe4-9610-47c8-ba25-03f501ea3100",
        "id": "R_0glP-L956O"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.6449340668482264,\n",
              " -1.5708937983031839,\n",
              " -1.2020569031595942,\n",
              " -1.2020580619524968,\n",
              " 1.8940656589944915,\n",
              " 1.8940654448676988,\n",
              " -0.5624502228752933,\n",
              " 3.839619282262798e-07,\n",
              " 1.7739561382799387e-06,\n",
              " -3.90493232622108e-06,\n",
              " -0.5624498928853159,\n",
              " -0.5624501257869979,\n",
              " 0.9999995859158504)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#CHECKS FOR OPEN STRING\n",
        "def w00fullX():\n",
        "    return torch.dot(coeff_vals,w00comp_vals)\n",
        "\n",
        "def w10fullX():\n",
        "    return torch.dot(coeff_vals,w10comp_vals)\n",
        "\n",
        "def w01fullX():\n",
        "    return torch.dot(coeff_vals,w01comp_vals)\n",
        "\n",
        "def ampfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1PfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1MfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD3_vals, dim = 0)\n",
        "\n",
        "(w00, w00fullX().item(),w10, w10fullX().item(),w01, w01fullX().item(),ampfullX()[1].item(),\n",
        " ampD1fullX()[1].item(),ampD2fullX()[1].item(),ampD3fullX()[1].item(),\n",
        " ampD1PfullX()[1].item(),ampD1MfullX()[1].item(),(ampD1PfullX()/ampD1MfullX())[1].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITLCsDJE956O"
      },
      "outputs": [],
      "source": [
        "# Same definitions for the PINN\n",
        "def w00full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w00comp_vals)\n",
        "\n",
        "def w01full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w01comp_vals)\n",
        "\n",
        "def w10full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w10comp_vals)\n",
        "\n",
        "def ampfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1Pfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1Mfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD3_vals, dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network. We will impose multi-lambda equality. But lets first check how well\n",
        "# this is satisfied with three derivatives imposed.\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.0001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "\n",
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 4*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQrFYY_e-jNa",
        "outputId": "bcb78fbf-036a-400d-ffb5-e84bd1f622f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 341834.0335534883, Const: (0.22679816104075412, -4.990971892851955, 14.89791886876163, 9.916554164021845, 10.598252882842647, 2.9450139159638673), W00: -5490.58549038305\n",
            "Epoch 5000, Loss_Tot: 52800.41271418599, Const: (1.776044296713453, -1.198060700082214, 0.18094685140837283, 0.051474251257841865, 0.022703352463995165, 0.8304963737081668), W00: -6.342597063143899\n",
            "Epoch 10000, Loss_Tot: 46673.023786144746, Const: (1.760393392897039, -1.1981046291599193, 0.22370780409556054, 0.06453945665339879, 0.028678682138519734, 0.3633116594688823), W00: -8.674165384645763\n",
            "Epoch 15000, Loss_Tot: 25512.846579283083, Const: (1.3358172997715696, -0.8125491321644088, 0.6368299114479896, 0.19006545865669758, 0.08520202541119537, 0.32085997317559056), W00: -36.89584991926194\n",
            "\n",
            "Minimum Cons: 0.660597258573675572002060 at epoch 4647\n",
            "Values at minimum cons epoch:\n",
            "W00: -10.4930245297\n",
            "W10 diff: 1.7682111945\n",
            "W01 diff: -1.2135229421\n",
            "Comp3: 0.2496805220549490\n",
            "Comp4: 0.0737482991100804\n",
            "Comp5: 0.0334400602406296\n",
            "Comp6: 0.3080315531824664\n",
            "Coeffs: tensor([0.070777213019, 0.013108674963, 0.007865338708, 0.006370780626,\n",
            "        0.005174941462, 0.004178423429, 0.003359421044, 0.033590322431,\n",
            "        0.020766836021, 0.021474045464, 0.017810251082, 0.014338873420,\n",
            "        0.010476861925, 0.042878535790, 0.028757534404, 0.024556460438,\n",
            "        0.020263796762, 0.014801048019, 0.010697345521, 0.026508348042,\n",
            "        0.015375335904, 0.013675309252, 0.012010563453, 0.010035949023,\n",
            "        0.015612304530, 0.008787789201, 0.007115941296, 0.006455815714,\n",
            "        0.005835862306, 0.009174404262, 0.005162101848, 0.003770761718,\n",
            "        0.003354656270, 0.005384085255, 0.002941010423, 0.002161896209,\n",
            "        0.001791114611, 0.003170219914, 0.001671744744, 0.001245445265,\n",
            "        0.001872736030, 0.000950001748, 0.000703987288, 0.001114204557,\n",
            "        0.000553483528, 0.000662806220, 0.000325267481, 0.000393079503,\n",
            "        0.000232929573], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 496.4306819050619, Const: (0.023764405857864546, 0.026108275051814678, 1.6734939987061175, 0.4847038394274778, 0.21032257320084885, 0.19118598535319564), W00: -118.44598183075337\n",
            "Epoch 25000, Loss_Tot: 310.11758202311455, Const: (0.0011720353518105409, 0.0008692793343123562, 1.039396078883661, 0.29404928063912916, 0.1266011319897245, 0.15959340142431996), W00: -55.39575110700361\n",
            "Epoch 30000, Loss_Tot: 36.5807323737532, Const: (0.0020740134559713574, 0.05713562587255594, 0.01508204490456802, 0.003549792892219319, 0.0013133665671079963, 0.012297093638312709), W00: -2.3807344976165448\n",
            "Epoch 35000, Loss_Tot: 1.8458666756168698, Const: (-9.058094802449901e-05, -0.0002199682046406526, 0.0010892669129742055, 0.0009597134548265571, 0.000764324804122142, 0.0010489521106312454), W00: -1.834297761120914\n",
            "\n",
            "Minimum Cons: 0.000004095205857964533941 at epoch 29919\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.7475484505\n",
            "W10 diff: 0.0000448051\n",
            "W01 diff: -0.0002070448\n",
            "Comp3: 0.0062457252986729\n",
            "Comp4: 0.0015643039529104\n",
            "Comp5: 0.0007598086104364\n",
            "Comp6: 0.0193318480976677\n",
            "Coeffs: tensor([9.726389049516e-01, 3.896906939056e-01, 1.413961272574e-01,\n",
            "        1.138167330353e-01, 9.455482424014e-02, 6.176348819704e-02,\n",
            "        3.694708558867e-02, 5.147293132466e-02, 3.728716953597e-02,\n",
            "        1.700610539391e-02, 6.823557093187e-03, 4.462119388174e-03,\n",
            "        2.848119177175e-03, 8.930731606203e-03, 2.887923155629e-03,\n",
            "        1.280074168111e-03, 5.643621530567e-04, 2.322620312912e-04,\n",
            "        1.285366813880e-04, 2.891007934137e-03, 4.038941640479e-04,\n",
            "        1.315101629555e-04, 5.640242023884e-05, 2.369624047523e-05,\n",
            "        1.095211786760e-03, 1.106822721732e-04, 2.392852480355e-05,\n",
            "        7.316547944306e-06, 2.704309006433e-06, 4.022762969712e-04,\n",
            "        4.220910855015e-05, 5.182571638533e-06, 1.528139746581e-06,\n",
            "        1.485744389394e-04, 1.710110235147e-05, 1.470925086947e-06,\n",
            "        3.284840433070e-07, 5.495323420332e-05, 6.939984069010e-06,\n",
            "        5.495973819386e-07, 1.726931382458e-05, 2.550152873124e-06,\n",
            "        2.096155838058e-07, 5.253928868405e-06, 8.411882882891e-07,\n",
            "        1.593291779561e-06, 2.616857197623e-07, 4.792953909613e-07,\n",
            "        1.432915060061e-07], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 361.26646925290214, Const: (0.12830235239862486, -0.12241536856012947, 0.030373822836556732, 0.0067970314246149155, 0.002842018219243885, 0.06701677263479171), W00: -1.883830201735904\n",
            "Epoch 45000, Loss_Tot: 25.835843811558107, Const: (-0.006013480296319429, -0.006276769950227967, 0.02411151255373817, 0.0062124520076488445, 0.0028367177483174205, 0.04806056632502238), W00: -1.982065593916286\n",
            "Epoch 50000, Loss_Tot: 7.515441372220101, Const: (5.7591136543022614e-05, -0.00022776969093163402, 0.010512355272435993, 0.0032435558570454194, 0.0021250169510598537, 0.024017509176595467), W00: -1.7464819440304726\n",
            "Epoch 55000, Loss_Tot: 1.616515523264579, Const: (-1.1222755208262214e-05, -8.285407028685299e-05, 0.0008733403491231646, 0.000770412587268787, 0.0007356854035628602, 0.0017798971641376897), W00: -1.5847652766435494\n",
            "\n",
            "Minimum Cons: 0.000000036054550823113481 at epoch 58978\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5820584943\n",
            "W10 diff: -0.0000002618\n",
            "W01 diff: -0.0000338313\n",
            "Comp3: 0.0005517067406325\n",
            "Comp4: 0.0005126900288183\n",
            "Comp5: 0.0004283578601263\n",
            "Comp6: 0.0010656839109146\n",
            "Coeffs: tensor([9.858309338812e-01, 2.340066089695e-01, 3.765785865417e-02,\n",
            "        5.162267689506e-03, 7.770393615544e-04, 1.149500457876e-04,\n",
            "        1.604686637467e-05, 6.212531030727e-02, 2.300529885680e-02,\n",
            "        3.011491187024e-03, 3.521413620717e-04, 4.033875175955e-05,\n",
            "        5.952832376529e-06, 1.106093539409e-02, 1.667653280868e-03,\n",
            "        1.956585883613e-04, 2.393258473625e-05, 3.025816232139e-06,\n",
            "        3.825529237817e-07, 2.812229104370e-03, 2.869557959970e-04,\n",
            "        2.645746960023e-05, 2.721439780908e-06, 3.053974088916e-07,\n",
            "        8.492082267629e-04, 6.659404325347e-05, 5.914963042333e-06,\n",
            "        5.483556544715e-07, 5.574059298678e-08, 2.556480673788e-04,\n",
            "        1.450163109482e-05, 1.255585866162e-06, 1.178041358471e-07,\n",
            "        6.055886171157e-05, 3.476769343153e-06, 2.640652043664e-07,\n",
            "        2.547302258126e-08, 1.370218758004e-05, 8.298405568050e-07,\n",
            "        5.527065777296e-08, 3.090923095449e-06, 1.999635550278e-07,\n",
            "        1.184414576879e-08, 6.972438723074e-07, 4.833217305414e-08,\n",
            "        1.573064511578e-07, 1.168670232674e-08, 3.538004423107e-08,\n",
            "        7.935946589517e-09], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 60000, Loss_Tot: 1.5880293763862496, Const: (2.313719086100008e-06, -6.303150783826261e-05, 0.00045852399131727795, 0.0004829893304468825, 0.0004004740124584536, 0.000946833318219651), W00: -1.5790246598185775\n",
            "Epoch 65000, Loss_Tot: 1.5709293854702835, Const: (0.0005920025925829631, -0.0006155143605084401, 0.0002792965725450876, 0.0003067863430397251, 0.0003825416856930718, 0.0007881573246303188), W00: -1.5574242158104286\n",
            "Epoch 70000, Loss_Tot: 1.5510747845540553, Const: (-2.1321197164692052e-05, -9.674886793287207e-05, 0.00030001537483083336, 0.0004125930931480339, 0.0005283760633148991, 0.0006154155314105896), W00: -1.5471892724220933\n",
            "Epoch 75000, Loss_Tot: 1.6213011021022594, Const: (0.0018934942485135942, -0.0019433876644330272, 0.0002736023082180299, 0.000497332538203564, 0.0005641429740778108, 0.0007354789196975109), W00: -1.5422710488548201\n",
            "\n",
            "Minimum Cons: 0.000000009843929742848726 at epoch 76809\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5451474403\n",
            "W10 diff: 0.0000085335\n",
            "W01 diff: -0.0000127245\n",
            "Comp3: 0.0003316455978984\n",
            "Comp4: 0.0004763308669642\n",
            "Comp5: 0.0005871511920594\n",
            "Comp6: 0.0006425097990759\n",
            "Coeffs: tensor([9.916267972224e-01, 1.369078724075e-01, 9.312272187058e-03,\n",
            "        2.679325061015e-04, 1.089870395333e-05, 4.167776325902e-07,\n",
            "        1.405197083824e-08, 6.526532454259e-02, 1.847708334549e-02,\n",
            "        5.030630397819e-04, 1.270917133753e-05, 3.327399575267e-07,\n",
            "        1.316409712364e-08, 1.144240956815e-02, 9.151263857151e-04,\n",
            "        2.760792942767e-05, 9.058870839464e-07, 3.146556209846e-08,\n",
            "        1.073469484874e-09, 2.817481302906e-03, 1.443742512568e-04,\n",
            "        4.388201373351e-06, 1.423214455890e-07, 4.750916184843e-09,\n",
            "        8.635770814274e-04, 2.779313740126e-05, 8.169882937664e-07,\n",
            "        2.577797309276e-08, 8.144845249949e-10, 3.118280185901e-04,\n",
            "        5.300561866888e-06, 1.397828048898e-07, 4.613207594120e-09,\n",
            "        5.368599520489e-05, 9.600879518629e-07, 2.440936091463e-08,\n",
            "        7.308655572164e-10, 9.190352110461e-06, 1.746999898509e-07,\n",
            "        3.750225776631e-09, 1.573241106342e-06, 3.191296764471e-08,\n",
            "        5.970672266793e-10, 2.693128332952e-07, 5.927904041834e-09,\n",
            "        4.610187382800e-08, 1.137720068061e-09, 7.790573593134e-09,\n",
            "        1.311575188743e-09], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 80000, Loss_Tot: 1.5486682354056176, Const: (-0.00025614169091725536, 0.00014013422355962923, 0.00034134291068760775, 0.0005144712899306492, 0.0006249430802618896, 0.0006500382764303153), W00: -1.5435902761329865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YICKmzJXxYo_",
        "outputId": "9a2d3682-2462-458f-eab5-948d256a1e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.5492168815899061, Const: (0.0002749127028447429, -0.0003767118823196913, 0.0003284241872289632, 0.0005215938616544739, 0.000620932793352143, 0.0006533818313309842), W00: -1.5427729150501095\n",
            "Epoch 5000, Loss_Tot: 1.551239505337353, Const: (0.00048113463009236135, -0.0005662474536110018, 0.00033981447701311095, 0.0005750269624871248, 0.0006603907002091663, 0.000667816419364008), W00: -1.541258450527681\n",
            "Epoch 10000, Loss_Tot: 1.546118847902478, Const: (-8.228296329182427e-05, -3.328869339314089e-05, 0.0003706741784292797, 0.0005935079135301533, 0.0006925614730263987, 0.0006839796480041044), W00: -1.541361780082081\n",
            "Epoch 15000, Loss_Tot: 1.5450670773388095, Const: (-0.00017804064299031808, 5.2571607609186444e-05, 0.00039282780478444676, 0.0006362299765821588, 0.0007411037978903055, 0.0007096387847567669), W00: -1.5396865828456687\n",
            "\n",
            "Minimum Cons: 0.000000002503893295150918 at epoch 18513\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5399421222\n",
            "W10 diff: 0.0000034630\n",
            "W01 diff: 0.0000008047\n",
            "Comp3: 0.0004147001366925\n",
            "Comp4: 0.0006451988750685\n",
            "Comp5: 0.0007452112543981\n",
            "Comp6: 0.0007042771279500\n",
            "Coeffs: tensor([9.918160987263e-01, 1.358910380580e-01, 7.856602234804e-04,\n",
            "        2.809618281047e-06, 1.721305513769e-08, 1.428835640238e-10,\n",
            "        9.762890371168e-13, 6.537886598915e-02, 1.892128173638e-02,\n",
            "        8.257780700482e-05, 3.135173128863e-07, 1.574936059605e-09,\n",
            "        1.034682411627e-11, 1.155600447771e-02, 5.836391768266e-04,\n",
            "        6.803617163071e-06, 5.304600634034e-08, 3.504634087131e-10,\n",
            "        2.278180363162e-12, 2.827553836831e-03, 8.982121951149e-05,\n",
            "        8.900754997429e-07, 1.118313826087e-08, 9.042095932540e-11,\n",
            "        8.670617717226e-04, 1.769847586806e-05, 1.691487479295e-07,\n",
            "        1.618375482663e-09, 1.763606328846e-11, 3.237509081697e-04,\n",
            "        2.545471472121e-06, 2.277930843111e-08, 3.179760759432e-10,\n",
            "        4.178866511148e-05, 3.548360294938e-07, 2.776118485993e-09,\n",
            "        3.639996705505e-11, 4.790813851679e-06, 4.701682895585e-08,\n",
            "        3.376122050172e-10, 5.492283946480e-07, 6.248800357591e-09,\n",
            "        4.780756666939e-11, 6.190082180341e-08, 8.305006119305e-10,\n",
            "        6.877495251457e-09, 1.103781886645e-10, 7.641245850874e-10,\n",
            "        8.489811458973e-11], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 1.5452616547480549, Const: (-0.00031628253583293464, 0.00018399680983449684, 0.0004130025627077951, 0.0006771576387075686, 0.0007810484560990913, 0.0007378995204057353), W00: -1.5384778030408828\n",
            "Epoch 25000, Loss_Tot: 1.5487454988953857, Const: (-0.0005632872483385398, 0.0004271388444734292, 0.0004282463082311945, 0.0007070055355666386, 0.0008251715613311699, 0.000764198913524494), W00: -1.5379080979350765\n",
            "Epoch 30000, Loss_Tot: 1.543058934516776, Const: (-0.0001645442122542029, 7.700917744513092e-05, 0.00044577700621279873, 0.0007485256126571555, 0.0008735540361912335, 0.0007835131023013866), W00: -1.5365899545900252\n",
            "Epoch 35000, Loss_Tot: 1.5711717170537283, Const: (-0.0011866050869673295, 0.0010358142859654418, 0.00047766561834878646, 0.0007637601168551824, 0.000906840297211021, 0.0009498665096767157), W00: -1.5373398245174057\n",
            "\n",
            "Minimum Cons: 0.000000029386485192538002 at epoch 39464\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5360057122\n",
            "W10 diff: -0.0000361183\n",
            "W01 diff: -0.0000080145\n",
            "Comp3: 0.0004545324490519\n",
            "Comp4: 0.0008253711210603\n",
            "Comp5: 0.0009138069543976\n",
            "Comp6: 0.0007942979020074\n",
            "Coeffs: tensor([9.919085945448e-01, 1.348816538383e-01, 4.039406212182e-05,\n",
            "        2.274816606941e-08, 1.451225309956e-11, 1.132921318353e-14,\n",
            "        7.606434933296e-18, 6.544026086067e-02, 1.910195100030e-02,\n",
            "        6.411477006383e-06, 2.742510118338e-09, 1.415241198219e-12,\n",
            "        1.070950298869e-15, 1.164122643722e-02, 3.432007433112e-04,\n",
            "        8.513409068561e-07, 6.645554942059e-10, 4.520244878847e-13,\n",
            "        3.610734628995e-16, 2.835405320335e-03, 5.487812991990e-05,\n",
            "        1.117920811565e-07, 1.964650502378e-10, 1.725682829463e-13,\n",
            "        8.666589704538e-04, 8.875053016407e-06, 2.326342159641e-08,\n",
            "        4.284506444740e-11, 4.807952336792e-14, 3.321143251319e-04,\n",
            "        8.085264230012e-07, 2.382538933090e-09, 9.980465150123e-12,\n",
            "        2.355446711498e-05, 7.172252586980e-08, 1.880787481029e-10,\n",
            "        8.373053354298e-13, 1.489846137909e-06, 6.371455122370e-09,\n",
            "        1.542267551529e-11, 1.032710210173e-07, 5.684346096188e-10,\n",
            "        1.396607232181e-12, 7.126551476019e-09, 5.071336113223e-11,\n",
            "        4.917907576302e-10, 4.524434214952e-12, 3.393761335443e-11,\n",
            "        2.341974878704e-12], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 1.5492236426266528, Const: (0.0005803999332369969, -0.000673626208530953, 0.0004307859140305366, 0.000797213553449161, 0.0009047246078681581, 0.0008019944566065536), W00: -1.5348853280291634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACLco7XjyHtK",
        "outputId": "0cd8f067-017d-4f03-87b2-09a497915c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.5498305205866723, Const: (-0.0006702324052028086, 0.00048158839522716157, 0.00045418775370276187, 0.0007841381520005983, 0.0009133735561466511, 0.0008071493786082759), W00: -1.5365042307987808\n",
            "Epoch 5000, Loss_Tot: 1.5569483352814342, Const: (-0.0009100440782778829, 0.0007506890579946912, 0.00048449500791958945, 0.0008345487937472614, 0.0009701743192856769, 0.0008111886421651941), W00: -1.5364509222876401\n",
            "Epoch 10000, Loss_Tot: 1.8231990711288457, Const: (-4.9628095678988515e-05, -0.0001373302320211156, 0.0020623886042948456, 0.0015770777124773326, 0.0016027583150132845, 0.005724190679839145), W00: -1.4953222563321955\n",
            "Epoch 15000, Loss_Tot: 1.7827462603017277, Const: (-4.241554295991712e-05, -0.00012777266132912857, 0.0017073403047778825, 0.0014598777736435974, 0.0012876185626245706, 0.005357779238838889), W00: -1.4955070272677202\n",
            "\n",
            "Minimum Cons: 0.000000050270080597097319 at epoch 19638\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5156175985\n",
            "W10 diff: -0.0000272379\n",
            "W01 diff: -0.0000325981\n",
            "Comp3: 0.0004979703314840\n",
            "Comp4: 0.0003970143688901\n",
            "Comp5: 0.0003009423575423\n",
            "Comp6: 0.0011833870960712\n",
            "Coeffs: tensor([9.971736250872e-01, 5.388460569762e-02, 1.059291083190e-06,\n",
            "        1.192793842975e-10, 1.520328467181e-14, 2.133268028766e-18,\n",
            "        3.489552910083e-22, 7.029566382580e-02, 2.482666063901e-03,\n",
            "        1.150450121228e-07, 1.398086404794e-11, 2.200309036959e-15,\n",
            "        4.116410991655e-19, 1.199648765056e-02, 5.357321916544e-05,\n",
            "        2.218860981554e-08, 3.660478085653e-12, 5.741403972806e-16,\n",
            "        1.030494843168e-19, 3.031514758131e-03, 6.237544364539e-06,\n",
            "        3.142308715823e-09, 1.230897547536e-12, 2.218805847766e-16,\n",
            "        9.297378871967e-04, 7.708287188312e-07, 4.577428137170e-10,\n",
            "        3.002137719425e-13, 6.944667745284e-17, 2.935016432682e-04,\n",
            "        9.373966274653e-08, 4.523705802615e-11, 5.180267867105e-14,\n",
            "        7.825307289754e-05, 1.200566260504e-08, 4.300885795221e-12,\n",
            "        5.011902863685e-15, 8.312495258672e-06, 1.516549385768e-09,\n",
            "        2.602316192755e-13, 7.641692592679e-07, 1.890879126257e-10,\n",
            "        2.321854692633e-14, 6.938637962903e-08, 2.357604639580e-11,\n",
            "        6.300264109019e-09, 2.939531966364e-12, 5.720622248523e-10,\n",
            "        5.194309041463e-11], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 1.5266718398421175, Const: (3.814341611607652e-05, -4.8924621401491564e-05, 0.00046158445697017797, 0.0002583082954349496, 0.000345655999795518, 0.0009533276005200135), W00: -1.5175450193152622\n",
            "Epoch 25000, Loss_Tot: 1.5348102106929207, Const: (0.0008963538200077448, -0.0007282058135711367, 0.00048626692204749604, 0.0005123273071159149, 0.0008270470327371874, 0.001237410282888932), W00: -1.5061610298353152\n",
            "Epoch 30000, Loss_Tot: 1.526943148126624, Const: (0.0007410086937371663, -0.0006168448973724594, 0.0003865032368155876, 0.0005197625195798818, 0.0007494516015806038, 0.001132107665036678), W00: -1.504830555358191\n",
            "Epoch 35000, Loss_Tot: 1.5300018225507295, Const: (0.0008574175666993256, -0.0006990488240841142, 0.00035181413553741586, 0.0004247846043390565, 0.0006464473586976932, 0.0011371698367701692), W00: -1.5048319287527507\n",
            "\n",
            "Minimum Cons: 0.000000004000680242045077 at epoch 34882\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5064015362\n",
            "W10 diff: -0.0000036305\n",
            "W01 diff: 0.0000023291\n",
            "Comp3: 0.0003408570531064\n",
            "Comp4: 0.0004366116292422\n",
            "Comp5: 0.0006496100880704\n",
            "Comp6: 0.0009274996556952\n",
            "Coeffs: tensor([9.991221289234e-01, 1.814188063382e-02, 8.757243518527e-08,\n",
            "        2.565015554777e-12, 7.861090857548e-17, 3.430090937957e-21,\n",
            "        1.534953102508e-25, 7.132653123452e-02, 3.133794068183e-04,\n",
            "        6.832119608440e-09, 2.401243306104e-13, 1.133310337236e-17,\n",
            "        6.341632050873e-22, 1.206697548651e-02, 7.750371355809e-06,\n",
            "        1.343568121162e-09, 6.242785519127e-14, 2.998003543170e-18,\n",
            "        1.491440621602e-22, 2.997946054654e-03, 9.282158636788e-07,\n",
            "        1.553106408603e-10, 1.726519347380e-14, 9.286447730919e-19,\n",
            "        9.216933000598e-04, 9.809209068396e-08, 2.475841730234e-11,\n",
            "        3.484165150092e-15, 2.254027525708e-19, 3.377425021048e-04,\n",
            "        1.280001497597e-08, 2.429139220326e-12, 6.013110729044e-16,\n",
            "        7.000030449311e-05, 1.555693809904e-09, 2.136710386274e-13,\n",
            "        6.541234683221e-17, 5.849970385334e-06, 1.862197358297e-10,\n",
            "        1.301270623200e-14, 4.889311837822e-07, 2.229138510098e-11,\n",
            "        1.017419791397e-15, 4.086398725065e-08, 2.668384462423e-12,\n",
            "        3.415337713306e-09, 3.194182688506e-13, 2.854477078858e-10,\n",
            "        2.385719968569e-11], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 1.5161788439582515, Const: (0.0002969440969053938, -0.00023186350802695266, 0.000341186218363766, 0.00045057930834298954, 0.000612493921254642, 0.000899093111010109), W00: -1.506675794905178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMGCbsu210wx",
        "outputId": "6a5622c9-6a90-4a70-dc22-2affdef27220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.5181678110290284, Const: (-0.0003989440875591388, 0.0003119786153116255, 0.0003326123265898206, 0.0004469424472744337, 0.0006145542443029353, 0.0008926947766853837), W00: -1.5076339009717132\n",
            "Epoch 5000, Loss_Tot: 1.5426086121671359, Const: (-0.0010941706612088709, 0.0008372349732261863, 0.0003150029629509764, 0.0004136957651763728, 0.0005962707908549695, 0.0012346969837197586), W00: -1.5083821273886362\n",
            "Epoch 10000, Loss_Tot: 1.5264134111596583, Const: (0.0007549073720320987, -0.0006514260895003066, 0.0003268598460253454, 0.00038387546096520534, 0.0005749518553034035, 0.0010677151351602312), W00: -1.5050708441568554\n",
            "Epoch 15000, Loss_Tot: 1.5151652313071682, Const: (-0.0001228999105817863, 0.00011527815049561241, 0.00034126944153142985, 0.00042855771901023594, 0.000569793728655396, 0.0008514729609372689), W00: -1.5076312348750684\n",
            "\n",
            "Minimum Cons: 0.000000119465702296387439 at epoch 852\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5058554152\n",
            "W10 diff: 0.0000659313\n",
            "W01 diff: -0.0001142664\n",
            "Comp3: 0.0003249811121267\n",
            "Comp4: 0.0003882744324870\n",
            "Comp5: 0.0006090633276153\n",
            "Comp6: 0.0009055695312464\n",
            "Coeffs: tensor([9.991851517012e-01, 1.595073917850e-02, 4.812061357023e-08,\n",
            "        9.164539659767e-13, 1.884709860999e-17, 5.401086578874e-22,\n",
            "        1.540641116210e-26, 7.135649786974e-02, 1.798413465254e-04,\n",
            "        3.265151204442e-09, 7.503265412659e-14, 2.291464678892e-18,\n",
            "        8.317124482434e-23, 1.205620743436e-02, 5.212762630452e-06,\n",
            "        5.444503337005e-10, 1.789804048500e-14, 5.654639282899e-19,\n",
            "        1.821195205813e-23, 2.998822316448e-03, 6.259014714817e-07,\n",
            "        6.223383037972e-11, 4.236003114482e-15, 1.652850700927e-19,\n",
            "        9.227174669764e-04, 6.459921926291e-08, 1.015952488463e-11,\n",
            "        8.136379880155e-16, 3.259560450669e-20, 3.350643692816e-04,\n",
            "        7.974085337817e-09, 9.878155101579e-13, 1.478981133331e-16,\n",
            "        7.404424248923e-05, 8.880384326860e-10, 8.493418787338e-14,\n",
            "        1.618003291138e-17, 5.824553281213e-06, 1.014622510649e-10,\n",
            "        4.931220764980e-15, 4.581764388610e-07, 1.159250321682e-11,\n",
            "        3.742910204813e-16, 3.604141296865e-08, 1.324493882368e-12,\n",
            "        2.835115658501e-09, 1.513291833187e-13, 2.230179122309e-10,\n",
            "        1.754319580114e-11], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 1.5150207216510707, Const: (0.0002036996945697922, -0.00024695237510319323, 0.00031789920673267244, 0.0003802087968141848, 0.0005544211526188383, 0.0008882892639104251), W00: -1.506105353075916\n",
            "Epoch 25000, Loss_Tot: 1.519005549272479, Const: (0.00046385485373123814, -0.00045827536242870615, 0.00032138884629998764, 0.0003784426582486532, 0.0005500236340533323, 0.0009581267804038993), W00: -1.5055737036678156\n",
            "Epoch 30000, Loss_Tot: 1.5174013947919283, Const: (-0.00039509253850011916, 0.0002953066522646264, 0.0003032762228326685, 0.0003675437358256116, 0.0005412918773183633, 0.0008832137444758139), W00: -1.5071676882791163\n",
            "Epoch 35000, Loss_Tot: 1.5136508798840076, Const: (-5.565391698869604e-05, 2.0960718603246775e-05, 0.00030850496020975836, 0.00037080301835511794, 0.0005431407686826747, 0.0008357163574589083), W00: -1.5066312944807583\n",
            "\n",
            "Minimum Cons: 0.000000044680976147671817 at epoch 38985\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5059112100\n",
            "W10 diff: 0.0000028889\n",
            "W01 diff: -0.0000521052\n",
            "Comp3: 0.0003015702274018\n",
            "Comp4: 0.0003531691260571\n",
            "Comp5: 0.0005452639944394\n",
            "Comp6: 0.0008562004316723\n",
            "Coeffs: tensor([9.993667472672e-01, 1.346706541218e-02, 6.401434284304e-09,\n",
            "        1.875733941401e-14, 6.377081992302e-20, 3.057888592934e-25,\n",
            "        1.472855700503e-30, 7.145293988770e-02, 4.324204708594e-05,\n",
            "        2.271398388500e-10, 9.276835460277e-16, 4.687418072893e-21,\n",
            "        2.876423055655e-26, 1.207753591305e-02, 1.524076702881e-06,\n",
            "        1.957959273852e-11, 2.118198457604e-16, 1.050828347060e-21,\n",
            "        5.595759864956e-27, 3.002819915860e-03, 1.842276191040e-07,\n",
            "        2.225452592778e-12, 2.830674500808e-17, 2.986760413211e-22,\n",
            "        9.220474891228e-04, 1.888464034004e-08, 3.708219112511e-13,\n",
            "        3.500283284190e-18, 4.443037398398e-23, 3.319760219835e-04,\n",
            "        2.462606791682e-09, 3.528472557117e-14, 6.574658560685e-19,\n",
            "        8.203064522224e-05, 1.465015544226e-10, 2.935151492456e-15,\n",
            "        7.183060352062e-20, 4.219570932112e-06, 1.074704582353e-11,\n",
            "        1.318289890497e-16, 2.170423228737e-07, 8.729138761279e-13,\n",
            "        8.012748103926e-18, 1.116399777401e-08, 7.090121765973e-14,\n",
            "        5.742420762826e-10, 5.758852967178e-15, 2.953726498269e-11,\n",
            "        1.519307028284e-12], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 1.5638545947363736, Const: (-0.0014170705015221774, 0.0015053152105899414, 0.0003085766736359085, 0.0003552444082433052, 0.0004906146904856936, 0.0010141387943459073), W00: -1.5108291928992224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solTensor = torch.tensor([9.993667472672e-01, 1.346706541218e-02, 6.401434284304e-09,\n",
        "        1.875733941401e-14, 6.377081992302e-20, 3.057888592934e-25,\n",
        "        1.472855700503e-30, 7.145293988770e-02, 4.324204708594e-05,\n",
        "        2.271398388500e-10, 9.276835460277e-16, 4.687418072893e-21,\n",
        "        2.876423055655e-26, 1.207753591305e-02, 1.524076702881e-06,\n",
        "        1.957959273852e-11, 2.118198457604e-16, 1.050828347060e-21,\n",
        "        5.595759864956e-27, 3.002819915860e-03, 1.842276191040e-07,\n",
        "        2.225452592778e-12, 2.830674500808e-17, 2.986760413211e-22,\n",
        "        9.220474891228e-04, 1.888464034004e-08, 3.708219112511e-13,\n",
        "        3.500283284190e-18, 4.443037398398e-23, 3.319760219835e-04,\n",
        "        2.462606791682e-09, 3.528472557117e-14, 6.574658560685e-19,\n",
        "        8.203064522224e-05, 1.465015544226e-10, 2.935151492456e-15,\n",
        "        7.183060352062e-20, 4.219570932112e-06, 1.074704582353e-11,\n",
        "        1.318289890497e-16, 2.170423228737e-07, 8.729138761279e-13,\n",
        "        8.012748103926e-18, 1.116399777401e-08, 7.090121765973e-14,\n",
        "        5.742420762826e-10, 5.758852967178e-15, 2.953726498269e-11,\n",
        "        1.519307028284e-12], dtype=torch.float64)\n",
        "\n",
        "def CoeffFuncSol(solTensor, ell, n):\n",
        "    solTensor = solTensor.unsqueeze(1)  # Shape [49, 1]\n",
        "    combined_tensor = torch.cat((ell_n_tensor, solTensor), dim=1)\n",
        "    mask = (combined_tensor[:, 0] == ell) & (combined_tensor[:, 1] == n)\n",
        "    # Check if there is an entry\n",
        "    if mask.any():\n",
        "        return combined_tensor[mask, 2].item()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "CoeffFuncSol(solTensor,0,1),CoeffFuncSol(solTensor,1,2),CoeffFuncSol(solTensor,2,3),CoeffFuncSol(solTensor,3,4),CoeffFuncSol(solTensor,4,5),CoeffFuncSol(solTensor,5,6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL6ytEKz-ovx",
        "outputId": "3c74192e-7911-4125-f926-a0189100abb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9993667472672"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CoeffFuncSol(solTensor,0,1),CoeffFuncSol(solTensor,1,2),CoeffFuncSol(solTensor,2,3),CoeffFuncSol(solTensor,3,4),CoeffFuncSol(solTensor,4,5),CoeffFuncSol(solTensor,5,6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxtIx6OMjNm8",
        "outputId": "42e8e21f-8c05-4a5a-ca2c-abc88a4e6812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9993667472672,\n",
              " 0.0714529398877,\n",
              " 0.01207753591305,\n",
              " 0.00300281991586,\n",
              " 0.0009220474891228,\n",
              " 0.0003319760219835)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTRr_Aax5Kub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section With some null constraints, and multi-lambda equality constraints (Using product of cons as the strategy), FINAL IMPROVEMENTS"
      ],
      "metadata": {
        "id": "b1zEe6CejZc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755903b3-c681-43fd-9a7f-03b2228cc89c",
        "id": "8tLoXvlDjZdK"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.500000000000, -0.200000000000],\n",
              "        [-5.500000000000,  0.163636363636],\n",
              "        [-4.490000000000, -0.200000000000],\n",
              "        [-4.490000000000,  0.163636363636],\n",
              "        [-3.480000000000, -0.200000000000],\n",
              "        [-3.480000000000,  0.163636363636],\n",
              "        [-2.470000000000, -0.200000000000],\n",
              "        [-2.470000000000,  0.163636363636],\n",
              "        [-1.460000000000, -0.200000000000],\n",
              "        [-1.460000000000,  0.163636363636],\n",
              "        [-0.450000000000, -0.200000000000],\n",
              "        [-0.450000000000,  0.163636363636],\n",
              "        [ 0.560000000000, -0.200000000000],\n",
              "        [ 0.560000000000,  0.163636363636],\n",
              "        [ 1.570000000000, -0.200000000000],\n",
              "        [ 1.570000000000,  0.163636363636],\n",
              "        [ 2.580000000000, -0.200000000000],\n",
              "        [ 2.580000000000,  0.163636363636],\n",
              "        [ 3.590000000000, -0.200000000000],\n",
              "        [ 3.590000000000,  0.163636363636],\n",
              "        [ 4.600000000000, -0.200000000000],\n",
              "        [ 4.600000000000,  0.163636363636]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#THIS SECTION IS JUST TO TEST THE EXPRESSIONS FOR THE OPEN STRING AMPLITUDE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.special import legendre\n",
        "from scipy import integrate\n",
        "from scipy.special import hyp2f1 as scipy_hyp2f1\n",
        "from scipy.special import gamma as scipy_gamma\n",
        "from scipy.special import zeta\n",
        "#This is all for to check formulaes using the Open String Amplitude\n",
        "#Some definitions\n",
        "d = 10;\n",
        "alpha = (d-3)/2;\n",
        "lm = 56/10;\n",
        "dlm = 1/2\n",
        "lmp = lm + dlm\n",
        "lmm = lm - dlm\n",
        "ellmax = 12;\n",
        "w00 = -(np.pi)**2/6\n",
        "w10 = -zeta(3)\n",
        "w01 = 7*np.pi**4/360\n",
        "\n",
        "torch.set_printoptions(precision=12)\n",
        "\n",
        "def hyp2f1(a, b, c, z):\n",
        "    if isinstance(z, torch.Tensor):\n",
        "        z_np = z.detach().numpy()\n",
        "        result = scipy_hyp2f1(a,b,c,z_np)\n",
        "        return torch.tensor(result, dtype=z.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_hyp2f1(a,b,c,z))\n",
        "\n",
        "def gamma(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x_np = x.detach().numpy()\n",
        "        result = scipy_gamma(x_np)\n",
        "        return torch.tensor(result, dtype=x.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_gamma(x))\n",
        "\n",
        "def ker(lm,n,s1,s2):\n",
        "    return (1/(s1 - n) + 1/(s2 - n) + 1/(n + lm))\n",
        "\n",
        "def geg(ell,beta, z):\n",
        "    return gamma(ell + 2*beta) / (gamma(2*beta) * gamma(ell + 1)) * \\\n",
        "           hyp2f1(-ell, ell + 2*beta, beta + 1/2, (1-z)/2)\n",
        "\n",
        "#ell,n tensor and Null grid tensor\n",
        "ell = torch.arange(ellmax + 1, dtype = torch.float64).unsqueeze(1).repeat(1, ellmax + 1)\n",
        "n = torch.arange(1, ellmax + 2, dtype = torch.float64).unsqueeze(0).repeat(ellmax + 1, 1)\n",
        "mask = (n > ell) & (n <= ellmax + 1) & ((n - ell - 1) % 2 == 0)\n",
        "ell_n_tensor = torch.stack([ell[mask], n[mask]], dim=1)\n",
        "\n",
        "NullPoints = torch.stack(torch.meshgrid(\n",
        "    torch.arange(-6 +1/2, 5+1/2 + 1e-6, 101/100, dtype=torch.float64),\n",
        "    torch.arange(-0.2, 0.2 + 1e-6, 4/11, dtype=torch.float64),\n",
        "    indexing='ij'\n",
        ")).permute(1, 2, 0).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def w00comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    result = (-1/n)*geg(ell,alpha,1)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def w10comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-((2 * ell * (-3 + d + ell) * lm * (n + 2 * lm) * \\\n",
        "            hyp2f1(1 - ell, -2 + d + ell, d / 2, lm / (n + lm))) / ((-2 + d) * (n + lm)**2)) - \\\n",
        "            hyp2f1(-ell, -3 + d + ell, 1/2 * (-2 + d), lm / (n + lm)))) / (n**2 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def w01comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-(1/((d - 2) * (n + lm)**2)) *\\\n",
        "             2 * ell * (-3 + d + ell) * n * (n + 2*lm) *  hyp2f1(1 - ell, -2 + d + ell, d/2, lm/(n + lm)) + \\\n",
        "             2 * hyp2f1(-ell, -3 + d + ell, (d - 2)/2, lm/(n + lm)))) / (n**3 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcomp(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lm,n, s1, s2) * geg(ell, alpha, 1 + 2 * (((s1 + lm) * (s2 + lm)) / (n + lm) - lm) / n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1P(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lmp,n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmp)*(s2 + lmp))/(n + lmp)-lmp)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1M(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lmm,n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmm)*(s2 + lmm))/(n + lmm)-lmm)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD1(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ((2 * (-3 + d) * (n**2 + 2 * n * lm - s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) /  (n * (n + lm))) / n - (n + lm) * geg(ell, 1/2 * (-3 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) / (n * (n + lm)))) / (n + lm)**3)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD2(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = (1 / (n + lm)**4 * 2 * (-((2 * (-3 + d) * (-1 + d) * (n - s1) * (n - s2) *\n",
        "               (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "               geg(-2 + ell,(d+1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) /\n",
        "              (n**2 * (n + lm))) + ((-3 + d) * (n - s1) * (n - s2) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            ((-3 + d) * (2*n - s1 - s2) * (n + lm) *\n",
        "             geg(-1 + ell, (d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n -\n",
        "            (3 * (-3 + d) * (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            (n + lm) * geg(ell,(d-3)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))))\n",
        "\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD3(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result =  (2 / (n**3 * (n + lm)**7)) * (4 * (-3 + d) * (-1 + d) * (1 + d) * (n - s1)**2 * (n - s2)**2 * (n**2 + 2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-3 + ell, (3 + d)/2, 1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * (-1 + d) * n * (n - s1)**2 * (n -s2)**2 * (n + lm) * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     4 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (2 * n - s1 -s2) * (n + lm)**2 * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     8 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (n + lm) * (n**2 +2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (n - s1) * (n -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (2 * n - s1 - s2) * (n + lm)**3 * geg(-1 + ell, 1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     n**3 * (n + lm)**3 * geg(ell, 1/2 * (-3 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +\n",
        "     4 * n * (n + lm) * (2 * (-3 + d) * (-1 + d) * (n - s1) * (n -s2) * (n**2 + 2 * n * lm - s2 * lm -\n",
        "     s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))\n",
        "     - (-3 + d) * n * (n - s1) * (n -s2) * (n + lm) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) - (-3 + d) * n * (2 * n - s1 -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +3 * (-3 + d) * n * (n + lm) * (n**2 + 2 * n * lm -\n",
        "      s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell,1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "        n**2 * (n + lm)**2 * geg(ell, 1/2 * (-3 + d),  1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "#First, lets calculate cc(ell,n) for open string\n",
        "\n",
        "def AbsOpString(n,s2):\n",
        "    return (-1)**(1 + n)*gamma(-s2)/(gamma(n+1)*gamma(1 - n - s2))\n",
        "\n",
        "\n",
        "\n",
        "def cc(ell_n_tensor):\n",
        "    z = torch.linspace(-1 + 10**(-6), 1 - 10**(-6), 10**6).view(1, -1)\n",
        "\n",
        "    # Directly use ell and n from ell_n_tensor\n",
        "    ell_values = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n_values = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "\n",
        "    # Compute normfac for all combinations of ell and n\n",
        "    normfac = ((np.pi * 2**(1 - 2 * alpha) * gamma(2 * alpha + ell_values)) /\n",
        "                (gamma(ell_values + 1) * (ell_values + alpha) * gamma(alpha)**2))**-1\n",
        "\n",
        "    # Compute integrand\n",
        "    integrand = normfac * (1 - z**2)**(alpha - 1/2) * \\\n",
        "                AbsOpString(n_values, n_values * (z - 1) / 2) * \\\n",
        "                geg(ell_values, alpha, z)\n",
        "\n",
        "    # Integrate across z\n",
        "    return torch.trapz(integrand, z, dim=1)\n",
        "\n",
        "NullPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6d522b-d066-4527-e8b0-414d24018c7d",
        "id": "r632GBLvjZdQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-1ea80a7cb81f>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(result, dtype=z.dtype)\n",
            "<ipython-input-1-1ea80a7cb81f>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(scipy_hyp2f1(a,b,c,z))\n"
          ]
        }
      ],
      "source": [
        "#Store all the values\n",
        "coeff_vals = cc(ell_n_tensor)\n",
        "w00comp_vals = w00comp(ell_n_tensor)\n",
        "w01comp_vals = w01comp(ell_n_tensor)\n",
        "w10comp_vals = w10comp(ell_n_tensor)\n",
        "\n",
        "ampcomp_vals = ampcomp(ell_n_tensor, NullPoints)\n",
        "ampcompD1P_vals = ampcompD1P(ell_n_tensor, NullPoints)\n",
        "ampcompD1M_vals = ampcompD1M(ell_n_tensor, NullPoints)\n",
        "ampcompD1_vals = ampcompD1(ell_n_tensor, NullPoints)\n",
        "ampcompD2_vals = ampcompD2(ell_n_tensor, NullPoints)\n",
        "ampcompD3_vals = ampcompD3(ell_n_tensor, NullPoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad429fdf-5a8f-4236-f1be-982719f1d163",
        "id": "A8jTwsSljZdR"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.6449340668482264,\n",
              " -1.5708937983031839,\n",
              " -1.2020569031595942,\n",
              " -1.2020580619524968,\n",
              " 1.8940656589944915,\n",
              " 1.8940654448676988,\n",
              " -0.5624502228752933,\n",
              " 3.839619282262798e-07,\n",
              " 1.7739561382799387e-06,\n",
              " -3.90493232622108e-06,\n",
              " -0.5624498928853159,\n",
              " -0.5624501257869979,\n",
              " 0.9999995859158504)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#CHECKS FOR OPEN STRING\n",
        "def w00fullX():\n",
        "    return torch.dot(coeff_vals,w00comp_vals)\n",
        "\n",
        "def w10fullX():\n",
        "    return torch.dot(coeff_vals,w10comp_vals)\n",
        "\n",
        "def w01fullX():\n",
        "    return torch.dot(coeff_vals,w01comp_vals)\n",
        "\n",
        "def ampfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1PfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1MfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD3_vals, dim = 0)\n",
        "\n",
        "(w00, w00fullX().item(),w10, w10fullX().item(),w01, w01fullX().item(),ampfullX()[1].item(),\n",
        " ampD1fullX()[1].item(),ampD2fullX()[1].item(),ampD3fullX()[1].item(),\n",
        " ampD1PfullX()[1].item(),ampD1MfullX()[1].item(),(ampD1PfullX()/ampD1MfullX())[1].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mKi7psbjZdX"
      },
      "outputs": [],
      "source": [
        "# Same definitions for the PINN\n",
        "def w00full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w00comp_vals)\n",
        "\n",
        "def w01full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w01comp_vals)\n",
        "\n",
        "def w10full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w10comp_vals)\n",
        "\n",
        "def ampfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1Pfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1Mfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD3_vals, dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network. We will impose multi-lambda equality. But lets first check how well\n",
        "# this is satisfied with three derivatives imposed.\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.0001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "\n",
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c15de1-c1ce-4ee9-c24c-ef489af1274f",
        "id": "oyR4Q5jHjZdX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 256051.46406119806, Const: (0.5501998547284257, -3.330277994707946, 11.531696177996201, 6.641633703692179, 6.846140015201417, 3.718215594078753), W00: -3865.47799890233\n",
            "Epoch 5000, Loss_Tot: 115461.14346657263, Const: (1.1077462973768828, -2.1293593164188716, 4.9850537418645064, 3.1946200801028137, 3.1111240331200585, 2.377507358527765), W00: -1323.0014903155447\n",
            "Epoch 10000, Loss_Tot: 79670.71874190436, Const: (1.169346255045514, -2.0491072583485224, 4.902542271434544, 2.661720646191805, 2.456857629849814, 1.509204586337057), W00: -1231.6217036406229\n",
            "Epoch 15000, Loss_Tot: 78422.08982242925, Const: (1.1700610120608235, -2.034091710996785, 4.8728251845364605, 2.6360227504281473, 2.427846909229025, 1.4878560322292216), W00: -1219.215489114151\n",
            "\n",
            "Minimum Cons: 4.393467870442067102487727 at epoch 5055\n",
            "Values at minimum cons epoch:\n",
            "W00: -1102.1174269535\n",
            "W10 diff: 1.2026196524\n",
            "W01 diff: -1.8668261360\n",
            "Comp3: 4.5572307133983543\n",
            "Comp4: 2.3865053290437213\n",
            "Comp5: 2.1630288088915028\n",
            "Comp6: 1.9784507882573852\n",
            "Coeffs: tensor([0.392558567301, 0.264749479048, 0.185112944589, 0.118429212648,\n",
            "        0.075367306231, 0.047588206821, 0.029617367871, 0.330108544704,\n",
            "        0.312541515799, 0.261078401812, 0.183474652403, 0.118885124306,\n",
            "        0.076072884565, 0.310195039943, 0.305698761998, 0.335361923214,\n",
            "        0.290957547333, 0.197558782701, 0.128239983421, 0.279787874258,\n",
            "        0.252610291722, 0.285766682745, 0.322411778485, 0.299315059849,\n",
            "        0.255883084762, 0.218969432030, 0.231122561337, 0.266457822538,\n",
            "        0.303656323334, 0.236353685877, 0.191246070814, 0.187680865565,\n",
            "        0.215218238625, 0.218829662177, 0.167484739952, 0.156332972030,\n",
            "        0.172651457993, 0.205906826582, 0.146745371207, 0.135803047123,\n",
            "        0.193681982802, 0.129913621563, 0.118003664370, 0.181879536101,\n",
            "        0.116223091518, 0.170685868146, 0.106039169795, 0.160004229329,\n",
            "        0.149731508353], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 73326.87442947902, Const: (1.181911985041658, -1.9544908539671249, 4.706773022568109, 2.488824189707178, 2.262913686111317, 1.4144731340716077), W00: -1150.0275731129282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-97SXJa4lJR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 4*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29958bef-4e59-4ce2-96b0-eb62e1297013",
        "id": "vdBCRmX2jZd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 73325.46503601906, Const: (1.1819139373720078, -1.9544728691559712, 4.706734509363361, 2.4887885245929917, 2.262873072276772, 1.4144471063681452), W00: -1150.0113526770997\n",
            "\n",
            "Minimum Cons: 3.230668021400498357564857 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1150.0113526771\n",
            "W10 diff: 1.1819139374\n",
            "W01 diff: -1.9544728692\n",
            "Comp3: 4.7067345093633612\n",
            "Comp4: 2.4887885245929917\n",
            "Comp5: 2.2628730722767720\n",
            "Comp6: 1.4144471063681452\n",
            "Coeffs: tensor([0.396068259120, 0.265860195554, 0.184093066041, 0.117528285561,\n",
            "        0.074655085764, 0.046877151903, 0.029004053770, 0.331534267659,\n",
            "        0.319293587394, 0.266127098330, 0.186128371730, 0.119711479237,\n",
            "        0.076508776901, 0.315687462502, 0.314591522011, 0.350208638978,\n",
            "        0.299874968711, 0.202366433586, 0.130890745405, 0.284789056989,\n",
            "        0.259426583112, 0.301286930510, 0.345638535799, 0.314886516900,\n",
            "        0.262210319437, 0.224095406893, 0.245058500373, 0.287403074999,\n",
            "        0.332508774739, 0.242758042277, 0.196442306933, 0.198102501280,\n",
            "        0.233608478098, 0.225406243507, 0.172901938363, 0.163525765677,\n",
            "        0.188539635438, 0.212544027509, 0.152093799801, 0.142413081101,\n",
            "        0.200344406868, 0.135899017829, 0.124251539476, 0.188780759322,\n",
            "        0.122345156601, 0.177580444576, 0.112083053776, 0.166813983851,\n",
            "        0.156650316258], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 54353.55759786531, Const: (0.9587983518643681, -1.6661343742824886, 4.363634827059036, 2.14761852961721, 1.862521005253141, 1.2802821776224975), W00: -1009.3527274519747\n",
            "Epoch 10000, Loss_Tot: 42842.088883399585, Const: (0.4908381366581456, -1.5979764449900178, 4.705324248487319, 2.380403681783827, 1.9561156927609253, 1.1765344266292876), W00: -1055.2483615507622\n",
            "Epoch 15000, Loss_Tot: 124177.13179713639, Const: (0.31215314183940546, -2.602870012988367, 5.29883095555641, 3.386344298849347, 3.257521392466753, 2.32290834876322), W00: -1494.3809448549514\n",
            "Epoch 20000, Loss_Tot: 57422.462732791515, Const: (1.62469520931226, -1.54257141197849, 1.3209520413537064, 0.46224167172574043, 0.23680123013574178, 0.8420143524464311), W00: -140.9701933784838\n",
            "\n",
            "Minimum Cons: 0.920210605234270895280702 at epoch 14404\n",
            "Values at minimum cons epoch:\n",
            "W00: -922.5150860135\n",
            "W10 diff: 0.3962699757\n",
            "W01 diff: -1.4160369482\n",
            "Comp3: 5.0732173637542015\n",
            "Comp4: 2.7198550544984017\n",
            "Comp5: 2.1534048164958364\n",
            "Comp6: 0.6258067648080340\n",
            "Coeffs: tensor([0.764354661302, 0.155436303635, 0.174728270766, 0.120910716713,\n",
            "        0.087977331930, 0.063439298908, 0.045186072442, 0.172703488062,\n",
            "        0.277371729331, 0.483864018627, 0.712105000509, 0.644528597958,\n",
            "        0.507690557265, 0.258565605915, 0.345272155943, 0.674780685694,\n",
            "        1.291166029057, 1.878722459990, 2.092099164131, 0.194289451093,\n",
            "        0.244991620074, 0.437817459290, 0.874019063499, 1.573231444747,\n",
            "        0.169426209233, 0.187353825648, 0.276360480426, 0.563255231094,\n",
            "        1.086775183847, 0.161422717718, 0.151041548876, 0.205559118479,\n",
            "        0.336787267315, 0.151437423556, 0.120867413280, 0.165829705702,\n",
            "        0.256042757211, 0.142241922691, 0.097389617129, 0.133998780932,\n",
            "        0.133489327079, 0.078307655106, 0.107660654238, 0.133006793144,\n",
            "        0.063160914391, 0.133878543567, 0.051842308697, 0.134755631376,\n",
            "        0.135638084527], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1437.5155627429215, Const: (0.058208822949954, -0.237322826433769, 1.5170813676894583, 0.5868490307744121, 0.3146255015023172, 0.27478414733220596), W00: -85.34837633472725\n",
            "Epoch 30000, Loss_Tot: 375.5175924533469, Const: (-0.05781621745298149, -0.07430617990036281, 0.8392982141818489, 0.2718376498979476, 0.13249715088794503, 0.15232039169551462), W00: -54.86134147104168\n",
            "Epoch 35000, Loss_Tot: 212.5030164978819, Const: (-0.024992494453306646, -0.03298490740921234, 0.7053368199291856, 0.22552644449686904, 0.10965127028944692, 0.12296225325063449), W00: -44.17957029522145\n",
            "Epoch 40000, Loss_Tot: 93.12586656515164, Const: (-0.0060917320443949485, -0.007917220119796564, 0.2762768398553778, 0.08209832316448933, 0.038880831479031396, 0.08945437178324822), W00: -12.107104516535472\n",
            "\n",
            "Minimum Cons: 0.000794172251807086031737 at epoch 39967\n",
            "Values at minimum cons epoch:\n",
            "W00: -12.3419875330\n",
            "W10 diff: -0.0061728122\n",
            "W01 diff: -0.0063174029\n",
            "Comp3: 0.2807392092825803\n",
            "Comp4: 0.0834849886961114\n",
            "Comp5: 0.0395620395073807\n",
            "Comp6: 0.0899146982561983\n",
            "Coeffs: tensor([1.005136645259e+00, 6.315953542868e-02, 2.396034348209e-02,\n",
            "        3.841623057850e-03, 1.819672756687e-03, 8.724604319939e-04,\n",
            "        4.079005533888e-04, 7.431474294297e-02, 1.299914055273e-01,\n",
            "        7.333333915729e-02, 2.712810976963e-02, 1.544077853083e-02,\n",
            "        8.746742341154e-03, 3.072384253194e-02, 9.671075201256e-02,\n",
            "        5.024773011927e-02, 2.143611322663e-02, 1.718127439627e-02,\n",
            "        1.436662152702e-02, 2.497652649552e-02, 2.734359341211e-02,\n",
            "        2.043258272166e-02, 1.200556038266e-02, 9.621599159125e-03,\n",
            "        1.564519075274e-02, 1.108380020453e-02, 8.765390366146e-03,\n",
            "        7.017689898237e-03, 5.579257021375e-03, 9.606617714438e-03,\n",
            "        6.145209444994e-03, 5.019193595446e-03, 4.024142453960e-03,\n",
            "        5.361273373096e-03, 3.291204921861e-03, 2.904674474682e-03,\n",
            "        2.304671659408e-03, 2.972835495985e-03, 1.679564289898e-03,\n",
            "        1.540906264254e-03, 1.644933907973e-03, 8.263665582068e-04,\n",
            "        7.923394955638e-04, 9.482014393121e-04, 4.136643478595e-04,\n",
            "        5.498652747900e-04, 2.181585110329e-04, 3.207788442013e-04,\n",
            "        1.871261218663e-04], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 45000, Loss_Tot: 1.9447177907857691, Const: (-0.0004306856511901991, -0.0007731814635074663, 0.004108098904044261, 0.003989935657399887, 0.001644445269379378, 0.006073620124079467), W00: -1.5679961796130115\n",
            "Epoch 50000, Loss_Tot: 1.667515148575411, Const: (-0.0008986965178499062, 0.0002300844318132711, 0.0019127131255121303, 0.0024441310778610973, 0.0011992451391580484, 0.0027090333206864674), W00: -1.5855205904799314\n",
            "Epoch 55000, Loss_Tot: 1.5941017588456532, Const: (-0.0004913822639023291, 4.151590697243357e-05, 0.0009033778958710979, 0.0020793140648122876, 0.001226745364594181, 0.0014121548879300028), W00: -1.571728143572512\n",
            "Epoch 60000, Loss_Tot: 1.6473490902523926, Const: (-0.0020672388829550137, 0.0016000367166832952, 0.0006720670774121636, 0.0023770698987157645, 0.0013493490929406466, 0.001280021657159094), W00: -1.562628594885072\n",
            "\n",
            "Minimum Cons: 0.000000164620305976885714 at epoch 59001\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5618169226\n",
            "W10 diff: -0.0000476851\n",
            "W01 diff: -0.0001305560\n",
            "Comp3: 0.0007372446350370\n",
            "Comp4: 0.0023312284842483\n",
            "Comp5: 0.0013323828336022\n",
            "Comp6: 0.0011843880349902\n",
            "Coeffs: tensor([9.855618937158e-01, 2.040122226591e-01, 2.432852444147e-02,\n",
            "        2.212673950179e-03, 2.327636789190e-04, 2.463058830990e-05,\n",
            "        2.617824883821e-06, 5.719828411996e-02, 2.184594056211e-02,\n",
            "        4.030626962478e-03, 7.659272098906e-04, 1.162225152253e-04,\n",
            "        1.604668506658e-05, 5.186007438730e-03, 1.421658628343e-02,\n",
            "        2.328208529527e-03, 4.024303086742e-04, 7.664539950678e-05,\n",
            "        2.115912432932e-05, 1.378400969082e-03, 1.902063499614e-03,\n",
            "        5.595638355973e-04, 1.379687067040e-04, 4.821906114059e-05,\n",
            "        4.464926641002e-04, 1.741861354045e-04, 9.103402352593e-05,\n",
            "        3.978302687747e-05, 1.904031407945e-05, 1.471414185642e-04,\n",
            "        5.304493715458e-05, 2.536911615979e-05, 1.237316996591e-05,\n",
            "        4.278645495143e-05, 1.432987844241e-05, 7.398978940412e-06,\n",
            "        3.526245677757e-06, 1.244118074290e-05, 3.687484843323e-06,\n",
            "        1.897383771000e-06, 3.621075718536e-06, 9.684077798179e-07,\n",
            "        4.901929117714e-07, 1.049771223868e-06, 2.641434911788e-07,\n",
            "        3.001592097033e-07, 7.406052095248e-08, 8.619944443203e-08,\n",
            "        2.475467489775e-08], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 65000, Loss_Tot: 1.5563462359674183, Const: (-1.0615415453996135e-05, -0.00021546256325888358, 0.0005163434196167311, 0.0026837304136215594, 0.001438212338268154, 0.0008234534437209209), W00: -1.5491001121955466\n",
            "Epoch 70000, Loss_Tot: 1.5520708462306643, Const: (0.0005771048914426924, -0.0007417458267595656, 0.00048192083265420757, 0.0031191475122474143, 0.0015425733057145526, 0.0007170472358115531), W00: -1.5380969095743913\n",
            "Epoch 75000, Loss_Tot: 1.5337085180692942, Const: (0.00014021825761401097, -0.00029877099186625955, 0.00043115049073418456, 0.0035175655279254883, 0.0017023805595689054, 0.0005165226430745724), W00: -1.5299513090077164\n",
            "Epoch 80000, Loss_Tot: 1.5349451741755002, Const: (-0.0007866963180782616, 0.0005358374752444384, 0.00046941393177878224, 0.004137037156996381, 0.001844342828918354, 0.0005508324222692518), W00: -1.5228508816337278\n",
            "\n",
            "Minimum Cons: 0.000000039229116772789349 at epoch 78863\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5236308481\n",
            "W10 diff: -0.0000737022\n",
            "W01 diff: -0.0000104900\n",
            "Comp3: 0.0004743265599745\n",
            "Comp4: 0.0039607240751496\n",
            "Comp5: 0.0018081630151613\n",
            "Comp6: 0.0005269542711863\n",
            "Coeffs: tensor([9.860056424933e-01, 2.004417207301e-01, 6.494775797810e-03,\n",
            "        1.610939068278e-04, 6.451014798347e-06, 2.695210382956e-07,\n",
            "        1.138949020443e-08, 5.725663115638e-02, 1.379235671031e-02,\n",
            "        8.010530927855e-04, 4.297487203463e-05, 1.992691651889e-06,\n",
            "        8.910442338086e-08, 2.920235545192e-03, 2.183396810106e-02,\n",
            "        8.946331880043e-04, 3.642645182770e-05, 1.610761203031e-06,\n",
            "        1.133376407247e-07, 1.162674367009e-03, 2.081021288172e-03,\n",
            "        3.147469248055e-04, 2.307310819417e-05, 1.326682878673e-06,\n",
            "        3.821816206583e-04, 8.919090748631e-05, 3.231802524055e-05,\n",
            "        5.664564592861e-06, 1.237089140087e-06, 9.231375389617e-05,\n",
            "        1.955108307782e-05, 6.581422836125e-06, 1.681186484426e-06,\n",
            "        2.218785745360e-05, 4.222262300980e-06, 1.398419033886e-06,\n",
            "        4.651595350679e-07, 5.090505810256e-06, 9.973914004599e-07,\n",
            "        2.626057911485e-07, 1.130823790770e-06, 2.387436000648e-07,\n",
            "        5.339909552005e-08, 2.518200311364e-07, 5.709608726295e-08,\n",
            "        5.607708905241e-08, 1.364994957910e-08, 1.248764708338e-08,\n",
            "        2.780838479179e-09], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prvgWjXtyZGY",
        "outputId": "bafea59c-d1cc-4c7d-ca50-9352dc642237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.536410400506526, Const: (0.0007513443333646563, -0.000839175472490794, 0.0004913423844400277, 0.004124272679380109, 0.0018352223152830872, 0.0005753642848931888), W00: -1.5204126220941263\n",
            "\n",
            "Minimum Cons: 0.000000648079302486109005 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5204126221\n",
            "W10 diff: 0.0007513443\n",
            "W01 diff: -0.0008391755\n",
            "Comp3: 0.0004913423844400\n",
            "Comp4: 0.0041242726793801\n",
            "Comp5: 0.0018352223152831\n",
            "Comp6: 0.0005753642848932\n",
            "Coeffs: tensor([9.854712507732e-01, 2.008082825890e-01, 5.931666332485e-03,\n",
            "        1.336595274161e-04, 4.847336084723e-06, 1.835057158533e-07,\n",
            "        6.837828880809e-09, 5.707823014287e-02, 1.342639951155e-02,\n",
            "        7.193554198774e-04, 3.613680857593e-05, 1.508621868305e-06,\n",
            "        6.020545218254e-08, 2.776132615220e-03, 2.226944718195e-02,\n",
            "        8.401590083742e-04, 3.191715194418e-05, 1.324124365714e-06,\n",
            "        9.492252735702e-08, 1.165407766779e-03, 1.973561369897e-03,\n",
            "        3.035986100936e-04, 2.086331502226e-05, 1.159971866596e-06,\n",
            "        3.809561981437e-04, 8.615481584780e-05, 2.871275857727e-05,\n",
            "        5.280058884625e-06, 1.111120773357e-06, 9.130605369161e-05,\n",
            "        1.874848330313e-05, 6.178133193142e-06, 1.544761078149e-06,\n",
            "        2.175622757727e-05, 4.005523418257e-06, 1.269636527832e-06,\n",
            "        4.213857489981e-07, 4.814839971276e-06, 9.377357521733e-07,\n",
            "        2.339115036353e-07, 1.054959867718e-06, 2.223396144729e-07,\n",
            "        4.788068746788e-08, 2.317111570040e-07, 5.268075133008e-08,\n",
            "        5.089297202384e-08, 1.210621944958e-08, 1.117811698053e-08,\n",
            "        2.455158194251e-09], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 1.5179284438545133, Const: (-0.00010665072297677547, -5.6553100157730185e-05, 0.0005429604654567287, 0.004848518181164013, 0.00195562013633763, 0.0005593056467969736), W00: -1.5146544894906344\n",
            "Epoch 10000, Loss_Tot: 1.5145033265140404, Const: (0.0005408678976566161, -0.0006755582678987793, 0.0007121387375532539, 0.0062578610759228374, 0.0022431401717463314, 0.000768691883657615), W00: -1.5011052838336107\n",
            "Epoch 15000, Loss_Tot: 1.5039353935086657, Const: (5.795224986515635e-05, -0.00018897388491456724, 0.0006897598185443085, 0.006529316470027654, 0.002260192144564006, 0.0006343209620982571), W00: -1.4995210667546517\n",
            "Epoch 20000, Loss_Tot: 1.5002880472826075, Const: (-1.2948579974914054e-05, -0.00012705757564823728, 0.0007420842912686629, 0.006649291946269681, 0.002280931205873216, 0.0006451039051351041), W00: -1.4959633438658722\n",
            "\n",
            "Minimum Cons: 0.000000026056436468133496 at epoch 17947\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4983237630\n",
            "W10 diff: 0.0000112182\n",
            "W01 diff: -0.0000417905\n",
            "Comp3: 0.0007006121929242\n",
            "Comp4: 0.0065728266541442\n",
            "Comp5: 0.0022846015133963\n",
            "Comp6: 0.0006021826350258\n",
            "Coeffs: tensor([9.845717385226e-01, 2.214472589058e-01, 1.185118740226e-03,\n",
            "        4.165228663438e-06, 2.350445193971e-08, 1.507626436098e-10,\n",
            "        1.959883709341e-12, 5.588655427232e-02, 8.046366973444e-03,\n",
            "        1.458852247119e-04, 8.807738206329e-07, 6.121918741511e-09,\n",
            "        8.054297496000e-11, 8.456871200417e-04, 2.976218611022e-02,\n",
            "        3.171728213036e-04, 1.601264664682e-06, 9.446730787177e-09,\n",
            "        1.795718013976e-10, 1.273426368579e-03, 2.307599182899e-04,\n",
            "        4.091320390103e-05, 1.622734707390e-06, 2.733361197250e-08,\n",
            "        3.404928631854e-04, 4.490461154576e-05, 8.040766118802e-06,\n",
            "        7.303689277888e-07, 3.787249345525e-08, 7.030368701111e-05,\n",
            "        7.353966993586e-06, 1.360022005524e-06, 1.671001220933e-07,\n",
            "        9.568383672135e-06, 1.346367239861e-06, 2.005076644363e-07,\n",
            "        2.494215704070e-08, 1.296539554962e-06, 2.637386349326e-07,\n",
            "        2.667112191902e-08, 1.757143113656e-07, 4.091195850852e-08,\n",
            "        3.604824294337e-09, 2.385625982164e-08, 5.403649299202e-09,\n",
            "        3.242273250341e-09, 7.165912143438e-10, 4.413274459286e-10,\n",
            "        6.034342721787e-11], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.4993522111606696, Const: (-3.833951825771287e-05, -8.677043099458714e-05, 0.0007472103522773375, 0.006680259814856748, 0.002306768767863923, 0.0006149137857894913), W00: -1.4954810312575777\n",
            "Epoch 30000, Loss_Tot: 1.668653807352309, Const: (0.0028134440610743106, -0.0030927183972175865, 0.0007055296999519348, 0.006623194334188456, 0.0023304174504698533, 0.0005249125522527787), W00: -1.4910947297843595\n",
            "Epoch 35000, Loss_Tot: 1.5307121703134177, Const: (-0.0011809156787851993, 0.0010179306235211794, 0.0007934049256941962, 0.0065636694729993145, 0.002322866478953241, 0.0008252692530158719), W00: -1.4995940309666547\n",
            "Epoch 40000, Loss_Tot: 1.4965203859047036, Const: (-2.4265336357132128e-05, -9.005185649857417e-05, 0.0007363862425505035, 0.00673668213513689, 0.0024007381664529763, 0.0005763268921455977), W00: -1.493111877604528\n",
            "\n",
            "Minimum Cons: 0.000000024935067444365742 at epoch 34549\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4941862724\n",
            "W10 diff: -0.0000236495\n",
            "W01 diff: -0.0000375156\n",
            "Comp3: 0.0007278190947229\n",
            "Comp4: 0.0067213481701514\n",
            "Comp5: 0.0023791488808091\n",
            "Comp6: 0.0005622631104823\n",
            "Coeffs: tensor([9.845277899965e-01, 2.222342387609e-01, 1.600016436587e-05,\n",
            "        1.446601531505e-09, 2.165282770227e-13, 3.081809941981e-17,\n",
            "        8.646525541083e-21, 5.577425623107e-02, 7.724241471112e-03,\n",
            "        3.628788363084e-06, 5.188635568766e-10, 8.166429885503e-14,\n",
            "        2.334212792075e-17, 6.491595617749e-04, 3.052601904907e-02,\n",
            "        5.353270224222e-06, 3.618701274972e-10, 4.920259937315e-14,\n",
            "        1.970058097294e-17, 1.264157863952e-03, 1.756845299374e-04,\n",
            "        6.559951631701e-07, 2.032743994691e-10, 6.807854130218e-14,\n",
            "        3.376437231005e-04, 2.395325001485e-05, 5.760539943422e-08,\n",
            "        5.160172267440e-11, 4.043871345092e-14, 7.005593404464e-05,\n",
            "        1.181724725795e-06, 3.803296839460e-09, 5.803888939333e-12,\n",
            "        3.063904268658e-06, 6.028336847074e-08, 2.158053087693e-10,\n",
            "        5.307731532915e-13, 1.342523947419e-07, 3.165902840717e-09,\n",
            "        1.064655383231e-11, 6.090646341890e-09, 1.629360072310e-10,\n",
            "        5.252378133772e-13, 2.737675143065e-10, 7.090271186271e-12,\n",
            "        1.213271313663e-11, 3.019996458543e-13, 5.376924592758e-13,\n",
            "        2.384289248771e-14], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yfyKIKDDjZeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section With some null constraints, and multi-lambda equality constraints (Using product of cons as the strategy), Diff Null grid"
      ],
      "metadata": {
        "id": "unfpcb5i2YU5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d549d25-a09b-4dc0-9116-4d20ad00bed5",
        "id": "NcDnyq9b2YVD"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.500000000000, -0.200000000000],\n",
              "        [-5.500000000000,  0.200000000000],\n",
              "        [-4.500000000000, -0.200000000000],\n",
              "        [-4.500000000000,  0.200000000000],\n",
              "        [-3.500000000000, -0.200000000000],\n",
              "        [-3.500000000000,  0.200000000000],\n",
              "        [-2.500000000000, -0.200000000000],\n",
              "        [-2.500000000000,  0.200000000000],\n",
              "        [-1.500000000000, -0.200000000000],\n",
              "        [-1.500000000000,  0.200000000000],\n",
              "        [-0.500000000000, -0.200000000000],\n",
              "        [-0.500000000000,  0.200000000000],\n",
              "        [ 0.500000000000, -0.200000000000],\n",
              "        [ 0.500000000000,  0.200000000000],\n",
              "        [ 1.500000000000, -0.200000000000],\n",
              "        [ 1.500000000000,  0.200000000000],\n",
              "        [ 2.500000000000, -0.200000000000],\n",
              "        [ 2.500000000000,  0.200000000000],\n",
              "        [ 3.500000000000, -0.200000000000],\n",
              "        [ 3.500000000000,  0.200000000000],\n",
              "        [ 4.500000000000, -0.200000000000],\n",
              "        [ 4.500000000000,  0.200000000000],\n",
              "        [ 5.500000000000, -0.200000000000],\n",
              "        [ 5.500000000000,  0.200000000000]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#THIS SECTION IS JUST TO TEST THE EXPRESSIONS FOR THE OPEN STRING AMPLITUDE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.special import legendre\n",
        "from scipy import integrate\n",
        "from scipy.special import hyp2f1 as scipy_hyp2f1\n",
        "from scipy.special import gamma as scipy_gamma\n",
        "from scipy.special import zeta\n",
        "#This is all for to check formulaes using the Open String Amplitude\n",
        "#Some definitions\n",
        "d = 10;\n",
        "alpha = (d-3)/2;\n",
        "lm = 56/10;\n",
        "dlm = 1/2\n",
        "lmp = lm + dlm\n",
        "lmm = lm - dlm\n",
        "ellmax = 12;\n",
        "w00 = -(np.pi)**2/6\n",
        "w10 = -zeta(3)\n",
        "w01 = 7*np.pi**4/360\n",
        "\n",
        "torch.set_printoptions(precision=12)\n",
        "\n",
        "def hyp2f1(a, b, c, z):\n",
        "    if isinstance(z, torch.Tensor):\n",
        "        z_np = z.detach().numpy()\n",
        "        result = scipy_hyp2f1(a,b,c,z_np)\n",
        "        return torch.tensor(result, dtype=z.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_hyp2f1(a,b,c,z))\n",
        "\n",
        "def gamma(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x_np = x.detach().numpy()\n",
        "        result = scipy_gamma(x_np)\n",
        "        return torch.tensor(result, dtype=x.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_gamma(x))\n",
        "\n",
        "def ker(lm,n,s1,s2):\n",
        "    return (1/(s1 - n) + 1/(s2 - n) + 1/(n + lm))\n",
        "\n",
        "def geg(ell,beta, z):\n",
        "    return gamma(ell + 2*beta) / (gamma(2*beta) * gamma(ell + 1)) * \\\n",
        "           hyp2f1(-ell, ell + 2*beta, beta + 1/2, (1-z)/2)\n",
        "\n",
        "#ell,n tensor and Null grid tensor\n",
        "ell = torch.arange(ellmax + 1, dtype = torch.float64).unsqueeze(1).repeat(1, ellmax + 1)\n",
        "n = torch.arange(1, ellmax + 2, dtype = torch.float64).unsqueeze(0).repeat(ellmax + 1, 1)\n",
        "mask = (n > ell) & (n <= ellmax + 1) & ((n - ell - 1) % 2 == 0)\n",
        "ell_n_tensor = torch.stack([ell[mask], n[mask]], dim=1)\n",
        "\n",
        "NullPoints = torch.stack(torch.meshgrid(\n",
        "    torch.arange(-6 +1/2, 5+1/2 + 1e-6, 1, dtype=torch.float64),\n",
        "    torch.arange(-0.2, 0.2 + 1e-6, 0.4, dtype=torch.float64),\n",
        "    indexing='ij'\n",
        ")).permute(1, 2, 0).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def w00comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    result = (-1/n)*geg(ell,alpha,1)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def w10comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-((2 * ell * (-3 + d + ell) * lm * (n + 2 * lm) * \\\n",
        "            hyp2f1(1 - ell, -2 + d + ell, d / 2, lm / (n + lm))) / ((-2 + d) * (n + lm)**2)) - \\\n",
        "            hyp2f1(-ell, -3 + d + ell, 1/2 * (-2 + d), lm / (n + lm)))) / (n**2 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def w01comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-(1/((d - 2) * (n + lm)**2)) *\\\n",
        "             2 * ell * (-3 + d + ell) * n * (n + 2*lm) *  hyp2f1(1 - ell, -2 + d + ell, d/2, lm/(n + lm)) + \\\n",
        "             2 * hyp2f1(-ell, -3 + d + ell, (d - 2)/2, lm/(n + lm)))) / (n**3 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcomp(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lm,n, s1, s2) * geg(ell, alpha, 1 + 2 * (((s1 + lm) * (s2 + lm)) / (n + lm) - lm) / n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1P(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lmp,n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmp)*(s2 + lmp))/(n + lmp)-lmp)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1M(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lmm,n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmm)*(s2 + lmm))/(n + lmm)-lmm)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD1(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ((2 * (-3 + d) * (n**2 + 2 * n * lm - s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) /  (n * (n + lm))) / n - (n + lm) * geg(ell, 1/2 * (-3 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) / (n * (n + lm)))) / (n + lm)**3)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD2(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = (1 / (n + lm)**4 * 2 * (-((2 * (-3 + d) * (-1 + d) * (n - s1) * (n - s2) *\n",
        "               (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "               geg(-2 + ell,(d+1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) /\n",
        "              (n**2 * (n + lm))) + ((-3 + d) * (n - s1) * (n - s2) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            ((-3 + d) * (2*n - s1 - s2) * (n + lm) *\n",
        "             geg(-1 + ell, (d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n -\n",
        "            (3 * (-3 + d) * (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            (n + lm) * geg(ell,(d-3)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))))\n",
        "\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD3(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result =  (2 / (n**3 * (n + lm)**7)) * (4 * (-3 + d) * (-1 + d) * (1 + d) * (n - s1)**2 * (n - s2)**2 * (n**2 + 2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-3 + ell, (3 + d)/2, 1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * (-1 + d) * n * (n - s1)**2 * (n -s2)**2 * (n + lm) * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     4 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (2 * n - s1 -s2) * (n + lm)**2 * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     8 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (n + lm) * (n**2 +2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (n - s1) * (n -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (2 * n - s1 - s2) * (n + lm)**3 * geg(-1 + ell, 1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     n**3 * (n + lm)**3 * geg(ell, 1/2 * (-3 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +\n",
        "     4 * n * (n + lm) * (2 * (-3 + d) * (-1 + d) * (n - s1) * (n -s2) * (n**2 + 2 * n * lm - s2 * lm -\n",
        "     s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))\n",
        "     - (-3 + d) * n * (n - s1) * (n -s2) * (n + lm) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) - (-3 + d) * n * (2 * n - s1 -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +3 * (-3 + d) * n * (n + lm) * (n**2 + 2 * n * lm -\n",
        "      s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell,1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "        n**2 * (n + lm)**2 * geg(ell, 1/2 * (-3 + d),  1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "#First, lets calculate cc(ell,n) for open string\n",
        "\n",
        "def AbsOpString(n,s2):\n",
        "    return (-1)**(1 + n)*gamma(-s2)/(gamma(n+1)*gamma(1 - n - s2))\n",
        "\n",
        "\n",
        "\n",
        "def cc(ell_n_tensor):\n",
        "    z = torch.linspace(-1 + 10**(-6), 1 - 10**(-6), 10**6).view(1, -1)\n",
        "\n",
        "    # Directly use ell and n from ell_n_tensor\n",
        "    ell_values = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n_values = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "\n",
        "    # Compute normfac for all combinations of ell and n\n",
        "    normfac = ((np.pi * 2**(1 - 2 * alpha) * gamma(2 * alpha + ell_values)) /\n",
        "                (gamma(ell_values + 1) * (ell_values + alpha) * gamma(alpha)**2))**-1\n",
        "\n",
        "    # Compute integrand\n",
        "    integrand = normfac * (1 - z**2)**(alpha - 1/2) * \\\n",
        "                AbsOpString(n_values, n_values * (z - 1) / 2) * \\\n",
        "                geg(ell_values, alpha, z)\n",
        "\n",
        "    # Integrate across z\n",
        "    return torch.trapz(integrand, z, dim=1)\n",
        "\n",
        "NullPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5334a296-0ecd-410c-861c-fbbbb430603a",
        "id": "GzsLpOux2YVE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-33f1b916b2d8>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(result, dtype=z.dtype)\n",
            "<ipython-input-24-33f1b916b2d8>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(scipy_hyp2f1(a,b,c,z))\n"
          ]
        }
      ],
      "source": [
        "#Store all the values\n",
        "coeff_vals = cc(ell_n_tensor)\n",
        "w00comp_vals = w00comp(ell_n_tensor)\n",
        "w01comp_vals = w01comp(ell_n_tensor)\n",
        "w10comp_vals = w10comp(ell_n_tensor)\n",
        "\n",
        "ampcomp_vals = ampcomp(ell_n_tensor, NullPoints)\n",
        "ampcompD1P_vals = ampcompD1P(ell_n_tensor, NullPoints)\n",
        "ampcompD1M_vals = ampcompD1M(ell_n_tensor, NullPoints)\n",
        "ampcompD1_vals = ampcompD1(ell_n_tensor, NullPoints)\n",
        "ampcompD2_vals = ampcompD2(ell_n_tensor, NullPoints)\n",
        "ampcompD3_vals = ampcompD3(ell_n_tensor, NullPoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb4ed33-49f8-4a97-caa2-f8b2665419f2",
        "id": "QqdLLvr52YVE"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.6449340668482264,\n",
              " -1.5708937983031839,\n",
              " -1.2020569031595942,\n",
              " -1.2020580619524968,\n",
              " 1.8940656589944915,\n",
              " 1.8940654448676988,\n",
              " -0.6006966395237955,\n",
              " 3.8352236174573874e-07,\n",
              " 1.770410565962509e-06,\n",
              " -3.886982374550674e-06,\n",
              " -0.6006963098081171,\n",
              " -0.6006965429156984,\n",
              " 0.9999996119378677)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#CHECKS FOR OPEN STRING\n",
        "def w00fullX():\n",
        "    return torch.dot(coeff_vals,w00comp_vals)\n",
        "\n",
        "def w10fullX():\n",
        "    return torch.dot(coeff_vals,w10comp_vals)\n",
        "\n",
        "def w01fullX():\n",
        "    return torch.dot(coeff_vals,w01comp_vals)\n",
        "\n",
        "def ampfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1PfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1MfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD3_vals, dim = 0)\n",
        "\n",
        "(w00, w00fullX().item(),w10, w10fullX().item(),w01, w01fullX().item(),ampfullX()[1].item(),\n",
        " ampD1fullX()[1].item(),ampD2fullX()[1].item(),ampD3fullX()[1].item(),\n",
        " ampD1PfullX()[1].item(),ampD1MfullX()[1].item(),(ampD1PfullX()/ampD1MfullX())[1].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsJXT3oZ2YVF"
      },
      "outputs": [],
      "source": [
        "# Same definitions for the PINN\n",
        "def w00full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w00comp_vals)\n",
        "\n",
        "def w01full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w01comp_vals)\n",
        "\n",
        "def w10full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w10comp_vals)\n",
        "\n",
        "def ampfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1Pfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1Mfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD3_vals, dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network. We will impose multi-lambda equality. But lets first check how well\n",
        "# this is satisfied with three derivatives imposed.\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.0001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "\n",
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a193a21a-984d-4cd0-bc85-3b14a386970e",
        "id": "3Q74m6Xu2YVF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 189980.284108349, Const: (0.6191000483218319, -2.877804170995457, 21.948242580185813, 6.779894033145742, 5.717897197004856, 3.1616399141430804), W00: -3370.197477012916\n",
            "Epoch 5000, Loss_Tot: 32572.106268427233, Const: (1.395905127589047, -1.0285955033970475, 5.279119027821399, 1.5849906577282786, 0.7678880740883774, 0.4749649951609432), W00: -250.59045376469362\n",
            "Epoch 10000, Loss_Tot: 27485.205933561127, Const: (1.3869602401243903, -0.8085609613877025, 5.519884395062325, 1.6547477011638747, 0.7942621001224516, 0.3810039304134601), W00: -259.27062399496134\n",
            "Epoch 15000, Loss_Tot: 5951.751090871003, Const: (0.6473528582409411, -0.11399074069011861, 8.640242874567436, 2.647858757290833, 1.3033898698483461, 0.34460570603676616), W00: -443.6240441820438\n",
            "\n",
            "Minimum Cons: 0.739904374782945795097078 at epoch 6361\n",
            "Values at minimum cons epoch:\n",
            "W00: -253.8943417368\n",
            "W10 diff: 1.3930827722\n",
            "W01 diff: -0.9844350696\n",
            "Comp3: 5.3210734990924813\n",
            "Comp4: 1.5972162787765296\n",
            "Comp5: 0.7731900493558879\n",
            "Comp6: 0.4337551269488567\n",
            "Coeffs: tensor([0.342796107573, 0.233305821184, 0.239835863128, 0.254868165134,\n",
            "        0.275636914797, 0.302488347735, 0.331549080377, 0.250939247729,\n",
            "        0.319524791483, 0.402617377323, 0.448322599027, 0.496194299542,\n",
            "        0.540032024777, 0.269629170956, 0.286197216163, 0.329545565921,\n",
            "        0.396329032459, 0.459507889661, 0.506875949230, 0.211627315027,\n",
            "        0.218863869635, 0.236743265135, 0.275028514852, 0.334933598277,\n",
            "        0.152224421469, 0.162979710213, 0.181657411229, 0.197818386463,\n",
            "        0.235454828520, 0.105518684498, 0.116774372386, 0.139274305790,\n",
            "        0.153389324426, 0.072724135976, 0.079150265056, 0.101826466434,\n",
            "        0.118792459183, 0.050483439114, 0.053070273131, 0.074102649259,\n",
            "        0.036333653123, 0.035993113992, 0.050785771971, 0.026590015762,\n",
            "        0.024557028560, 0.019455291431, 0.016724113691, 0.014221368103,\n",
            "        0.010388177627], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 1222.7671589029783, Const: (0.014800856576433263, 0.015812427169028753, 7.563595344665315, 2.282832447847212, 1.0974278981170111, 0.29818339969483443), W00: -328.94277828356473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 4*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6278f0-4bc4-4dda-a7a8-412d83355baa",
        "id": "k13AqgSZ2YVG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1222.6977131297722, Const: (0.014797582530841291, 0.01580974702324589, 7.5629614121893445, 2.282618234584476, 1.0973146778165375, 0.298176744665952), W00: -328.9148370369678\n",
            "\n",
            "Minimum Cons: 0.006456862667961107277170 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -328.9148370370\n",
            "W10 diff: 0.0147975825\n",
            "W01 diff: 0.0158097470\n",
            "Comp3: 7.5629614121893445\n",
            "Comp4: 2.2826182345844761\n",
            "Comp5: 1.0973146778165375\n",
            "Comp6: 0.2981767446659520\n",
            "Coeffs: tensor([1.085689969219, 0.624362224378, 0.742575126561, 0.957486214642,\n",
            "        1.212250967408, 1.506990392009, 1.828955908956, 0.376812068204,\n",
            "        0.535718113340, 0.740467035332, 0.932777165430, 1.175767647729,\n",
            "        1.445523906654, 0.386397262322, 0.431371833866, 0.523143535459,\n",
            "        0.648721610471, 0.795204756944, 0.964304684474, 0.291390542622,\n",
            "        0.316493346875, 0.365582972089, 0.448686762202, 0.575468092677,\n",
            "        0.206423763687, 0.221323473177, 0.268287656707, 0.311328499998,\n",
            "        0.393374169722, 0.140680060336, 0.151341580877, 0.191450776850,\n",
            "        0.229428983250, 0.094845276927, 0.099384250728, 0.132099473166,\n",
            "        0.165259382673, 0.064054459518, 0.064663175062, 0.090815882680,\n",
            "        0.045204976603, 0.042857596282, 0.059848122595, 0.032147762504,\n",
            "        0.028401999295, 0.022811220331, 0.019050633385, 0.016164327472,\n",
            "        0.011791109004], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 760.3954219876183, Const: (0.003072168167176814, 0.009261100175639969, 3.8366829220815246, 1.074524136511606, 0.47642860719578406, 0.24336724141122376), W00: -167.16721812942313\n",
            "Epoch 10000, Loss_Tot: 286.7932423186382, Const: (0.004096488196430492, 0.00729574553728396, 1.0383109559318027, 0.26878961063086837, 0.11349995727243004, 0.15972170182378004), W00: -30.982930798910317\n",
            "Epoch 15000, Loss_Tot: 251.10864029126077, Const: (-0.0007955901754566064, 0.00077187256735467, 0.9969779534303519, 0.25305755291646914, 0.10101093465353098, 0.14780634637378764), W00: -32.62919249770466\n",
            "Epoch 20000, Loss_Tot: 1.8278838642867563, Const: (-0.00020797175464992712, -0.00041014806980421703, 0.0030068479471024147, 0.0010297921684806154, 0.0005761147472718704, 0.0036631729007492665), W00: -1.6915807703799555\n",
            "\n",
            "Minimum Cons: 0.000002338639357958029712 at epoch 19264\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.6912534999\n",
            "W10 diff: -0.0002577109\n",
            "W01 diff: -0.0004975170\n",
            "Comp3: 0.0033880159982834\n",
            "Comp4: 0.0012255862086441\n",
            "Comp5: 0.0006155843927687\n",
            "Comp6: 0.0041738925080800\n",
            "Coeffs: tensor([9.785146534149e-01, 2.360527133231e-01, 1.747768056340e-01,\n",
            "        1.348547420880e-01, 1.148799131063e-01, 1.013284744119e-01,\n",
            "        8.930496129297e-02, 5.057741462283e-02, 4.122812589049e-02,\n",
            "        2.705012090470e-02, 1.583338924706e-02, 9.901413147677e-03,\n",
            "        6.344241520791e-03, 8.307190103427e-03, 3.116634563840e-03,\n",
            "        1.642758968401e-03, 1.200794967723e-03, 8.378638944614e-04,\n",
            "        5.373727102940e-04, 1.907410572662e-03, 5.756446590795e-04,\n",
            "        2.297285211922e-04, 9.742892493027e-05, 7.264292610312e-05,\n",
            "        7.148831248226e-04, 1.162758923899e-04, 4.362156107097e-05,\n",
            "        1.745972909130e-05, 7.230376932160e-06, 2.616090444849e-04,\n",
            "        2.513959394638e-05, 8.324430539777e-06, 3.315022231877e-06,\n",
            "        9.259024565047e-05, 8.727270890938e-06, 1.771962654262e-06,\n",
            "        6.294089316222e-07, 3.303489917208e-05, 3.192375708540e-06,\n",
            "        3.702451695060e-07, 1.135484626778e-05, 1.145421159900e-06,\n",
            "        1.250439702661e-07, 3.448430801979e-06, 4.110036709498e-07,\n",
            "        9.985680012899e-07, 1.239544859052e-07, 2.891567912259e-07,\n",
            "        8.373153228876e-08], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.7482247417377041, Const: (-0.00010451573002923098, -0.00024918050414268755, 0.0011194153461996812, 0.000455504957031312, 0.00043144020722086693, 0.0011709340486931662), W00: -1.733783731659131\n",
            "Epoch 30000, Loss_Tot: 1.745061255453241, Const: (-0.0002134801405724307, -0.0002883843102605699, 0.0012703829559025258, 0.0004024084049771695, 0.00040690400935774026, 0.0013174844737792782), W00: -1.7264162092585136\n",
            "Epoch 35000, Loss_Tot: 1.7171952123008578, Const: (-5.0326819583057514e-05, -0.00015844333004899624, 0.0010766455624780678, 0.001129749807976016, 0.00044190888821906343, 0.001995365234533001), W00: -1.6771040173329648\n",
            "Epoch 40000, Loss_Tot: 1.6764198681958742, Const: (-0.00010221385810127437, -0.00021730199648684057, 0.0009437664715010581, 0.000978422534418443, 0.00045871437744746574, 0.0013665170396320266), W00: -1.6571695016951762\n",
            "\n",
            "Minimum Cons: 0.000000066291478885262411 at epoch 38949\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.6883699864\n",
            "W10 diff: 0.0000520094\n",
            "W01 diff: 0.0000113740\n",
            "Comp3: 0.0007705809683581\n",
            "Comp4: 0.0008720182714672\n",
            "Comp5: 0.0004364933142364\n",
            "Comp6: 0.0012451775774396\n",
            "Coeffs: tensor([9.818265993300e-01, 1.706642919087e-01, 2.162038498178e-01,\n",
            "        2.060306711085e-01, 1.656894337038e-01, 1.415009456708e-01,\n",
            "        1.187496217608e-01, 5.410750275443e-02, 3.371651776975e-02,\n",
            "        1.809171318774e-02, 8.835868003665e-03, 4.436836625367e-03,\n",
            "        2.922600117550e-03, 8.545303358047e-03, 4.852961066252e-03,\n",
            "        2.459752145362e-03, 1.212456394979e-03, 5.085741936655e-04,\n",
            "        2.111423167185e-04, 2.173064147561e-03, 8.363090038592e-04,\n",
            "        3.713931270344e-04, 1.563723556860e-04, 6.990669820403e-05,\n",
            "        7.184292635055e-04, 1.150798520395e-04, 4.987908553529e-05,\n",
            "        2.088630200480e-05, 9.000256606286e-06, 2.280009322443e-04,\n",
            "        2.640083419894e-05, 7.130425962488e-06, 2.804660393945e-06,\n",
            "        6.526670022755e-05, 6.545620431277e-06, 1.459331017504e-06,\n",
            "        4.868385480193e-07, 1.388059326950e-05, 1.654507508894e-06,\n",
            "        3.551236025650e-07, 2.908580700100e-06, 4.153239350757e-07,\n",
            "        8.767754603988e-08, 6.094699872745e-07, 1.037520609379e-07,\n",
            "        1.277094796940e-07, 2.558306530692e-08, 2.676047785581e-08,\n",
            "        5.607439230162e-09], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 45000, Loss_Tot: 1.653586825901137, Const: (-0.00021584331123825073, -0.0001323831667208175, 0.000892353419316321, 0.0009964926620540397, 0.00047640762606866667, 0.001210952040349387), W00: -1.6382816410825005\n",
            "Epoch 50000, Loss_Tot: 1.676061875393707, Const: (0.0007522479290944517, -0.0009050481498762419, 0.0006312429975119304, 0.0009456425844792563, 0.0004973493027178689, 0.0015473541901266914), W00: -1.6382689344924677\n",
            "Epoch 55000, Loss_Tot: 1.634991916665715, Const: (0.0001085089679759843, -0.0002995833070733589, 0.0008921433224480679, 0.0010231773259019672, 0.0005080132809409284, 0.0012267440587522072), W00: -1.6189276632687943\n",
            "Epoch 60000, Loss_Tot: 1.6275399203162395, Const: (-7.611529514828241e-05, -0.00019288997921407258, 0.0008739380483405729, 0.0010029947831787789, 0.0005818697613419533, 0.0010867034684007707), W00: -1.6153006752115295\n",
            "\n",
            "Minimum Cons: 0.000000012230765781282485 at epoch 56476\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.6290764523\n",
            "W10 diff: 0.0000093092\n",
            "W01 diff: -0.0000074809\n",
            "Comp3: 0.0007694222463854\n",
            "Comp4: 0.0009940295736622\n",
            "Comp5: 0.0004931415987904\n",
            "Comp6: 0.0010241332175996\n",
            "Coeffs: tensor([9.840916557359e-01, 1.510315322531e-01, 2.623117749299e-01,\n",
            "        1.217266611857e-01, 4.699351558708e-02, 1.835510078793e-02,\n",
            "        7.407898600020e-03, 5.617731763532e-02, 3.320631036259e-02,\n",
            "        1.086785135984e-02, 2.878690551022e-03, 6.963539413506e-04,\n",
            "        2.276301423732e-04, 8.657864094902e-03, 5.580250812324e-03,\n",
            "        2.221586088715e-03, 5.046910806632e-04, 1.026243354677e-04,\n",
            "        2.122896209439e-05, 2.232014743793e-03, 7.118433794095e-04,\n",
            "        2.333153117832e-04, 6.670008616525e-05, 1.711361551216e-05,\n",
            "        7.178408244896e-04, 8.643960062105e-05, 2.832717187917e-05,\n",
            "        8.316551824093e-06, 2.301817602280e-06, 2.315948419757e-04,\n",
            "        1.445558046528e-05, 3.400340421822e-06, 1.004769933649e-06,\n",
            "        7.413474814481e-05, 3.131325306092e-06, 5.949666200642e-07,\n",
            "        1.217358730943e-07, 4.862403145128e-06, 7.197158799573e-07,\n",
            "        1.058467285667e-07, 3.119010184075e-07, 1.609289872740e-07,\n",
            "        1.905429566213e-08, 1.993949712122e-08, 3.591576121408e-08,\n",
            "        1.275201455603e-09, 4.610160753235e-09, 1.103125064182e-10,\n",
            "        1.161083794918e-11], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 65000, Loss_Tot: 1.631385374410432, Const: (-0.00025751442962484994, -0.00018413517258641932, 0.0008518449303660815, 0.00102169316272465, 0.0005818966801158101, 0.0010331648471400105), W00: -1.6197088839642892\n",
            "Epoch 70000, Loss_Tot: 1.6236143989629215, Const: (-0.0003326128909202364, -0.0001668223439279526, 0.0008065012705249867, 0.0010171044119501014, 0.0005708365538635136, 0.0009638124676538262), W00: -1.6129404439384727\n",
            "Epoch 75000, Loss_Tot: 1.6168217438903056, Const: (0.00022953846782880483, -0.0003595611228341955, 0.0008464318873384746, 0.001100767126503835, 0.0006363251360985756, 0.001001915509591635), W00: -1.604963675914033\n",
            "Epoch 80000, Loss_Tot: 1.6119769339943573, Const: (-0.00015034712033989983, -0.00012816745282973052, 0.0008429823605252779, 0.001154313916405614, 0.0006712508936122907, 0.000986095597194251), W00: -1.6018627772007048\n",
            "\n",
            "Minimum Cons: 0.000000015859727948244762 at epoch 78085\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.6035687501\n",
            "W10 diff: -0.0000072425\n",
            "W01 diff: 0.0000121920\n",
            "Comp3: 0.0008288771055139\n",
            "Comp4: 0.0011301707555947\n",
            "Comp5: 0.0006542965787948\n",
            "Comp6: 0.0011183815968034\n",
            "Coeffs: tensor([9.846283257820e-01, 1.562513532807e-01, 2.637219155311e-01,\n",
            "        4.037507480358e-02, 4.510209660914e-03, 5.310673412403e-04,\n",
            "        6.786660244896e-05, 5.670273223229e-02, 3.341311229976e-02,\n",
            "        7.300918207786e-03, 8.299103426105e-04, 8.805744952720e-05,\n",
            "        1.009376256187e-05, 8.413677094746e-03, 6.979055840410e-03,\n",
            "        1.264420788876e-03, 1.275380169506e-04, 1.500896595777e-05,\n",
            "        1.789515467202e-06, 2.212770402981e-03, 7.047808639330e-04,\n",
            "        1.360508485340e-04, 2.198070388058e-05, 2.792658831541e-06,\n",
            "        7.110963104034e-04, 6.988698005776e-05, 1.410406086561e-05,\n",
            "        2.523418818434e-06, 4.139564063229e-07, 2.377760987520e-04,\n",
            "        7.577027245668e-06, 1.437817913463e-06, 2.775915190391e-07,\n",
            "        6.786780019751e-05, 1.090559983810e-06, 1.467736988806e-07,\n",
            "        2.829848683603e-08, 7.880609206463e-07, 1.636924846488e-07,\n",
            "        1.978307739330e-08, 1.866378014152e-08, 2.577394447780e-08,\n",
            "        2.550279163958e-09, 9.796523577364e-10, 4.067609021102e-09,\n",
            "        5.454769279318e-11, 2.141131871990e-10, 3.092770663728e-12,\n",
            "        1.755326838762e-13], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 4*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3684e6-272d-4a46-a920-ec631a03e7d7",
        "id": "-c2xIut42YVG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.6119347593011568, Const: (7.704423030219232e-06, -0.00022476130117476956, 0.0008452040577020881, 0.0011548938032536259, 0.0006715728214232341, 0.0009891565649714378), W00: -1.6016446821944959\n",
            "\n",
            "Minimum Cons: 0.000000222454693470804007 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.6016446822\n",
            "W10 diff: 0.0000077044\n",
            "W01 diff: -0.0002247613\n",
            "Comp3: 0.0008452040577021\n",
            "Comp4: 0.0011548938032536\n",
            "Comp5: 0.0006715728214232\n",
            "Comp6: 0.0009891565649714\n",
            "Coeffs: tensor([9.845771819594e-01, 1.563807423563e-01, 2.637292492286e-01,\n",
            "        3.740555394670e-02, 3.743908808155e-03, 4.037221569589e-04,\n",
            "        4.781827386587e-05, 5.661135937951e-02, 3.347563771332e-02,\n",
            "        7.087496179205e-03, 7.599301510998e-04, 7.643678121759e-05,\n",
            "        8.461522818067e-06, 8.352364878893e-03, 7.134101393795e-03,\n",
            "        1.216958013017e-03, 1.167829427876e-04, 1.315188355287e-05,\n",
            "        1.495329408729e-06, 2.199201771930e-03, 7.098580536852e-04,\n",
            "        1.313883945019e-04, 2.028877288424e-05, 2.439185309843e-06,\n",
            "        7.065430711727e-04, 6.796083425120e-05, 1.349800167888e-05,\n",
            "        2.315396050557e-06, 3.628179549714e-07, 2.363062450887e-04,\n",
            "        7.000097696754e-06, 1.354666441563e-06, 2.534855606928e-07,\n",
            "        6.647055717002e-05, 9.748872215952e-07, 1.363008531066e-07,\n",
            "        2.543979063554e-08, 6.420226362614e-07, 1.483560501862e-07,\n",
            "        1.735373555285e-08, 1.571201557264e-08, 2.289257779200e-08,\n",
            "        2.062865407305e-09, 7.761620711260e-10, 3.532515792261e-09,\n",
            "        4.082501036853e-11, 1.641105723659e-10, 2.171458194329e-12,\n",
            "        1.167134762070e-13], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 1.717333932655989, Const: (0.002694253913477551, -0.0018969361913008242, 0.0008947151887594646, 0.001226754213131767, 0.0007376450746000582, 0.001024481358098722), W00: -1.5982646014835111\n",
            "Epoch 10000, Loss_Tot: 1.606919561484298, Const: (0.00012329434214164436, -0.0002880701433816135, 0.0009310040218510051, 0.0013414576164997999, 0.0008276185133472128, 0.001059660875653628), W00: -1.5947088907472673\n",
            "Epoch 15000, Loss_Tot: 1.6168680719560788, Const: (-0.0005145608523937639, 0.00011405705246159137, 0.0010652617961976495, 0.0014046612990470623, 0.0009847496591932593, 0.0010475979270936687), W00: -1.6031156389672452\n",
            "Epoch 20000, Loss_Tot: 1.6006127071142637, Const: (-6.531074053883756e-05, -0.0001676107939576621, 0.0009693005826832533, 0.0015613017779899515, 0.0009756820070710975, 0.0010664357136758304), W00: -1.5889162670894226\n",
            "\n",
            "Minimum Cons: 0.000000010115157747674699 at epoch 13703\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5926882003\n",
            "W10 diff: 0.0000037107\n",
            "W01 diff: 0.0000063338\n",
            "Comp3: 0.0009162301027860\n",
            "Comp4: 0.0014155633561702\n",
            "Comp5: 0.0008789903623201\n",
            "Comp6: 0.0013779448404655\n",
            "Coeffs: tensor([9.843919060590e-01, 1.653892998564e-01, 2.514972783412e-01,\n",
            "        1.496452753179e-02, 8.235243770240e-04, 5.448695082354e-05,\n",
            "        4.503633535712e-06, 5.638549200098e-02, 3.360522975903e-02,\n",
            "        5.629667527366e-03, 3.629825147540e-04, 2.517783995385e-05,\n",
            "        2.025058620773e-06, 7.887246842604e-03, 8.623381246522e-03,\n",
            "        8.034115571341e-04, 5.463985176067e-05, 4.225483760155e-06,\n",
            "        3.300208751828e-07, 2.136408720471e-03, 7.329528553731e-04,\n",
            "        1.013901810880e-04, 9.532899334719e-06, 7.247980388730e-07,\n",
            "        6.912840796027e-04, 5.205428598248e-05, 9.913903131218e-06,\n",
            "        1.217673926979e-06, 1.162230826374e-07, 2.352137251683e-04,\n",
            "        4.001621387273e-06, 8.786785588112e-07, 1.217373462074e-07,\n",
            "        5.019048635766e-05, 5.202093831236e-07, 7.638597328174e-08,\n",
            "        1.178347533352e-08, 2.339590086127e-07, 6.948821394256e-08,\n",
            "        5.904623203872e-09, 5.556425412565e-09, 9.254388555044e-09,\n",
            "        4.833965734164e-10, 1.941458701579e-10, 1.229452383030e-09,\n",
            "        7.279568728115e-12, 4.094993399500e-11, 2.688705912279e-13,\n",
            "        9.885399037982e-15], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.7053145502116032, Const: (0.0027161437673803857, -0.0018696894478031911, 0.0009840628603531069, 0.0016472943300007569, 0.0010265278121403024, 0.0010414560430487237), W00: -1.585736487352517\n",
            "Epoch 30000, Loss_Tot: 1.5948243730805896, Const: (-6.297274132127662e-05, -0.0001705587002323039, 0.0009531435452670859, 0.0018016480123361702, 0.001103503683863503, 0.0010028436332611545), W00: -1.5844368611891206\n",
            "Epoch 35000, Loss_Tot: 1.5909793282838576, Const: (-0.00018669033127438617, -0.00013210131961716165, 0.0009659776428505195, 0.0019369738822955264, 0.0011810806392471493, 0.0010391013250639113), W00: -1.5796589722620022\n",
            "Epoch 40000, Loss_Tot: 1.589192888069517, Const: (6.945786581757929e-05, -0.0002514108919628999, 0.001029212301956407, 0.002118312489039842, 0.0013116271188919908, 0.001041433367488315), W00: -1.5776667351631215\n",
            "\n",
            "Minimum Cons: 0.000000014773600990170681 at epoch 34664\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5811736040\n",
            "W10 diff: 0.0000102270\n",
            "W01 diff: -0.0000053557\n",
            "Comp3: 0.0009295233716393\n",
            "Comp4: 0.0019187213055233\n",
            "Comp5: 0.0011766128606239\n",
            "Comp6: 0.0012797064958656\n",
            "Coeffs: tensor([9.839361471061e-01, 1.800558198538e-01, 2.216116135650e-01,\n",
            "        5.090303067032e-03, 1.535871238742e-04, 6.532049589888e-06,\n",
            "        3.193830655954e-07, 5.582533401829e-02, 3.286205267644e-02,\n",
            "        3.766501354293e-03, 1.284873697392e-04, 5.452872983416e-06,\n",
            "        2.577460397623e-07, 6.978022158520e-03, 1.132333346029e-02,\n",
            "        5.047849034473e-04, 1.998771932334e-05, 8.771998813250e-07,\n",
            "        3.881228562730e-08, 2.013115838377e-03, 7.113166062158e-04,\n",
            "        6.815708262827e-05, 3.419538094306e-06, 1.454137597891e-07,\n",
            "        6.525936918071e-04, 3.690325359561e-05, 6.679245137682e-06,\n",
            "        4.283908126065e-07, 2.329831487672e-08, 2.285865931209e-04,\n",
            "        2.385709462280e-06, 4.926105793643e-07, 4.503457007541e-08,\n",
            "        3.522133644423e-05, 2.705641362672e-07, 2.874935853585e-08,\n",
            "        4.104756732661e-09, 5.354481733271e-08, 3.044930725344e-08,\n",
            "        1.820309180831e-09, 9.158148435931e-10, 3.413277510204e-09,\n",
            "        1.199557194511e-10, 1.812443785486e-11, 3.807525548274e-10,\n",
            "        3.559370313977e-13, 6.576073166457e-12, 6.984043499520e-15,\n",
            "        1.370379008097e-16], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 45000, Loss_Tot: 1.598886263611217, Const: (-0.0005496845663783834, 0.0005142952061287787, 0.0008176678683464656, 0.0022019472998873815, 0.0013956423805937931, 0.0008719488538898769), W00: -1.585616788757602\n",
            "Epoch 50000, Loss_Tot: 1.583696701911568, Const: (-6.104788700067587e-05, -0.0001614304081727358, 0.0010987224780602202, 0.0024256973410603584, 0.0015337570346455323, 0.0010239767634115742), W00: -1.572913551579599\n",
            "Epoch 55000, Loss_Tot: 1.5768524888925501, Const: (3.4822073320617264e-05, -0.0002204343275529208, 0.0011066531731789783, 0.0026145247561709436, 0.00167342535071384, 0.0010828866364900327), W00: -1.5646280155221226\n",
            "Epoch 60000, Loss_Tot: 1.6783089890577232, Const: (-0.002702255904220907, 0.0015977781475065012, 0.0008852354464209418, 0.0027302725352043177, 0.0017562876705685775, 0.0008984886156223697), W00: -1.5716853513282327\n",
            "\n",
            "Minimum Cons: 0.000000020133372058114691 at epoch 49346\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5704710546\n",
            "W10 diff: 0.0000137993\n",
            "W01 diff: -0.0000018916\n",
            "Comp3: 0.0010378470468339\n",
            "Comp4: 0.0023897293309759\n",
            "Comp5: 0.0015190366628928\n",
            "Comp6: 0.0014454960366097\n",
            "Coeffs: tensor([9.835483100817e-01, 1.921468236541e-01, 1.929839141540e-01,\n",
            "        2.841315635191e-03, 5.531354811939e-05, 1.753965813250e-06,\n",
            "        5.876773694999e-08, 5.531239985271e-02, 3.196617315966e-02,\n",
            "        2.827411622388e-03, 6.283266425359e-05, 1.959345517809e-06,\n",
            "        6.705773851730e-08, 6.179776533053e-03, 1.354583939468e-02,\n",
            "        3.840527417006e-04, 1.026928400564e-05, 3.039135570217e-07,\n",
            "        9.048386262393e-09, 1.899205809206e-03, 6.736115221324e-04,\n",
            "        5.285985276743e-05, 1.779785359614e-06, 5.073501407707e-08,\n",
            "        6.191113089330e-04, 2.798702650407e-05, 4.507739033601e-06,\n",
            "        2.211166045097e-07, 8.465113727773e-09, 2.255425391198e-04,\n",
            "        1.652616121391e-06, 2.953599039907e-07, 2.091391953156e-08,\n",
            "        1.161343751837e-05, 1.710388366031e-07, 1.383717090105e-08,\n",
            "        1.600408130565e-09, 1.707871798130e-08, 1.774402500903e-08,\n",
            "        7.086897969600e-10, 2.425266327610e-10, 1.843084191653e-09,\n",
            "        4.360478623781e-11, 3.750168861214e-12, 1.901644668803e-10,\n",
            "        5.703316031596e-14, 1.011382495411e-12, 8.681548746382e-16,\n",
            "        1.311418738568e-17], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 65000, Loss_Tot: 1.5765072235509492, Const: (-0.0006365906628751361, 0.00019254392067247217, 0.0010098081621375216, 0.0029283137581549725, 0.0018292095938623213, 0.0008891082994882162), W00: -1.5641788795342837\n",
            "Epoch 70000, Loss_Tot: 1.614371427836689, Const: (-0.00028401706743919064, 0.0009174437505439226, 0.0006453299114779476, 0.0031190853535187466, 0.001978425723252967, 0.0018559142533966884), W00: -1.5707035633769906\n",
            "Epoch 75000, Loss_Tot: 1.5641339511344572, Const: (-2.497334200568524e-05, -0.00015883312802200678, 0.0008554905504943465, 0.00327668884524169, 0.0020473609599576514, 0.0007579586673546449), W00: -1.558130421416595\n",
            "Epoch 80000, Loss_Tot: 1.5669381963027376, Const: (0.00016898722664215882, -0.0003425187830137233, 0.000818383573091868, 0.003348969996014079, 0.0020977533029920338, 0.0006758994594936841), W00: -1.5609110375144448\n",
            "\n",
            "Minimum Cons: 0.000000004169222342018850 at epoch 77218\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5553084260\n",
            "W10 diff: -0.0000022559\n",
            "W01 diff: 0.0000024612\n",
            "Comp3: 0.0007845779042094\n",
            "Comp4: 0.0033386407035586\n",
            "Comp5: 0.0021637023177637\n",
            "Comp6: 0.0012487918792659\n",
            "Coeffs: tensor([9.825882226426e-01, 2.201208825331e-01, 1.309882028576e-01,\n",
            "        8.719754578094e-04, 9.132157502793e-06, 1.533236468653e-07,\n",
            "        2.822048209956e-09, 5.411924739359e-02, 2.914166668042e-02,\n",
            "        1.420396711307e-03, 1.525745280380e-05, 2.646378091220e-07,\n",
            "        4.677208523521e-09, 4.320204128880e-03, 1.887068017665e-02,\n",
            "        2.154807624389e-04, 2.619409603930e-06, 3.668162860406e-08,\n",
            "        5.191661032623e-10, 1.648183914688e-03, 5.591505722328e-04,\n",
            "        3.282786703192e-05, 5.056506038099e-07, 6.352314867756e-09,\n",
            "        5.453979976778e-04, 1.571997079679e-05, 2.266641925706e-06,\n",
            "        6.444270070523e-08, 1.228117970150e-09, 2.307045370493e-04,\n",
            "        1.021954131116e-06, 9.792579152493e-08, 5.118176731726e-09,\n",
            "        6.736806771516e-08, 9.061304093070e-08, 3.663025295954e-09,\n",
            "        3.256490030867e-10, 4.562318135238e-10, 8.036594605721e-09,\n",
            "        1.698048436120e-10, 3.461332715196e-12, 2.916651640773e-10,\n",
            "        8.389931728113e-12, 2.443156594973e-14, 5.546173848255e-13,\n",
            "        1.767161682770e-16, 3.558458429876e-15, 1.282068664520e-18,\n",
            "        9.301356387312e-21], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 4*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1Pfull()/ampD1Mfull(),one_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8WU9ds7jZeM",
        "outputId": "8d33eadd-bbbe-4e96-d0dc-1eb6285615ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.567781829746423, Const: (-0.0004270458733175442, 6.710731808312609e-05, 0.0008077332099939884, 0.0033524507050305434, 0.002101680779283418, 0.000661732824934782), W00: -1.5615342107298829\n",
            "\n",
            "Minimum Cons: 0.000000286058130068924872 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5615342107\n",
            "W10 diff: -0.0004270459\n",
            "W01 diff: 0.0000671073\n",
            "Comp3: 0.0008077332099940\n",
            "Comp4: 0.0033524507050305\n",
            "Comp5: 0.0021016807792834\n",
            "Comp6: 0.0006617328249348\n",
            "Coeffs: tensor([9.817302281532e-01, 2.299627139655e-01, 1.594512643255e-01,\n",
            "        1.070166183329e-03, 1.091407218780e-05, 1.859035800269e-07,\n",
            "        3.490046036914e-09, 5.326044142693e-02, 3.029398723400e-02,\n",
            "        1.655955432510e-03, 1.790453654329e-05, 3.157220513870e-07,\n",
            "        5.671291206118e-09, 4.057232861634e-03, 1.966139447323e-02,\n",
            "        2.393082639361e-04, 2.808709500950e-06, 3.860636616529e-08,\n",
            "        5.346233354295e-10, 1.605499823952e-03, 5.670834179939e-04,\n",
            "        3.579559492991e-05, 5.527441132187e-07, 6.791937335218e-09,\n",
            "        5.143724465180e-04, 1.698108342023e-05, 2.380616802688e-06,\n",
            "        6.883524775410e-08, 1.335796422042e-09, 2.141718610353e-04,\n",
            "        1.097429241415e-06, 9.669755539486e-08, 5.303331224463e-09,\n",
            "        1.334681407729e-07, 9.677689676686e-08, 3.525057757934e-09,\n",
            "        3.350853933342e-10, 5.398802626715e-10, 8.539423877124e-09,\n",
            "        1.580137046272e-10, 4.121655737199e-12, 7.453962337996e-10,\n",
            "        8.790548188414e-12, 2.937305971842e-14, 7.765614080309e-13,\n",
            "        2.070059551181e-16, 4.762789959633e-15, 1.486410105049e-18,\n",
            "        1.067319536354e-20], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 1.561529267671347, Const: (0.0002678651932637166, -0.0002910148372150534, 0.0007926194200182124, 0.0035946702811861958, 0.0021768943986614424, 0.0006901145298622654), W00: -1.5552022730556618\n",
            "Epoch 10000, Loss_Tot: 1.5574540765289773, Const: (-0.0005010005414707308, 0.00010096551597360559, 0.0007308417631602906, 0.003784287460884993, 0.00230990300837993, 0.0007146561413293051), W00: -1.5497347867458826\n",
            "Epoch 15000, Loss_Tot: 1.5682211948569937, Const: (-0.00044348347327249904, 0.0005192578947013082, 0.0007936083236112797, 0.003874589673750163, 0.002186681969278711, 0.0006251891133102911), W00: -1.559649517060222\n",
            "Epoch 20000, Loss_Tot: 1.550559117047991, Const: (-2.626694594209944e-05, -0.0001602166757443957, 0.0007408252003314854, 0.004188515403411998, 0.002357468098490892, 0.0006869919943068919), W00: -1.5455759436892165\n",
            "\n",
            "Minimum Cons: 0.000000015814469733346123 at epoch 17458\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5517851897\n",
            "W10 diff: 0.0000114659\n",
            "W01 diff: -0.0000236909\n",
            "Comp3: 0.0007593674220700\n",
            "Comp4: 0.0040177523661934\n",
            "Comp5: 0.0022624731852789\n",
            "Comp6: 0.0006008600440788\n",
            "Coeffs: tensor([9.812829265049e-01, 2.435715939132e-01, 1.222385994304e-01,\n",
            "        5.580357193820e-04, 4.150462535471e-06, 4.914312569564e-08,\n",
            "        6.405656721570e-10, 5.311820297642e-02, 2.738130354860e-02,\n",
            "        1.063349828941e-03, 7.530239341006e-06, 9.429961165286e-08,\n",
            "        1.240744740459e-09, 3.220903303493e-03, 2.230741304312e-02,\n",
            "        1.431887972287e-04, 1.004913688285e-06, 8.484955755311e-09,\n",
            "        7.682978220417e-11, 1.536276098670e-03, 3.933469084278e-04,\n",
            "        2.166246531796e-05, 1.908529375984e-07, 1.453088736649e-09,\n",
            "        4.849654093036e-04, 1.051482864913e-05, 1.158692118184e-06,\n",
            "        2.383835931383e-08, 2.707350341888e-10, 1.997609259030e-04,\n",
            "        7.292611921659e-07, 3.286249123475e-08, 1.536892784454e-09,\n",
            "        3.923243851307e-08, 5.665016840587e-08, 9.646014254841e-10,\n",
            "        8.897501465152e-11, 9.573798798146e-11, 4.374610856555e-09,\n",
            "        4.312963772080e-11, 4.296897419958e-13, 2.520458577291e-10,\n",
            "        2.385138311709e-12, 1.733048556122e-15, 9.350122696265e-14,\n",
            "        7.245580504873e-18, 3.274342194808e-16, 3.056068250127e-20,\n",
            "        1.288999983252e-22], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.550527740337496, Const: (-0.00011870871650820902, -9.064250850610023e-05, 0.0007897200907743349, 0.004383722337118482, 0.0023294591229006435, 0.0006409549789185534), W00: -1.5461964292502581\n",
            "Epoch 30000, Loss_Tot: 1.5535213571905309, Const: (0.0002344958111168971, -0.00029223300572556177, 0.0009952489502770717, 0.004578868844345933, 0.0022077568448187904, 0.0006754871031043258), W00: -1.5475546447752606\n",
            "Epoch 35000, Loss_Tot: 1.5459219421530457, Const: (-4.5942968098122705e-05, -0.0001286004850888922, 0.0007771838768105921, 0.004659342478553607, 0.002389983019731983, 0.0006280927749943267), W00: -1.5417904484022174\n",
            "Epoch 40000, Loss_Tot: 1.5457056833882707, Const: (-4.4619671569101627e-05, -0.00013973874901718197, 0.0007635276096548293, 0.004743220149394983, 0.0024455317805909165, 0.0006520931646114859), W00: -1.5412382501042623\n",
            "\n",
            "Minimum Cons: 0.000000010126538696260745 at epoch 30648\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5424172718\n",
            "W10 diff: -0.0000075604\n",
            "W01 diff: 0.0000024712\n",
            "Comp3: 0.0007337626406499\n",
            "Comp4: 0.0045739421203526\n",
            "Comp5: 0.0023880602785134\n",
            "Comp6: 0.0012731295179049\n",
            "Coeffs: tensor([9.816715392864e-01, 2.434506321792e-01, 9.299640415104e-02,\n",
            "        3.020957953004e-04, 1.916143839279e-06, 1.761916146813e-08,\n",
            "        1.728581560151e-10, 5.326753277640e-02, 2.505230928574e-02,\n",
            "        7.359349375778e-04, 3.462044993607e-06, 3.351390519074e-08,\n",
            "        3.351936553113e-10, 2.686012124174e-03, 2.388992735853e-02,\n",
            "        9.479798842644e-05, 4.569422732048e-07, 2.712253473410e-09,\n",
            "        1.840170873427e-11, 1.492499126222e-03, 2.851505891409e-04,\n",
            "        1.403151337046e-05, 8.307306919036e-08, 4.495268849038e-10,\n",
            "        4.769005110259e-04, 7.673703200497e-06, 6.306955743749e-07,\n",
            "        1.007616164292e-08, 7.889618204188e-11, 1.919604713171e-04,\n",
            "        5.600775501284e-07, 1.384793590437e-08, 6.201883056621e-10,\n",
            "        2.610376345821e-08, 3.996176723576e-08, 4.142321656796e-10,\n",
            "        3.072392027678e-11, 3.445520630033e-11, 2.851587390061e-09,\n",
            "        1.686646616012e-11, 1.093408495626e-13, 1.704455412905e-10,\n",
            "        1.173871716168e-12, 3.248257156096e-16, 2.523441474262e-14,\n",
            "        9.824961283670e-19, 6.218390624266e-17, 2.990542452555e-21,\n",
            "        9.116384853317e-24], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 45000, Loss_Tot: 1.592717809618264, Const: (5.627625725845853e-05, -0.00043729186117835717, 0.0006883823941535098, 0.0046991676535936356, 0.0024703488946944087, 0.002214934568548042), W00: -1.5417145462989343\n",
            "Epoch 50000, Loss_Tot: 1.541821674435569, Const: (-4.8830254035570064e-05, -0.0001494762615155132, 0.0007451747478269356, 0.0049613898106961795, 0.00247975018643806, 0.0006182449061727643), W00: -1.5377521313308251\n",
            "Epoch 55000, Loss_Tot: 1.5445447997133348, Const: (-2.6750176322698493e-05, -0.0001404681222942905, 0.0009165119450808503, 0.005121477305218082, 0.002378080343448489, 0.0006306612800053726), W00: -1.540362994559213\n",
            "Epoch 60000, Loss_Tot: 1.5480435718201058, Const: (-0.00015490486957658867, 0.00015862051103465724, 0.0009086586593178202, 0.005217882029429691, 0.0023253193915473273, 0.0005799653371394584), W00: -1.5441884140458786\n",
            "\n",
            "Minimum Cons: 0.000000037559109484359946 at epoch 56973\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5372192118\n",
            "W10 diff: -0.0000442369\n",
            "W01 diff: -0.0000419987\n",
            "Comp3: 0.0007014965696523\n",
            "Comp4: 0.0050622714626453\n",
            "Comp5: 0.0025334549863204\n",
            "Comp6: 0.0006157399472542\n",
            "Coeffs: tensor([9.810865609424e-01, 2.567600749646e-01, 7.554734271432e-02,\n",
            "        1.649164186346e-04, 7.668997888356e-07, 4.949028225538e-09,\n",
            "        3.222408166194e-11, 5.271839389699e-02, 2.361515202504e-02,\n",
            "        4.445431694765e-04, 1.163063146837e-06, 7.644378458075e-09,\n",
            "        5.344636518495e-11, 1.937668340272e-03, 2.609809409778e-02,\n",
            "        4.847311891429e-05, 1.365328875582e-07, 4.798591853276e-10,\n",
            "        1.990623479340e-12, 1.414306948711e-03, 1.504998080593e-04,\n",
            "        6.364967455223e-06, 2.169400182791e-08, 7.063425123036e-11,\n",
            "        4.439806213565e-04, 3.991787963668e-06, 1.930505503884e-07,\n",
            "        2.201212998997e-09, 1.171192198007e-11, 1.812929176352e-04,\n",
            "        2.406701520909e-07, 2.786329863117e-09, 1.060055846381e-10,\n",
            "        1.978286201107e-08, 1.400858829348e-08, 8.292414089552e-11,\n",
            "        3.493031153802e-12, 6.621443508171e-12, 8.236435728711e-10,\n",
            "        3.419422864653e-12, 1.186932169516e-14, 4.885592176342e-11,\n",
            "        2.028293079551e-13, 2.380674498844e-17, 4.568568688204e-15,\n",
            "        4.843725554606e-20, 3.510122878208e-18, 9.869604498444e-23,\n",
            "        2.011036584496e-25], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
            "Epoch 65000, Loss_Tot: 1.5544021024811412, Const: (0.00015558416695671262, -0.00046204869216226463, 0.0008496393425642127, 0.005296331807836268, 0.0024015450321589744, 0.0012473075064259864), W00: -1.5364673880559103\n",
            "Epoch 70000, Loss_Tot: 1.5693258676706094, Const: (-0.0013997707374948654, 0.000734606433222984, 0.000788606925350141, 0.005336590225408332, 0.0024074055892244705, 0.0005494863460815472), W00: -1.5413164679325138\n",
            "Epoch 75000, Loss_Tot: 1.5983537068918467, Const: (-7.99155102884086e-05, -0.001182628032507349, 0.0008445155592289057, 0.005355013888097594, 0.0024478613686834006, 0.0022930201545675014), W00: -1.5317243370787503\n",
            "Epoch 80000, Loss_Tot: 1.5382047987339769, Const: (0.00028276941053761995, -0.0003468526118306503, 0.0008832369243980068, 0.005476885354790661, 0.00248776841703292, 0.0006392399123335366), W00: -1.5321158693400796\n",
            "\n",
            "Minimum Cons: 0.000000001625442391403438 at epoch 78298\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5343031427\n",
            "W10 diff: 0.0000012811\n",
            "W01 diff: -0.0000010667\n",
            "Comp3: 0.0008373191335583\n",
            "Comp4: 0.0054252449049783\n",
            "Comp5: 0.0024847374721168\n",
            "Comp6: 0.0009750347745883\n",
            "Coeffs: tensor([9.807937401549e-01, 2.616394627630e-01, 7.573910517377e-02,\n",
            "        1.402909615787e-04, 5.261582115530e-07, 2.767940735368e-09,\n",
            "        1.539763269266e-11, 5.257366813709e-02, 2.228362132746e-02,\n",
            "        3.636285048767e-04, 7.438090782422e-07, 3.583568922576e-09,\n",
            "        2.114003202308e-11, 1.571736786156e-03, 2.713410343328e-02,\n",
            "        3.323860591717e-05, 6.648285884815e-08, 1.674894018638e-10,\n",
            "        5.795567951655e-13, 1.381596565685e-03, 1.218311531594e-04,\n",
            "        3.958318789559e-06, 9.671718815029e-09, 2.421678595532e-11,\n",
            "        4.266681004134e-04, 2.527511611748e-06, 1.064287576359e-07,\n",
            "        8.683116495406e-10, 3.728013543730e-12, 1.581509576072e-04,\n",
            "        1.300539328541e-07, 1.231367564176e-09, 3.841837857922e-11,\n",
            "        3.553795761458e-09, 6.450308535489e-09, 3.034193011633e-11,\n",
            "        1.156980809711e-12, 1.433589668063e-12, 3.331269754569e-10,\n",
            "        1.129629137615e-12, 1.632811987514e-15, 9.840947990267e-12,\n",
            "        5.717651632217e-14, 2.078418290376e-18, 3.045015991057e-16,\n",
            "        2.675522502415e-21, 3.740167620806e-19, 3.453123093543e-24,\n",
            "        4.456721664050e-27], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbCws2MJDSVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPG8FECMHJ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking dM/M constraints for 1 derivative"
      ],
      "metadata": {
        "id": "X0rspKt2HKmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f212c4-3f37-478a-89ab-6f319808ac2f",
        "id": "h-q_ltbxHMrG"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.400000000000, 10.100000000000],\n",
              "        [ 1.400000000000, 10.100000000000],\n",
              "        [ 2.400000000000, 10.100000000000],\n",
              "        [ 3.400000000000, 10.100000000000],\n",
              "        [ 4.400000000000, 10.100000000000],\n",
              "        [ 5.400000000000, 10.100000000000],\n",
              "        [ 6.400000000000, 10.100000000000],\n",
              "        [ 7.400000000000, 10.100000000000],\n",
              "        [ 8.400000000000, 10.100000000000],\n",
              "        [ 9.400000000000, 10.100000000000],\n",
              "        [10.400000000000, 10.100000000000]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "#THIS SECTION IS JUST TO TEST THE EXPRESSIONS FOR THE OPEN STRING AMPLITUDE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.special import legendre\n",
        "from scipy import integrate\n",
        "from scipy.special import hyp2f1 as scipy_hyp2f1\n",
        "from scipy.special import gamma as scipy_gamma\n",
        "from scipy.special import zeta\n",
        "\n",
        "\n",
        "#This is all for to check formulaes using the Open String Amplitude\n",
        "#Some definitions\n",
        "d = 10;\n",
        "alpha = (d-3)/2;\n",
        "lm = 146/10;\n",
        "dlm = 1/2\n",
        "lmp = lm + dlm\n",
        "lmm = lm - dlm\n",
        "ellmax = 19;\n",
        "w00 = -(np.pi)**2/6\n",
        "w10 = -zeta(3)\n",
        "w01 = 7*np.pi**4/360\n",
        "\n",
        "torch.set_printoptions(precision=12)\n",
        "\n",
        "def hyp2f1(a, b, c, z):\n",
        "    if isinstance(z, torch.Tensor):\n",
        "        z_np = z.detach().numpy()\n",
        "        result = scipy_hyp2f1(a,b,c,z_np)\n",
        "        return torch.tensor(result, dtype=z.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_hyp2f1(a,b,c,z))\n",
        "\n",
        "def gamma(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x_np = x.detach().numpy()\n",
        "        result = scipy_gamma(x_np)\n",
        "        return torch.tensor(result, dtype=x.dtype)\n",
        "    else:\n",
        "        return torch.tensor(scipy_gamma(x))\n",
        "\n",
        "def ker(lm,n,s1,s2):\n",
        "    return (1/(s1 - n) + 1/(s2 - n) + 1/(n + lm))\n",
        "\n",
        "def geg(ell,beta, z):\n",
        "    return gamma(ell + 2*beta) / (gamma(2*beta) * gamma(ell + 1)) * \\\n",
        "           hyp2f1(-ell, ell + 2*beta, beta + 1/2, (1-z)/2)\n",
        "\n",
        "#ell,n tensor and Null grid tensor\n",
        "ell = torch.arange(ellmax + 1, dtype = torch.float64).unsqueeze(1).repeat(1, ellmax + 1)\n",
        "n = torch.arange(1, ellmax + 2, dtype = torch.float64).unsqueeze(0).repeat(ellmax + 1, 1)\n",
        "mask = (n > ell) & (n <= ellmax + 1) & ((n - ell - 1) % 2 == 0)\n",
        "ell_n_tensor = torch.stack([ell[mask], n[mask]], dim=1)\n",
        "\n",
        "NullPoints = torch.stack(torch.meshgrid(\n",
        "    torch.arange(4/10, 104/10 + 1e-6, 1, dtype=torch.float64),\n",
        "    torch.arange(101/10, 101/10 + 1e-6, 4/11, dtype=torch.float64),\n",
        "    indexing='ij'\n",
        ")).permute(1, 2, 0).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def w00comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    result = (-1/n)*geg(ell,alpha,1)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def w10comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-((2 * ell * (-3 + d + ell) * lm * (n + 2 * lm) * \\\n",
        "            hyp2f1(1 - ell, -2 + d + ell, d / 2, lm / (n + lm))) / ((-2 + d) * (n + lm)**2)) - \\\n",
        "            hyp2f1(-ell, -3 + d + ell, 1/2 * (-2 + d), lm / (n + lm)))) / (n**2 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def w01comp(ell_n_tensor):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "    result = (gamma(-3 + d + ell) * (-(1/((d - 2) * (n + lm)**2)) *\\\n",
        "             2 * ell * (-3 + d + ell) * n * (n + 2*lm) *  hyp2f1(1 - ell, -2 + d + ell, d/2, lm/(n + lm)) + \\\n",
        "             2 * hyp2f1(-ell, -3 + d + ell, (d - 2)/2, lm/(n + lm)))) / (n**3 * gamma(-3 + d) * gamma(1 + ell))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcomp(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lm,n, s1, s2) * geg(ell, alpha, 1 + 2 * (((s1 + lm) * (s2 + lm)) / (n + lm) - lm) / n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1P(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lmp,n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmp)*(s2 + lmp))/(n + lmp)-lmp)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD1M(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ker(lmm,n,s1,s2)*geg(ell,alpha,1 + 2*(((s1 + lmm)*(s2 + lmm))/(n + lmm)-lmm)/n)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD1(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = ((2 * (-3 + d) * (n**2 + 2 * n * lm - s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) /  (n * (n + lm))) / n - (n + lm) * geg(ell, 1/2 * (-3 + d),\n",
        "        1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm))) / (n * (n + lm)))) / (n + lm)**3)\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "def ampcompD2(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result = (1 / (n + lm)**4 * 2 * (-((2 * (-3 + d) * (-1 + d) * (n - s1) * (n - s2) *\n",
        "               (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "               geg(-2 + ell,(d+1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) /\n",
        "              (n**2 * (n + lm))) + ((-3 + d) * (n - s1) * (n - s2) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            ((-3 + d) * (2*n - s1 - s2) * (n + lm) *\n",
        "             geg(-1 + ell, (d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n -\n",
        "            (3 * (-3 + d) * (n**2 + 2*n*lm - s2*lm - s1*(s2 + lm)) *\n",
        "             geg(-1 + ell,(d-1)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))) / n +\n",
        "            (n + lm) * geg(ell,(d-3)/2, 1 + (2 * ((-n + s2)*lm + s1*(s2 + lm)))/(n*(n + lm)))))\n",
        "\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "\n",
        "def ampcompD3(ell_n_tensor, NullPoints):\n",
        "    ell = ell_n_tensor[:, 0].view(-1, 1)\n",
        "    n = ell_n_tensor[:, 1].view(-1, 1)\n",
        "    s1 = NullPoints[:, 0].view(1, -1)\n",
        "    s2 = NullPoints[:, 1].view(1, -1)\n",
        "    result =  (2 / (n**3 * (n + lm)**7)) * (4 * (-3 + d) * (-1 + d) * (1 + d) * (n - s1)**2 * (n - s2)**2 * (n**2 + 2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-3 + ell, (3 + d)/2, 1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * (-1 + d) * n * (n - s1)**2 * (n -s2)**2 * (n + lm) * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     4 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (2 * n - s1 -s2) * (n + lm)**2 * geg(-2 + ell, (1 + d)/2,\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     8 * (-3 + d) * (-1 + d) * n * (n - s1) * (n - s2) * (n + lm) * (n**2 +2 * n * lm - s2 * lm -\n",
        "        s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (n - s1) * (n -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "       1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "     2 * (-3 + d) * n**2 * (2 * n - s1 - s2) * (n + lm)**3 * geg(-1 + ell, 1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) +\n",
        "     n**3 * (n + lm)**3 * geg(ell, 1/2 * (-3 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +\n",
        "     4 * n * (n + lm) * (2 * (-3 + d) * (-1 + d) * (n - s1) * (n -s2) * (n**2 + 2 * n * lm - s2 * lm -\n",
        "     s1 * (s2 + lm)) * geg(-2 + ell, (1 + d)/2,1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))\n",
        "     - (-3 + d) * n * (n - s1) * (n -s2) * (n + lm) * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm))) - (-3 + d) * n * (2 * n - s1 -s2) * (n + lm)**2 * geg(-1 + ell, 1/2 * (-1 + d),\n",
        "      1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) +3 * (-3 + d) * n * (n + lm) * (n**2 + 2 * n * lm -\n",
        "      s2 * lm - s1 * (s2 + lm)) * geg(-1 + ell,1/2 * (-1 + d),1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/( n * (n + lm))) -\n",
        "        n**2 * (n + lm)**2 * geg(ell, 1/2 * (-3 + d),  1 + (2 * ((-n + s2) * lm + s1 * (s2 + lm)))/(n * (n + lm)))))\n",
        "    return result.squeeze(-1)\n",
        "\n",
        "#First, lets calculate cc(ell,n) for open string\n",
        "\n",
        "def AbsOpString(n,s2):\n",
        "    return (-1)**(1 + n)*gamma(-s2)/(gamma(n+1)*gamma(1 - n - s2))\n",
        "\n",
        "\n",
        "\n",
        "def cc(ell_n_tensor):\n",
        "    z = torch.linspace(-1 + 10**(-6), 1 - 10**(-6), 10**6).view(1, -1)\n",
        "\n",
        "    # Directly use ell and n from ell_n_tensor\n",
        "    ell_values = ell_n_tensor[:, 0].view(-1, 1)  # Shape (k, 1)\n",
        "    n_values = ell_n_tensor[:, 1].view(-1, 1)    # Shape (k, 1)\n",
        "\n",
        "    # Compute normfac for all combinations of ell and n\n",
        "    normfac = ((np.pi * 2**(1 - 2 * alpha) * gamma(2 * alpha + ell_values)) /\n",
        "                (gamma(ell_values + 1) * (ell_values + alpha) * gamma(alpha)**2))**-1\n",
        "\n",
        "    # Compute integrand\n",
        "    integrand = normfac * (1 - z**2)**(alpha - 1/2) * \\\n",
        "                AbsOpString(n_values, n_values * (z - 1) / 2) * \\\n",
        "                geg(ell_values, alpha, z)\n",
        "\n",
        "    # Integrate across z\n",
        "    return torch.trapz(integrand, z, dim=1)\n",
        "\n",
        "NullPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d42beb-e89c-48e2-9c39-c7e237e23536",
        "id": "ElvG-LaZHMrG"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-c351bcb08155>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(result, dtype=z.dtype)\n",
            "<ipython-input-31-c351bcb08155>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(scipy_hyp2f1(a,b,c,z))\n"
          ]
        }
      ],
      "source": [
        "#Store all the values\n",
        "coeff_vals = cc(ell_n_tensor)\n",
        "w00comp_vals = w00comp(ell_n_tensor)\n",
        "w01comp_vals = w01comp(ell_n_tensor)\n",
        "w10comp_vals = w10comp(ell_n_tensor)\n",
        "\n",
        "ampcomp_vals = ampcomp(ell_n_tensor, NullPoints)\n",
        "ampcompD1P_vals = ampcompD1P(ell_n_tensor, NullPoints)\n",
        "ampcompD1M_vals = ampcompD1M(ell_n_tensor, NullPoints)\n",
        "ampcompD1_vals = ampcompD1(ell_n_tensor, NullPoints)\n",
        "ampcompD2_vals = ampcompD2(ell_n_tensor, NullPoints)\n",
        "ampcompD3_vals = ampcompD3(ell_n_tensor, NullPoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250a17e9-99e5-4090-9737-a6b8d0994234",
        "id": "ujghcjyjHMrH"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.6449340668482264,\n",
              " -1.5961632442027958,\n",
              " -1.2020569031595942,\n",
              " -1.2020569018830987,\n",
              " 1.8940656589944915,\n",
              " 1.8940656589876617,\n",
              " 134345.68405250116,\n",
              " -2.5088225522098497,\n",
              " -1.8674381465275974e-05)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "#CHECKS FOR OPEN STRING\n",
        "def w00fullX():\n",
        "    return torch.dot(coeff_vals,w00comp_vals)\n",
        "\n",
        "def w10fullX():\n",
        "    return torch.dot(coeff_vals,w10comp_vals)\n",
        "\n",
        "def w01fullX():\n",
        "    return torch.dot(coeff_vals,w01comp_vals)\n",
        "\n",
        "def ampfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1PfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1MfullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3fullX():\n",
        "    return torch.sum(coeff_vals.view(-1, 1) * ampcompD3_vals, dim = 0)\n",
        "\n",
        "(w00, w00fullX().item(),w10, w10fullX().item(),w01, w01fullX().item(),ampfullX()[10].item(),\n",
        " ampD1fullX()[10].item(),(ampD1fullX()/ampfullX())[10].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND0JF567HMrH"
      },
      "outputs": [],
      "source": [
        "# Same definitions for the PINN\n",
        "def w00full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w00comp_vals)\n",
        "\n",
        "def w01full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w01comp_vals)\n",
        "\n",
        "def w10full():\n",
        "    return torch.dot(ImPW(ell_n_tensor),w10comp_vals)\n",
        "\n",
        "def ampfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcomp_vals, dim = 0)\n",
        "\n",
        "def ampD1Pfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1P_vals, dim = 0)\n",
        "\n",
        "def ampD1Mfull():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1M_vals, dim = 0)\n",
        "\n",
        "def ampD1full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD1_vals, dim = 0)\n",
        "\n",
        "def ampD2full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD2_vals, dim = 0)\n",
        "\n",
        "def ampD3full():\n",
        "    return torch.sum(ImPW(ell_n_tensor).view(-1, 1) * ampcompD3_vals, dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network. We will impose multi-lambda equality. But lets first check how well\n",
        "# this is satisfied with three derivatives imposed.\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.0001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "\n",
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a57c77-6809-4402-8975-27a85918972b",
        "id": "lw3nOBPFHMrI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 100722.53255268406, Const: (1.1221696229948912, 0.45418347064635767, 355885443.977414, 47382291.655973606, 8860561.751928331, 0.0808790677041104), W00: -86001.64543894853\n",
            "Epoch 5000, Loss_Tot: 52.75628831012998, Const: (-0.00024951706145492913, -0.00042757804670245925, 59898.05772812645, 7230.663778513237, 1260.5184169425427, 0.0656769607870327), W00: -9.619205710415885\n",
            "Epoch 10000, Loss_Tot: 15.440777086385776, Const: (-0.0010683769123445153, 0.0007088052042654169, 869.7901965697465, 85.17518762429057, 12.448907164212255, 0.037410532665230777), W00: -1.4288592029785412\n",
            "Epoch 15000, Loss_Tot: 5.432620542939739, Const: (0.00030332542323563416, -0.00039461618402358845, 176.52262769179492, 13.841697113409205, 1.6017781977652141, 0.02019045464569537), W00: -1.3535986724901787\n",
            "\n",
            "Minimum Cons: 0.000000335495058210502536 at epoch 13998\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.3322674776\n",
            "W10 diff: -0.0000009463\n",
            "W01 diff: -0.0000148251\n",
            "Comp3: 193.8522293524100348\n",
            "Comp4: 15.6559936312618966\n",
            "Comp5: 1.8766042346838105\n",
            "Comp6: 0.0225842405531242\n",
            "Coeffs: tensor([9.850551979244e-01, 9.353170334271e-02, 6.800498056609e-03,\n",
            "        7.344627816842e-04, 8.352828169731e-05, 9.478510581582e-06,\n",
            "        1.073136071430e-06, 1.214976473313e-07, 1.375563870360e-08,\n",
            "        1.557376552228e-09, 4.901150046697e-02, 4.096955599908e-03,\n",
            "        5.825450572337e-04, 8.598405446216e-05, 9.782271736180e-06,\n",
            "        1.112876215967e-06, 1.266054294667e-07, 1.440315421903e-08,\n",
            "        1.638561963864e-09, 1.864095359535e-10, 1.696445164094e-03,\n",
            "        3.491955379838e-04, 5.749401245844e-05, 8.143787785639e-06,\n",
            "        1.135144843918e-06, 1.414991705814e-07, 1.482663227419e-08,\n",
            "        1.686738566554e-09, 1.918903040636e-10, 6.629110651419e-04,\n",
            "        4.228228268880e-05, 5.842547622538e-06, 9.147103286201e-07,\n",
            "        1.258079578715e-07, 1.730344173892e-08, 2.358780122896e-09,\n",
            "        2.648363829298e-10, 2.723747404228e-11, 7.313469465787e-04,\n",
            "        3.681273275969e-05, 2.426415374622e-06, 1.862423511132e-07,\n",
            "        1.782412664394e-08, 2.111121234467e-09, 2.895814538636e-10,\n",
            "        3.982860567132e-11, 2.196325366254e-04, 1.324953656224e-05,\n",
            "        1.166673936062e-06, 1.032061473006e-07, 1.048634861008e-08,\n",
            "        7.414651152714e-10, 5.115419495297e-11, 5.227778903616e-12,\n",
            "        3.703634084974e-05, 4.429268150197e-06, 3.113603304452e-07,\n",
            "        2.569233823274e-08, 2.340211588062e-09, 2.340007640449e-10,\n",
            "        2.255273886977e-11, 6.245093951495e-06, 1.345630716705e-06,\n",
            "        7.710173002889e-08, 6.780023798946e-09, 5.657929393773e-10,\n",
            "        5.306457173535e-11, 5.187371941264e-12, 5.216458822949e-06,\n",
            "        4.041384186653e-07, 2.549914282732e-08, 1.819766964245e-09,\n",
            "        1.493086975591e-10, 1.266640571480e-11, 6.076597143994e-06,\n",
            "        8.324867135944e-08, 8.240736993827e-09, 4.448171185640e-10,\n",
            "        3.951217724726e-11, 3.288054401517e-12, 1.110621982895e-05,\n",
            "        1.403683095391e-08, 2.474970894918e-09, 1.463408632863e-10,\n",
            "        1.063575212003e-11, 1.810244769136e-05, 3.202681940397e-09,\n",
            "        7.433171235682e-10, 5.046681844105e-11, 2.629960135162e-12,\n",
            "        5.697162919943e-06, 1.719031904960e-09, 1.823878733353e-10,\n",
            "        1.515688542619e-11, 1.793390132962e-06, 1.469014079992e-09,\n",
            "        3.155079356635e-11, 4.552123215142e-12, 5.719203248986e-07,\n",
            "        1.452487714216e-09, 5.755140683506e-12, 1.828168573949e-07,\n",
            "        1.677148749922e-09, 1.644003945621e-12, 5.843820803163e-08,\n",
            "        2.972583427942e-09, 1.868002799151e-08, 5.342522771793e-09,\n",
            "        5.971152312808e-09, 1.908704837316e-09], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 2.24712452635974, Const: (-9.105549536547741e-05, -2.6291250110554998e-05, 20.202054820779647, 1.9450402683342611, 0.32361335628876026, 0.0095051266633091), W00: -1.343560374173558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 2*20000\n",
        "num_epochs = epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**2\n",
        "    beta4 = 10**2\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qXSQGOLInRn",
        "outputId": "38f75d96-0285-4b19-ed20-dbc7c919c550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 2.2457705267539296, Const: (3.495357213689765e-05, -0.00020360923966555866, 20.221730154432247, 1.9484820372101554, 0.3241630129298996, 0.009497254106240033), W00: -1.3433653864221748\n",
            "Epoch 5000, Loss_Tot: 4.186607574062043, Const: (0.0004335002647257724, 0.0006896404681322466, 74.08868530104097, 5.3113646623015285, 0.5492296984214411, 0.01664569309146374), W00: -1.4091813245619764\n",
            "Epoch 10000, Loss_Tot: 3.3307880522131565, Const: (1.9938022258969568e-05, -2.884761007271308e-05, 71.08413636762131, 5.10024506726909, 0.5315961216469218, 0.01382967110748439), W00: -1.4181777257078862\n",
            "Epoch 15000, Loss_Tot: 2.984441616922628, Const: (6.353915426915435e-06, -2.5099667815942084e-05, 59.46019729638826, 4.231672745798405, 0.4462672057093945, 0.012479433015115405), W00: -1.4270724294794468\n",
            "Epoch 20000, Loss_Tot: 1.9806320494752037, Const: (-8.035474356860028e-05, 1.1806849907314287e-05, 24.781966790592982, 2.737006192567001, 0.48189756647567034, 0.0072737553737051075), W00: -1.4514909142449772\n",
            "Epoch 25000, Loss_Tot: 1.6405540239421457, Const: (7.99690652522056e-05, -0.00014754550054996152, 23.488350182849196, 2.659447714268456, 0.45782685906919707, 0.005071419018878846), W00: -1.3830794680303864\n",
            "Epoch 30000, Loss_Tot: 1.2511494161793082, Const: (0.0007891005984270727, -0.0006915266701064127, 26.594883375765395, 2.451335242225896, 0.34445880616831526, 0.0004257276729130704), W00: -1.238328086765404\n",
            "Epoch 35000, Loss_Tot: 1.190163411726461, Const: (-0.0003135228575354976, 0.00032937150841227947, 16.66007286075197, 1.44176862080433, 0.18914412696457486, 0.0009300707112203908), W00: -1.1794452747202508\n",
            "\n",
            "Minimum Cons: 0.000000008526207176084501 at epoch 38012\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.1707442176\n",
            "W10 diff: -0.0000032021\n",
            "W01 diff: -0.0000077869\n",
            "Comp3: 17.0431859947658104\n",
            "Comp4: 1.4280486294253238\n",
            "Comp5: 0.1803773839332034\n",
            "Comp6: 0.0010126630019356\n",
            "Coeffs: tensor([9.882472713286e-01, 4.335332054588e-03, 1.776800347811e-05,\n",
            "        1.869609026592e-07, 2.196227259142e-09, 2.323629294256e-11,\n",
            "        2.240176041249e-13, 2.159720015638e-15, 2.082153571889e-17,\n",
            "        2.007372930537e-19, 4.803228391741e-02, 8.169825400227e-05,\n",
            "        6.602670620844e-07, 6.721930495985e-09, 6.450477397643e-11,\n",
            "        6.179949695269e-13, 5.920767701442e-15, 5.672455586696e-17,\n",
            "        5.434557477268e-19, 5.206636618362e-21, 5.972738355947e-05,\n",
            "        8.934954311603e-07, 9.518921685215e-09, 1.092902280220e-10,\n",
            "        1.268416982548e-12, 1.387883895531e-14, 1.476987120467e-16,\n",
            "        1.571810841706e-18, 1.672722319559e-20, 6.528888562795e-06,\n",
            "        4.950535332957e-08, 2.807528801911e-10, 3.277796181923e-12,\n",
            "        3.064149898943e-14, 2.864429049908e-16, 2.817562934379e-18,\n",
            "        3.270049609973e-20, 3.795203408311e-22, 7.632664476575e-06,\n",
            "        1.119922035609e-07, 4.382467631713e-10, 1.429927557504e-12,\n",
            "        3.132393698845e-15, 1.737848454690e-17, 1.624575742743e-19,\n",
            "        1.518686129844e-21, 1.239597658173e-05, 1.405789440533e-07,\n",
            "        1.219200947471e-09, 4.534751121853e-12, 1.510159848654e-14,\n",
            "        3.542878287486e-17, 5.802536330580e-20, 1.327918656811e-22,\n",
            "        4.997886518335e-06, 1.933202281598e-07, 3.054778039074e-09,\n",
            "        1.310252921726e-11, 4.692326209994e-14, 1.594893920687e-16,\n",
            "        4.007154836433e-19, 1.974003964621e-06, 7.531410677692e-08,\n",
            "        3.930885366103e-09, 3.325577996037e-11, 1.355782039886e-13,\n",
            "        4.855376771361e-16, 1.684382365555e-18, 2.077191675389e-06,\n",
            "        2.531964672014e-08, 1.114155723709e-09, 5.288510471621e-11,\n",
            "        3.620383821585e-13, 1.402893219459e-15, 1.627404379292e-05,\n",
            "        8.512143698310e-09, 3.883429358285e-10, 2.365552842990e-11,\n",
            "        3.868783879651e-13, 3.906698588585e-15, 9.985503802906e-07,\n",
            "        2.404757632920e-09, 1.418422355557e-10, 6.421174182758e-12,\n",
            "        2.761786086261e-13, 7.397778172093e-08, 9.813583766562e-10,\n",
            "        4.768555796987e-11, 1.995540284929e-12, 1.423557219542e-13,\n",
            "        5.140246982294e-09, 7.041193988629e-10, 1.603127890588e-11,\n",
            "        7.946090148081e-13, 3.232304422424e-10, 1.020608751268e-09,\n",
            "        4.462110723221e-12, 2.671374579566e-13, 2.116827455938e-11,\n",
            "        7.559523194586e-09, 1.173246585250e-12, 1.509531053127e-12,\n",
            "        1.591494346690e-09, 5.299767606944e-13, 1.076461850465e-13,\n",
            "        8.116498911216e-11, 7.676358251163e-15, 4.596367135931e-12,\n",
            "        5.474088652072e-16, 3.948474320797e-17], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 1.1849640434797941, Const: (0.0006376825330181024, -0.0005401775074085347, 15.204657573855853, 1.289535124299352, 0.1651954156363349, 0.0009910406837717255), W00: -1.168158119586622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network. We will impose multi-lambda equality. But lets first check how well\n",
        "# this is satisfied with three derivatives imposed.\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
        "        self.output = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.input(x))\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = F.softplus(self.output(x))\n",
        "        return x.squeeze()  # Remove extra dimensions for output\n",
        "\n",
        "# Create the neural network\n",
        "input_size = 2  # Since ell_n_tensor has 2 columns\n",
        "hidden_size = 64\n",
        "output_size = 1  # Since coeff(ell,n) it a 1D tensor\n",
        "ImPW = SimpleNN(input_size, hidden_size, output_size)\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(ImPW.parameters(), lr=lr)  # Adam optimizer with learning rate of 0.0001\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "\n",
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U63DWB5vKubg",
        "outputId": "86ba7ad5-65f0-4098-8fd9-0ebb45727723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1967347.0108321188, Const: (1.0287391234512722, 0.274083475532225, 286517923.46272576, 37823892.469479576, 7030747.432997793, 0.08734472476811098), W00: -71010.98067129756\n",
            "Epoch 5000, Loss_Tot: 41184.8615862186, Const: (0.0020879438877416945, -0.003878113911989667, 859.2354806696462, 69.53265610364494, 8.418875535376733, 0.02028880072008237), W00: -1.9188431043685934\n",
            "Epoch 10000, Loss_Tot: 181.43670341216654, Const: (-1.7802646110398612e-05, -2.9465846507648052e-05, 95.81431861895061, 8.557739044879659, 1.1597152178017833, 0.0013407718846606624), W00: -1.668593572197147\n",
            "Epoch 15000, Loss_Tot: 16.164049598868964, Const: (-2.0404052172295906e-06, 3.576857371401232e-06, 121.09351768636348, 11.52857317142221, 1.6660762085458154, 0.00037859956280881526), W00: -1.8302697458042487\n",
            "\n",
            "Minimum Cons: 0.000005060087423376469576 at epoch 6623\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.6081960128\n",
            "W10 diff: -0.0016383396\n",
            "W01 diff: -0.0028643917\n",
            "Comp3: 84.4584318896035029\n",
            "Comp4: 7.4460761231679102\n",
            "Comp5: 0.9943703887521655\n",
            "Comp6: 0.0015334375230323\n",
            "Coeffs: tensor([9.966910310086e-01, 9.136696581233e-02, 1.442782708494e-02,\n",
            "        2.070033176180e-03, 3.188449497495e-04, 4.905807183328e-05,\n",
            "        7.443454760997e-06, 1.167341300096e-06, 1.867908480987e-07,\n",
            "        2.988912202600e-08, 7.046914527968e-02, 1.213362409407e-02,\n",
            "        2.180536925208e-03, 4.761406698295e-04, 9.386483553649e-05,\n",
            "        1.521246255604e-05, 2.472188580453e-06, 3.995163725940e-07,\n",
            "        6.392815708187e-08, 1.022938976277e-08, 1.259261521161e-02,\n",
            "        2.173025785854e-03, 4.170053209319e-04, 8.255119732415e-05,\n",
            "        1.734515118094e-05, 3.782173402653e-06, 7.719783109518e-07,\n",
            "        1.300660502092e-07, 2.119461466912e-08, 2.491524430245e-03,\n",
            "        3.567613047721e-04, 7.081958864303e-05, 1.558617648158e-05,\n",
            "        3.479656522401e-06, 7.273174920715e-07, 1.547686653952e-07,\n",
            "        3.199037979423e-08, 6.320822830620e-09, 4.554217860739e-04,\n",
            "        6.251968767031e-05, 1.319229262292e-05, 2.738496365795e-06,\n",
            "        5.638111983733e-07, 1.250798862194e-07, 2.843149833548e-08,\n",
            "        6.243981690274e-09, 1.077489347488e-04, 1.090501671528e-05,\n",
            "        2.404280067481e-06, 4.582133736657e-07, 9.780505518617e-08,\n",
            "        2.008805741634e-08, 4.208661308559e-09, 9.110964868975e-10,\n",
            "        3.001363236090e-05, 1.990139572537e-06, 4.073879666614e-07,\n",
            "        8.126488019478e-08, 1.581046872487e-08, 3.280022021297e-09,\n",
            "        6.827456504573e-10, 2.455321282576e-05, 4.379426642165e-07,\n",
            "        6.594549590548e-08, 1.454664038112e-08, 2.709824790805e-09,\n",
            "        5.444736865321e-10, 1.111743105973e-10, 4.041907530084e-05,\n",
            "        1.249308041085e-07, 1.001405022586e-08, 2.473044580529e-09,\n",
            "        5.049493441177e-10, 9.033797016876e-11, 6.755057037975e-05,\n",
            "        3.911008240292e-08, 2.288735262757e-09, 4.223774383822e-10,\n",
            "        8.829671538199e-11, 1.740880038149e-11, 3.373963112905e-05,\n",
            "        1.369624972184e-08, 5.963038280532e-10, 7.058337562581e-11,\n",
            "        1.508790444482e-11, 4.061386418263e-06, 5.211562662574e-09,\n",
            "        1.858277074935e-10, 1.288540194179e-11, 2.627453438460e-12,\n",
            "        4.844582181301e-07, 3.680989539313e-09, 5.929118920236e-11,\n",
            "        3.090182579566e-12, 5.928760087969e-08, 4.325836147806e-09,\n",
            "        1.962089591571e-11, 9.048609461132e-13, 7.396864065680e-09,\n",
            "        6.967943944096e-09, 5.811667894654e-12, 9.248774242773e-10,\n",
            "        9.285256666785e-09, 1.806019660631e-12, 1.156433645286e-10,\n",
            "        3.557622884756e-09, 1.466594417625e-11, 1.364257343593e-09,\n",
            "        1.872921795248e-12, 2.396715795734e-13], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 12.194952959790575, Const: (-0.0001349031086592678, 8.22661545891723e-05, 138.7319775712331, 13.175600795421367, 1.9018683350554606, 0.0003204376816169127), W00: -1.9019556108715607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 2*20000\n",
        "num_epochs = epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsT2l6gANXji",
        "outputId": "2cef774a-6e63-42d0-94fe-8d0d78e106f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 12.18120843967089, Const: (9.425371840188213e-05, -5.6047941565662285e-05, 138.6478764152385, 13.16805954420045, 1.9008437354306413, 0.0003204299276300041), W00: -1.9016494523875926\n",
            "Epoch 5000, Loss_Tot: 9.421290559669394, Const: (0.00018047860882530742, -0.00011135931838324176, 183.30516782840454, 17.249622716063175, 2.4716303619906737, 0.00027061256624149335), W00: -2.053201032854433\n",
            "Epoch 10000, Loss_Tot: 7.8754679222462025, Const: (5.008359059943679e-05, -2.815724100746486e-05, 200.60975562271508, 18.738959275717114, 2.6691959838593013, 0.0002408190737893818), W00: -2.072784095900146\n",
            "Epoch 15000, Loss_Tot: 7.5002155328307865, Const: (0.0005652718744237895, -0.0003455677550450176, 174.35014718483797, 16.234891945585613, 2.3066714187504544, 0.00022627762337037587), W00: -1.9411098836747802\n",
            "Epoch 20000, Loss_Tot: 6.355232758210838, Const: (8.613813264890524e-07, 6.234999363474714e-07, 157.87692566377933, 14.641823042634101, 2.0731526237471427, 0.00021221287213189132), W00: -1.8518013176342327\n",
            "Epoch 25000, Loss_Tot: 5.8993576166786035, Const: (4.5654280800278e-06, -2.867770650727408e-06, 144.48007802489025, 13.378781985312731, 1.8923220784196206, 0.00020278626370773514), W00: -1.7871016745822352\n",
            "Epoch 30000, Loss_Tot: 5.5415812201472106, Const: (-1.7130435721046666e-06, -1.0458489105857893e-06, 135.5946754898667, 12.53879898193688, 1.7726688265888566, 0.00019458366091974484), W00: -1.7552970821359632\n",
            "Epoch 35000, Loss_Tot: 5.63349628228068, Const: (-0.000522487299717822, 0.0003200337687656596, 118.16939718210504, 10.90510587091274, 1.5399442889635908, 0.00018894480415550886), W00: -1.6880677890275466\n",
            "\n",
            "Minimum Cons: 0.000000000024405955679415 at epoch 27714\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.7726048721\n",
            "W10 diff: -0.0000000468\n",
            "W01 diff: -0.0000001126\n",
            "Comp3: 140.5829895868500898\n",
            "Comp4: 13.0086769305290275\n",
            "Comp5: 1.8393558998982176\n",
            "Comp6: 0.0002001752676833\n",
            "Coeffs: tensor([9.960424411420e-01, 9.097811645968e-02, 1.225836333596e-02,\n",
            "        1.555604621690e-03, 2.651413578615e-04, 4.536091232567e-05,\n",
            "        8.018569357164e-06, 1.574425013540e-06, 3.306003472806e-07,\n",
            "        6.965784557009e-08, 7.115570735513e-02, 1.155232714143e-02,\n",
            "        2.049690433594e-03, 4.276541274429e-04, 8.670147744434e-05,\n",
            "        1.592250634869e-05, 2.923264761620e-06, 5.670657787941e-07,\n",
            "        1.188792893029e-07, 2.504799088066e-08, 1.426807176509e-02,\n",
            "        2.118837206786e-03, 3.882396830045e-04, 9.050212956829e-05,\n",
            "        2.263491463755e-05, 5.498983056738e-06, 1.110599696246e-06,\n",
            "        2.066391783176e-07, 4.312545671212e-08, 4.599347890841e-03,\n",
            "        5.161829064828e-04, 1.052041347978e-04, 2.458664669908e-05,\n",
            "        5.690022658556e-06, 1.382105439034e-06, 3.356808258007e-07,\n",
            "        7.593588957429e-08, 1.627478415174e-08, 1.853209211825e-03,\n",
            "        1.512448544506e-04, 2.861592794869e-05, 7.174570808075e-06,\n",
            "        1.674934383032e-06, 3.975580154133e-07, 9.705035008608e-08,\n",
            "        2.364328371562e-08, 7.474874250109e-04, 5.099505882055e-05,\n",
            "        9.640535413867e-06, 1.974831558967e-06, 4.892594332042e-07,\n",
            "        1.178459086125e-07, 2.807820824719e-08, 6.921973034634e-09,\n",
            "        1.301156873130e-04, 2.667459785681e-05, 3.521600070752e-06,\n",
            "        6.405498096312e-07, 1.362846359196e-07, 3.480829306098e-08,\n",
            "        8.291458021263e-09, 2.264355944272e-05, 9.767054010896e-06,\n",
            "        1.309970872578e-06, 2.350871171307e-07, 4.273732498380e-08,\n",
            "        9.758896870721e-09, 2.457790391717e-09, 2.153549452035e-05,\n",
            "        2.599837059577e-06, 5.260700818311e-07, 7.698964065474e-08,\n",
            "        1.526334492205e-08, 2.955534233837e-09, 7.727731547421e-05,\n",
            "        5.757922142474e-07, 1.865548968437e-07, 2.542188338400e-08,\n",
            "        4.581555162802e-09, 9.232517709372e-10, 4.895466529228e-07,\n",
            "        1.637472679335e-07, 5.193552847268e-08, 1.008252746905e-08,\n",
            "        1.375232476141e-09, 9.436318200901e-09, 6.331776242475e-08,\n",
            "        1.533882252281e-08, 3.634419237586e-09, 4.888950141836e-10,\n",
            "        4.158425552978e-10, 3.316370855766e-08, 4.943039778546e-09,\n",
            "        1.017261593761e-09, 2.389680075461e-11, 4.755253396138e-08,\n",
            "        1.933337530159e-09, 3.246249641063e-10, 1.374682529941e-12,\n",
            "        1.107380731190e-07, 8.500633088043e-10, 7.983136037309e-14,\n",
            "        1.510727676307e-08, 4.149868777101e-10, 4.644148415959e-15,\n",
            "        1.471519896643e-09, 2.701709504718e-16, 1.551490583572e-10,\n",
            "        1.571705637744e-17, 9.143316879197e-19], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 19.955243497658373, Const: (-0.0031939451463209867, 0.001984363963012603, 14.29187069692261, 5.528008327093887, 1.2930551212702919, 0.00020283691369814367), W00: -1.701976206389086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l827sRcO8uM",
        "outputId": "b97860cd-1df5-42ce-f2ba-5bc0f4046714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 13.243121498108032, Const: (0.0019666562860753967, -0.001229459671541866, 8.460292600793133, 3.009772108271091, 0.8056927671425375, 0.0002488321827587895), W00: -1.6720679489499661\n",
            "Epoch 5000, Loss_Tot: 1.9896089358677966, Const: (-5.794485038457253e-07, -5.558786959625195e-07, 13.599134632034012, 4.6733550340740635, 1.0712386312522908, 5.911696671246713e-05), W00: -1.6401267157778088\n",
            "Epoch 10000, Loss_Tot: 1.9127887900316898, Const: (-1.6143588088546323e-07, -1.202403129507701e-06, 12.595633126243717, 4.276951826019244, 0.9765447790622456, 5.498681780125056e-05), W00: -1.6104323050060676\n",
            "Epoch 15000, Loss_Tot: 1.8772867860590048, Const: (2.8437561689909074e-05, -1.9049564644380013e-05, 11.97739756344299, 4.067517470051273, 0.9278111445833946, 5.33997645731295e-05), W00: -1.5909617195844314\n",
            "\n",
            "Minimum Cons: 0.000000000037401061169227 at epoch 4664\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.6403033410\n",
            "W10 diff: 0.0000003512\n",
            "W01 diff: -0.0000005246\n",
            "Comp3: 13.8490298485373913\n",
            "Comp4: 4.7135947497519339\n",
            "Comp5: 1.0793731655855607\n",
            "Comp6: 0.0000592450654570\n",
            "Coeffs: tensor([9.935470713359e-01, 8.202833400699e-02, 1.006276379935e-02,\n",
            "        1.250703425872e-03, 2.256512740032e-04, 4.122583804439e-05,\n",
            "        6.731438586130e-06, 1.087332214696e-06, 1.976355893106e-07,\n",
            "        3.662957728853e-08, 6.506405190852e-02, 9.953887725395e-03,\n",
            "        1.712742765151e-03, 3.579945338651e-04, 7.201910672824e-05,\n",
            "        1.440183462376e-05, 2.783428299527e-06, 4.977773259058e-07,\n",
            "        9.225754999137e-08, 1.709891889711e-08, 1.032039103424e-02,\n",
            "        1.619780841784e-03, 3.068491134218e-04, 7.674206121945e-05,\n",
            "        2.049294054860e-05, 5.841433389911e-06, 1.283336744392e-06,\n",
            "        2.409489707284e-07, 4.519924745846e-08, 2.920490945049e-03,\n",
            "        3.248022054167e-04, 7.257877910127e-05, 1.950794105563e-05,\n",
            "        4.915511089333e-06, 1.608947546438e-06, 5.446479136035e-07,\n",
            "        1.138911339476e-07, 2.239374511514e-08, 1.012616734493e-03,\n",
            "        8.579755783724e-05, 1.785196370488e-05, 5.443841945619e-06,\n",
            "        1.499687362673e-06, 4.694856361850e-07, 1.849047612530e-07,\n",
            "        4.728489000830e-08, 4.347125762967e-04, 2.661642293632e-05,\n",
            "        5.712736161810e-06, 1.320156735495e-06, 4.750200355811e-07,\n",
            "        1.573893215521e-07, 5.837325420291e-08, 1.861804740009e-08,\n",
            "        1.168591019281e-04, 1.367353776573e-05, 1.972885186240e-06,\n",
            "        4.312658339522e-07, 1.282404202879e-07, 5.477275094449e-08,\n",
            "        2.048388832948e-08, 1.922358532711e-05, 7.127198509677e-06,\n",
            "        7.225260595756e-07, 1.605333647411e-07, 4.246652089796e-08,\n",
            "        1.596279265301e-08, 7.209328476797e-09, 1.488785852246e-05,\n",
            "        3.259488934458e-06, 3.899438377178e-07, 5.975655147304e-08,\n",
            "        1.754918586307e-08, 5.955337461046e-09, 4.000289309492e-05,\n",
            "        1.458316187664e-06, 3.493950223403e-07, 3.211191771383e-08,\n",
            "        8.300111240758e-09, 2.819106631680e-09, 2.378292380895e-07,\n",
            "        8.760798900293e-07, 2.150824893598e-07, 2.797419690743e-08,\n",
            "        3.929063434272e-09, 1.736300186976e-09, 5.431777111203e-07,\n",
            "        1.094251122141e-07, 2.610436130406e-08, 2.518315094470e-09,\n",
            "        4.262212128989e-11, 3.730502698786e-07, 7.567139888005e-08,\n",
            "        1.803333856829e-08, 1.109240803136e-12, 9.197614532477e-07,\n",
            "        5.502964665306e-08, 8.167093179563e-09, 3.123554086651e-14,\n",
            "        3.272554918273e-06, 4.592084738205e-08, 8.925286824741e-16,\n",
            "        1.716523567413e-05, 3.934652957599e-08, 2.553849310601e-17,\n",
            "        6.315183824071e-07, 7.307492105663e-19, 2.279904554638e-08,\n",
            "        2.126141250474e-20, 6.514191591659e-22], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 1.8476110136544173, Const: (-3.308397222845372e-07, -1.4153078176093459e-06, 11.35619503556039, 3.861284925072908, 0.8797526318356633, 5.218445915458618e-05), W00: -1.5752871233776096\n",
            "Epoch 25000, Loss_Tot: 1.9468945481399222, Const: (-2.966951807503726e-05, 1.699082777029126e-05, 7.495864346778057, 3.5708927935761587, 0.8670101514085828, 6.006918965138575e-05), W00: -1.5848948250713826\n",
            "Epoch 30000, Loss_Tot: 1.8107391986106638, Const: (-3.0349630253390103e-07, -1.206398980047041e-06, 10.53073045506547, 3.571556014089791, 0.8110503577006515, 5.0464809027383474e-05), W00: -1.5560679560851307\n",
            "Epoch 35000, Loss_Tot: 1.8222908394569912, Const: (-0.00013688771223230312, 8.560640194743563e-05, 10.84821978432311, 3.5282990294984145, 0.7943775887210937, 5.0243032208188746e-05), W00: -1.5437879090951023\n",
            "\n",
            "Minimum Cons: 0.000000000034925744091728 at epoch 35314\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5437372547\n",
            "W10 diff: -0.0000000843\n",
            "W01 diff: -0.0000007006\n",
            "Comp3: 10.0903683652430320\n",
            "Comp4: 3.4151816124663474\n",
            "Comp5: 0.7738712655333139\n",
            "Comp6: 0.0000494951811924\n",
            "Coeffs: tensor([9.923159078082e-01, 7.458897202721e-02, 8.308910071256e-03,\n",
            "        9.595147203834e-04, 1.701896791295e-04, 2.306280679863e-05,\n",
            "        3.036579964529e-06, 3.902066330355e-07, 5.014227936126e-08,\n",
            "        6.747383681075e-09, 6.182975233752e-02, 9.113256903731e-03,\n",
            "        1.510289197295e-03, 2.906092782121e-04, 5.408490109994e-05,\n",
            "        8.618586746231e-06, 1.217665665142e-06, 1.680212850314e-07,\n",
            "        2.306536265590e-08, 3.188719983255e-09, 8.352770010329e-03,\n",
            "        1.325423636627e-03, 2.685513862950e-04, 6.789906233335e-05,\n",
            "        1.716283287838e-05, 3.743088337684e-06, 5.429692974559e-07,\n",
            "        7.813882333779e-08, 1.124497190980e-08, 2.227437832189e-03,\n",
            "        2.427567775845e-04, 5.433683606002e-05, 1.714171363235e-05,\n",
            "        4.506356818471e-06, 1.198961113227e-06, 2.152057163876e-07,\n",
            "        3.491016009394e-08, 5.136161006890e-09, 7.648704274670e-04,\n",
            "        6.286813829902e-05, 1.285381292754e-05, 4.134790421760e-06,\n",
            "        1.247922931185e-06, 3.468165156831e-07, 8.141061229351e-08,\n",
            "        1.358460978025e-08, 3.435229736883e-04, 1.874954761455e-05,\n",
            "        4.227221723639e-06, 9.987834735651e-07, 3.146317781870e-07,\n",
            "        1.089739186626e-07, 3.393413808113e-08, 7.365537317421e-09,\n",
            "        1.030532750965e-04, 1.002782407664e-05, 1.425396986518e-06,\n",
            "        3.214898497443e-07, 8.570590679748e-08, 3.471477249640e-08,\n",
            "        1.186393673539e-08, 1.702296649822e-05, 5.301383519234e-06,\n",
            "        5.232237629348e-07, 1.098655200449e-07, 3.008386259729e-08,\n",
            "        9.411701002591e-09, 3.984461285804e-09, 7.017766850063e-06,\n",
            "        2.936505124575e-06, 2.782885650516e-07, 4.155950703685e-08,\n",
            "        1.138000773845e-08, 3.240435502481e-09, 3.302550147792e-05,\n",
            "        1.378083254031e-06, 2.254637782954e-07, 1.962426412015e-08,\n",
            "        4.326968497684e-09, 1.236857453248e-09, 1.031753745394e-07,\n",
            "        8.658669726748e-07, 2.249805344838e-07, 1.549760193828e-08,\n",
            "        1.650336329643e-09, 6.343808173981e-10, 6.486331792475e-07,\n",
            "        1.203808515507e-07, 1.831704335020e-08, 1.303423695799e-09,\n",
            "        1.097897470842e-11, 4.690292257389e-07, 8.384458267200e-08,\n",
            "        1.978141936940e-08, 1.753269951061e-13, 9.891878086193e-07,\n",
            "        6.534451350402e-08, 1.080757740351e-08, 2.799856637722e-15,\n",
            "        3.546982839956e-06, 5.646761059826e-08, 4.471186645875e-17,\n",
            "        1.266666731006e-05, 5.149727691783e-08, 7.539480407209e-19,\n",
            "        5.182319029715e-08, 1.310056099587e-20, 1.694293107328e-10,\n",
            "        2.276346500514e-22, 3.955367554135e-24], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 1.7711924868003268, Const: (-3.351246250504758e-06, 6.015034115236517e-07, 9.74116840844519, 3.298401992384408, 0.7472788116544952, 4.883663687205132e-05), W00: -1.5326791840452811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifh3oBIJRERb",
        "outputId": "37a7a1dc-e46c-46af-b397-1393d50a5ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.7711369355017341, Const: (2.3665809543160776e-06, -3.0433615085811994e-06, 9.706588190317065, 3.292864795718211, 0.7462667167381815, 4.8836012293133896e-05), W00: -1.5326264630775364\n",
            "Epoch 5000, Loss_Tot: 1.8001005614326475, Const: (-0.0001508017431663955, 9.883800710253077e-05, 10.005098278206713, 3.3379123838000058, 0.754673553761852, 4.9071228845135006e-05), W00: -1.526791894005463\n",
            "Epoch 10000, Loss_Tot: 1.7545510600528416, Const: (1.0903582643884135e-05, -8.39680876718596e-06, 9.351710326949952, 3.1892946168078895, 0.7235254181812751, 4.858085228254207e-05), W00: -1.5183517446910795\n",
            "Epoch 15000, Loss_Tot: 1659.9005552707595, Const: (-0.008089582426125785, -0.014038011054296362, 859.6852599445373, 72.21992606064113, 9.196443651624103, 0.003734979928683595), W00: -2.3859503141982534\n",
            "\n",
            "Minimum Cons: 0.000000000039467013995111 at epoch 11186\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5151262769\n",
            "W10 diff: -0.0000003958\n",
            "W01 diff: -0.0000007195\n",
            "Comp3: 9.4808155468431128\n",
            "Comp4: 3.1696829617827778\n",
            "Comp5: 0.7161492926317181\n",
            "Comp6: 0.0000480633710654\n",
            "Coeffs: tensor([9.918937924799e-01, 7.184662195348e-02, 7.569880366426e-03,\n",
            "        8.465701113945e-04, 1.294747275246e-04, 1.573940697784e-05,\n",
            "        1.749861829078e-06, 1.945433769074e-07, 2.162862159150e-08,\n",
            "        2.456185874322e-09, 6.072943706723e-02, 8.746040724014e-03,\n",
            "        1.423273072241e-03, 2.568795867909e-04, 4.601584991441e-05,\n",
            "        6.162110179923e-06, 7.423326646021e-07, 8.828485884861e-08,\n",
            "        1.069641253030e-08, 1.295955372301e-09, 7.716493399012e-03,\n",
            "        1.194613146276e-03, 2.381291956115e-04, 6.263588104334e-05,\n",
            "        1.512204173558e-05, 2.720725479368e-06, 3.752938124914e-07,\n",
            "        4.814803138891e-08, 5.833516685548e-09, 2.032420124228e-03,\n",
            "        2.124554032740e-04, 4.644457797199e-05, 1.501086700458e-05,\n",
            "        4.079801841475e-06, 9.257885625214e-07, 1.505002292617e-07,\n",
            "        2.318435154803e-08, 3.182579021046e-09, 7.033221210664e-04,\n",
            "        5.583236021619e-05, 1.130130956299e-05, 3.440244837258e-06,\n",
            "        1.083519599439e-06, 3.083125470985e-07, 5.415137022753e-08,\n",
            "        8.648599384212e-09, 3.240161204828e-04, 1.667604598975e-05,\n",
            "        3.710960162409e-06, 8.729435521990e-07, 2.689161008839e-07,\n",
            "        8.377380050350e-08, 2.076261086047e-08, 3.541781939960e-09,\n",
            "        9.586158410304e-05, 9.020800571207e-06, 1.248440133428e-06,\n",
            "        2.805694346515e-07, 6.742818384283e-08, 2.215417300092e-08,\n",
            "        6.535229550926e-09, 1.577531899527e-05, 4.880095490067e-06,\n",
            "        4.657914970674e-07, 9.438897978721e-08, 2.121259172962e-08,\n",
            "        5.489151260186e-09, 2.092910437160e-09, 6.167265277550e-06,\n",
            "        2.734484381310e-06, 2.414185898567e-07, 3.175427489450e-08,\n",
            "        7.743896576997e-09, 1.974252330880e-09, 3.079099383601e-05,\n",
            "        1.317808531233e-06, 2.025011326269e-07, 1.591301141524e-08,\n",
            "        3.052118283750e-09, 7.834225483715e-10, 5.796740469802e-08,\n",
            "        8.434708839996e-07, 1.927406278503e-07, 1.121703543611e-08,\n",
            "        1.211141173377e-09, 3.297782440697e-10, 6.457036058090e-07,\n",
            "        1.082582973826e-07, 1.560939466703e-08, 7.450282087179e-10,\n",
            "        3.197583343874e-12, 4.752400476238e-07, 7.800608948418e-08,\n",
            "        1.421750213708e-08, 3.097650973773e-14, 9.527578430460e-07,\n",
            "        6.204676646562e-08, 8.614405683153e-09, 2.982846679920e-16,\n",
            "        3.351703795830e-06, 5.489619089323e-08, 3.046622800145e-18,\n",
            "        1.171709499502e-05, 5.112336562322e-08, 3.273847797316e-20,\n",
            "        1.776526010053e-08, 3.518019821647e-22, 1.609185557127e-11,\n",
            "        3.780402826193e-24, 4.062355032894e-26], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 1015.8542700340614, Const: (-0.001208045467484098, -0.0020741975978184612, 1134.806843374839, 91.40310922975777, 11.113816736716775, 0.0031738309163228933), W00: -2.772331967054983\n",
            "Epoch 25000, Loss_Tot: 556.8461495881002, Const: (-0.00016371876291398912, -0.00027992659758169225, 2951.758732910091, 178.47061196650117, 16.664199452205086, 0.0023464206355813823), W00: -6.1720069465226945\n",
            "Epoch 30000, Loss_Tot: 361.44022003726906, Const: (5.43835305937268e-05, -1.2838373211421228e-05, 10855.008081565025, 746.7611583211881, 76.77917549985857, 0.0018642010758877666), W00: -13.912532510931868\n",
            "Epoch 35000, Loss_Tot: 7.150735303199441, Const: (-2.065990723387756e-06, -6.884326477774039e-07, 97.58688263538289, 9.097725491593982, 1.2925876697225918, 0.0002358799311322743), W00: -1.5867963698456156\n",
            "\n",
            "Minimum Cons: 0.000000227459472251743053 at epoch 34430\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5660655499\n",
            "W10 diff: 0.0001509221\n",
            "W01 diff: -0.0004785893\n",
            "Comp3: 105.3658896359447823\n",
            "Comp4: 9.4828229705747145\n",
            "Comp5: 1.3016628045367016\n",
            "Comp6: 0.0004532673917058\n",
            "Coeffs: tensor([9.933127990691e-01, 6.177208519799e-02, 6.072095900252e-03,\n",
            "        7.222129832833e-04, 9.889549574348e-05, 1.110794078753e-05,\n",
            "        1.247595151624e-06, 1.401237979422e-07, 1.573801330737e-08,\n",
            "        1.795235156110e-09, 6.364997732152e-02, 8.758267835775e-03,\n",
            "        1.311245255868e-03, 2.224817219804e-04, 3.899733082303e-05,\n",
            "        4.545904479977e-06, 5.305157808384e-07, 6.238678332671e-08,\n",
            "        7.336464004576e-09, 8.627420731292e-10, 1.022562826380e-02,\n",
            "        1.431579382783e-03, 2.627574871355e-04, 6.215849526716e-05,\n",
            "        1.392106094117e-05, 2.096750247617e-06, 2.549517515182e-07,\n",
            "        2.998142245986e-08, 3.525708680732e-09, 3.422610303205e-03,\n",
            "        3.017567879497e-04, 5.691900008699e-05, 1.631832849233e-05,\n",
            "        4.002757167676e-06, 7.804938062976e-07, 1.056261379956e-07,\n",
            "        1.412764425904e-08, 1.760492731603e-09, 9.717544417739e-04,\n",
            "        9.307591429437e-05, 1.669342376478e-05, 4.416181520178e-06,\n",
            "        1.189952022019e-06, 2.935363287705e-07, 4.404531317201e-08,\n",
            "        5.890420815990e-09, 3.125059157673e-04, 2.479798115499e-05,\n",
            "        5.200044637381e-06, 1.137685611918e-06, 3.548995054316e-07,\n",
            "        9.500983803569e-08, 1.872923050757e-08, 2.662213102764e-09,\n",
            "        1.004499106847e-04, 9.035305048820e-06, 1.462043286780e-06,\n",
            "        3.047631347327e-07, 6.893213370637e-08, 2.216360478236e-08,\n",
            "        6.049372868719e-09, 2.343846230483e-05, 3.305180355653e-06,\n",
            "        4.295646453977e-07, 8.568697814922e-08, 1.788282332654e-08,\n",
            "        4.304491584700e-09, 1.408804360175e-09, 1.352091482638e-05,\n",
            "        1.300952006163e-06, 1.517337531455e-07, 2.409168567141e-08,\n",
            "        5.021912507028e-09, 1.087134896893e-09, 6.781953786089e-05,\n",
            "        3.825658788594e-07, 5.505574387197e-08, 7.427718812222e-09,\n",
            "        1.411957063887e-09, 3.051734294036e-10, 1.201081816196e-07,\n",
            "        1.164609141006e-07, 2.064277646663e-08, 2.488762846813e-09,\n",
            "        3.952115748001e-10, 4.305217282959e-10, 3.898764705994e-08,\n",
            "        6.463703972669e-09, 8.721091483404e-10, 1.270126893682e-10,\n",
            "        1.890158671260e-12, 1.525623485124e-08, 2.080631548645e-09,\n",
            "        3.329064203182e-10, 1.490417499263e-14, 2.313939478266e-08,\n",
            "        7.058852195656e-10, 1.083546236239e-10, 1.395863233994e-16,\n",
            "        6.351661981165e-08, 2.433261400451e-10, 1.307307629560e-18,\n",
            "        3.413810130173e-09, 9.591065151914e-11, 1.318934610430e-20,\n",
            "        5.293269065521e-12, 1.365431455262e-22, 3.294528331979e-14,\n",
            "        1.421065800550e-24, 1.479851356232e-26], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 5.361963810569021, Const: (-1.659595027092564e-05, 8.077490681657196e-06, 104.4220009966787, 9.675016230540269, 1.369462997144233, 0.00019395698040068003), W00: -1.5996921145329361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch+1) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oPd7qpdTINA",
        "outputId": "59302b71-da15-43c6-da95-28272e60383a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 5.362006663716068, Const: (1.7865321994881e-05, -1.3703969204970079e-05, 104.392373163185, 9.672500807431673, 1.3691394915624928, 0.00019395546904455753), W00: -1.5996272979846857\n",
            "Epoch 5000, Loss_Tot: 5.130974519199414, Const: (-1.5193284335079937e-06, -9.000531679959067e-07, 99.93484506111987, 9.233312159239668, 1.3047532848596568, 0.00018833467934239152), W00: -1.5839762564446773\n",
            "Epoch 10000, Loss_Tot: 1.8555514609005985, Const: (-6.537565904829457e-05, 5.714649967170082e-05, 9.979652921208997, 3.4990150064160432, 0.8014725620884626, 5.7701706068348186e-05), W00: -1.515063073360067\n",
            "Epoch 15000, Loss_Tot: 1.7479094541348086, Const: (-4.3033379548695905e-07, -1.2569863292455352e-06, 9.549574046469415, 3.269502316028815, 0.747062703876203, 5.025397484934888e-05), W00: -1.4953614901171022\n",
            "\n",
            "Minimum Cons: 0.000000000038164272748429 at epoch 8835\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5285157828\n",
            "W10 diff: -0.0000003051\n",
            "W01 diff: -0.0000005944\n",
            "Comp3: 8.7382248255990316\n",
            "Comp4: 3.4384873150613955\n",
            "Comp5: 0.8010579722528040\n",
            "Comp6: 0.0000571165850591\n",
            "Coeffs: tensor([9.920394779511e-01, 6.591923328304e-02, 6.641545794520e-03,\n",
            "        8.022970113547e-04, 1.602230985084e-04, 3.346420576408e-05,\n",
            "        6.134972586206e-06, 9.441727541514e-07, 1.453079405872e-07,\n",
            "        2.236284659671e-08, 6.048759876310e-02, 9.314132159209e-03,\n",
            "        1.487637284996e-03, 2.553882680536e-04, 5.213885759465e-05,\n",
            "        1.136509577053e-05, 2.498860702426e-06, 4.333688264419e-07,\n",
            "        6.932813231611e-08, 1.109075896159e-08, 7.675357445272e-03,\n",
            "        1.317554932539e-03, 2.868393885606e-04, 7.633773718241e-05,\n",
            "        1.846272867678e-05, 4.653830689265e-06, 1.023242075259e-06,\n",
            "        2.160517694574e-07, 3.528802951352e-08, 2.120476773608e-03,\n",
            "        2.510282913743e-04, 5.919574309290e-05, 2.046107770438e-05,\n",
            "        6.068165638408e-06, 1.475836187523e-06, 3.851328334030e-07,\n",
            "        1.008421973295e-07, 1.962376356439e-08, 8.221802321485e-04,\n",
            "        6.979346917914e-05, 1.689415783455e-05, 5.756455407695e-06,\n",
            "        2.044334101758e-06, 6.478244378634e-07, 1.534410105943e-07,\n",
            "        3.686438383378e-08, 3.472755091988e-04, 2.208293364114e-05,\n",
            "        6.225692333997e-06, 1.617521089265e-06, 5.871247461893e-07,\n",
            "        2.184924205604e-07, 6.950537843727e-08, 1.646275214761e-08,\n",
            "        9.419359426355e-05, 1.323391810730e-05, 2.319763389778e-06,\n",
            "        5.841370396222e-07, 1.621382336693e-07, 6.533313467466e-08,\n",
            "        2.395015429366e-08, 1.545438797031e-05, 5.734158621936e-06,\n",
            "        9.815819710295e-07, 2.206869348937e-07, 5.847383316863e-08,\n",
            "        1.914474087765e-08, 9.133607308662e-09, 5.420540553446e-06,\n",
            "        2.472459033450e-06, 3.886358575954e-07, 8.460591670327e-08,\n",
            "        2.750281048651e-08, 8.341989057746e-09, 3.515783075479e-05,\n",
            "        7.859146067577e-07, 1.632611984221e-07, 3.301727034899e-08,\n",
            "        9.789265649463e-09, 3.504151403218e-09, 2.780897351182e-07,\n",
            "        2.821500273993e-07, 7.870391539282e-08, 1.356064680981e-08,\n",
            "        3.316231367150e-09, 7.031978287348e-10, 1.651986455341e-07,\n",
            "        3.481910168074e-08, 6.001090548466e-09, 1.371187272041e-09,\n",
            "        2.648084711717e-12, 1.548086382721e-07, 1.847157527688e-08,\n",
            "        3.322662437668e-09, 1.252890638881e-14, 4.101275966255e-07,\n",
            "        1.307384460531e-08, 2.336616864810e-09, 7.831249907662e-17,\n",
            "        1.877143742488e-06, 1.013094502628e-08, 4.904954983733e-19,\n",
            "        1.430307096804e-05, 7.422240767806e-09, 3.141149770482e-21,\n",
            "        5.468037939424e-09, 2.186695919925e-23, 1.989709730586e-12,\n",
            "        1.522257579423e-25, 1.059712105828e-27], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 20000, Loss_Tot: 1.7306639273919433, Const: (-1.3774040419356481e-07, -1.3945947552329585e-06, 9.261137542973186, 3.171628174312996, 0.7246315934685332, 4.946960901280889e-05), W00: -1.4859377419369748\n",
            "Epoch 25000, Loss_Tot: 1.7187990703729128, Const: (-1.9064847214256275e-06, -3.1093590968644946e-07, 8.990652686728705, 3.088261857508069, 0.705717070331507, 4.892814746332687e-05), W00: -1.479398977588474\n",
            "Epoch 30000, Loss_Tot: 1.7139740268792878, Const: (1.2273193054301856e-05, -9.409691477557658e-06, 8.860624578589654, 3.0390102383155346, 0.6942361944915304, 4.869776601221759e-05), W00: -1.4765876118597672\n",
            "Epoch 35000, Loss_Tot: 5.505617720707074, Const: (-0.0015673208247950488, 0.001005874367258075, 10.05081115458093, 3.247853602572505, 0.7335628813207922, 7.469723730593066e-05), W00: -1.4793721840501572\n",
            "\n",
            "Minimum Cons: 0.000000000025896078806017 at epoch 36784\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4732005396\n",
            "W10 diff: -0.0000001634\n",
            "W01 diff: -0.0000005075\n",
            "Comp3: 8.7393321798708143\n",
            "Comp4: 3.0024023501572277\n",
            "Comp5: 0.6858331954631373\n",
            "Comp6: 0.0000485735900557\n",
            "Coeffs: tensor([9.914350500001e-01, 6.497645360658e-02, 5.998436551692e-03,\n",
            "        6.965716475204e-04, 1.330655616873e-04, 3.020542426004e-05,\n",
            "        5.418810771411e-06, 9.343951451093e-07, 1.606364477880e-07,\n",
            "        2.869945880430e-08, 5.906069693213e-02, 9.090113453221e-03,\n",
            "        1.351727944299e-03, 2.114067516712e-04, 4.005678738168e-05,\n",
            "        8.546666716209e-06, 1.863752014539e-06, 3.486964552804e-07,\n",
            "        6.545671401930e-08, 1.214037880080e-08, 6.703011069224e-03,\n",
            "        1.170236127176e-03, 2.589176451965e-04, 6.261237233326e-05,\n",
            "        1.377094726977e-05, 3.145601745333e-06, 7.339355790077e-07,\n",
            "        1.481372919758e-07, 2.864437702896e-08, 1.679447206304e-03,\n",
            "        2.011023806415e-04, 5.139509730060e-05, 1.799629691210e-05,\n",
            "        4.398922082951e-06, 1.019306942008e-06, 2.516188300508e-07,\n",
            "        6.225952101363e-08, 1.284152597069e-08, 6.038296142629e-04,\n",
            "        5.128070140206e-05, 1.357289890331e-05, 4.794719804492e-06,\n",
            "        1.715056274899e-06, 3.965657345677e-07, 8.722477293746e-08,\n",
            "        2.146747764393e-08, 3.020040439795e-04, 1.529444158464e-05,\n",
            "        4.610497670216e-06, 1.260201681572e-06, 4.646091629975e-07,\n",
            "        1.689016986406e-07, 3.765996200200e-08, 8.356990231880e-09,\n",
            "        8.103190770296e-05, 8.967080894951e-06, 1.588436395440e-06,\n",
            "        4.240810684156e-07, 1.200976550538e-07, 4.814992397142e-08,\n",
            "        1.625997603957e-08, 1.300071198832e-05, 4.946298046536e-06,\n",
            "        6.491593998859e-07, 1.478924721791e-07, 4.002945942775e-08,\n",
            "        1.218139180341e-08, 5.327423807908e-09, 4.172124544314e-06,\n",
            "        2.339849561792e-06, 3.483706595530e-07, 5.229885041424e-08,\n",
            "        1.420865747852e-08, 3.979784022997e-09, 2.814168849541e-05,\n",
            "        7.659898184546e-07, 1.397748109152e-07, 2.432532175591e-08,\n",
            "        5.081746715551e-09, 1.431466192884e-09, 4.493017289282e-08,\n",
            "        3.200606609396e-07, 7.385127768752e-08, 1.346583551794e-08,\n",
            "        2.283585336675e-09, 1.791579492821e-10, 2.361754451604e-07,\n",
            "        4.894015826657e-08, 7.385239377003e-09, 1.484341762207e-09,\n",
            "        1.458948756245e-12, 2.025070582340e-07, 3.178764020528e-08,\n",
            "        5.469694492049e-09, 1.348551171346e-14, 3.337542544945e-07,\n",
            "        2.762013073664e-08, 5.830674851295e-09, 1.257362175184e-16,\n",
            "        1.668580621855e-06, 2.463456271738e-08, 6.960693332169e-19,\n",
            "        1.190384591352e-05, 2.210942138163e-08, 4.293772515314e-21,\n",
            "        9.716945457735e-09, 2.732825997469e-23, 1.847355431762e-12,\n",
            "        1.868583837584e-25, 1.279877343642e-27], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 40000, Loss_Tot: 1.7318954333841483, Const: (-1.221259315009604e-06, -3.1880825341890073e-06, 8.573432815460757, 3.06692169996464, 0.704158450541961, 5.003260343373051e-05), W00: -1.4815576374038948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lFf9GZcVRmB",
        "outputId": "d125ccbc-edb6-430b-b267-534d99e54e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.7318231714612402, Const: (-3.5910245963499676e-06, -1.1946161795339805e-06, 8.578402752734767, 3.0675049444598796, 0.7042482868444416, 5.002646306903402e-05), W00: -1.4815441481760296\n",
            "\n",
            "Minimum Cons: 0.000000000189325977097350 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4815441482\n",
            "W10 diff: -0.0000035910\n",
            "W01 diff: -0.0000011946\n",
            "Comp3: 8.5784027527347675\n",
            "Comp4: 3.0675049444598796\n",
            "Comp5: 0.7042482868444416\n",
            "Comp6: 0.0000500264630690\n",
            "Coeffs: tensor([9.918321507130e-01, 6.220454278521e-02, 5.568322436444e-03,\n",
            "        6.328984339690e-04, 1.165227146282e-04, 2.349581785052e-05,\n",
            "        4.018726353680e-06, 6.600286889364e-07, 1.103256930660e-07,\n",
            "        1.892833417128e-08, 5.963842333198e-02, 8.592698456581e-03,\n",
            "        1.236379131991e-03, 1.896790949828e-04, 3.480831696014e-05,\n",
            "        7.212375580935e-06, 1.404577089772e-06, 2.551631754386e-07,\n",
            "        4.539497289747e-08, 8.076021875555e-09, 6.909284104650e-03,\n",
            "        1.202348686742e-03, 2.393669725925e-04, 5.559041309609e-05,\n",
            "        1.190061273720e-05, 2.622323565978e-06, 5.645774820453e-07,\n",
            "        1.101242702498e-07, 2.052919707308e-08, 1.736522013503e-03,\n",
            "        2.159803862111e-04, 4.754474065555e-05, 1.655289753802e-05,\n",
            "        3.788110307672e-06, 8.569457068853e-07, 2.057514318502e-07,\n",
            "        4.729245387099e-08, 9.275387259781e-09, 6.295954460430e-04,\n",
            "        5.526729154170e-05, 1.248494757878e-05, 4.346682267865e-06,\n",
            "        1.540074376864e-06, 3.237440687345e-07, 6.968396654015e-08,\n",
            "        1.703643641878e-08, 3.205556115234e-04, 1.688833436091e-05,\n",
            "        4.254470748118e-06, 1.126096795708e-06, 4.112025778640e-07,\n",
            "        1.354878933780e-07, 2.932413422412e-08, 6.636028058674e-09,\n",
            "        8.202763499706e-05, 9.359743138254e-06, 1.464416140625e-06,\n",
            "        3.759641647655e-07, 1.047160490479e-07, 4.155933920860e-08,\n",
            "        1.245323049870e-08, 1.209819098399e-05, 4.742560055296e-06,\n",
            "        5.854172420444e-07, 1.294091142213e-07, 3.468385385536e-08,\n",
            "        1.035604770770e-08, 4.483438450507e-09, 4.133475387932e-06,\n",
            "        2.102439444940e-06, 3.074318362984e-07, 4.499992481751e-08,\n",
            "        1.214898260136e-08, 3.366581461163e-09, 3.005850549170e-05,\n",
            "        6.986841939970e-07, 1.292854753228e-07, 2.254955443090e-08,\n",
            "        4.340217294782e-09, 1.219300738070e-09, 2.297948741637e-08,\n",
            "        2.999980095835e-07, 7.094264044488e-08, 1.263031238268e-08,\n",
            "        2.141383441540e-09, 9.466451064877e-11, 2.175398919530e-07,\n",
            "        4.610785400537e-08, 6.939877906111e-09, 1.388479836092e-09,\n",
            "        7.404390296014e-13, 1.854499457027e-07, 3.044534406433e-08,\n",
            "        5.251729037163e-09, 6.580790652890e-15, 3.148328688279e-07,\n",
            "        2.578515346377e-08, 5.580024320843e-09, 6.045160012842e-17,\n",
            "        1.761706260213e-06, 2.254110597783e-08, 3.492426495782e-19,\n",
            "        1.228589060941e-05, 1.864047838284e-08, 2.270712726261e-21,\n",
            "        2.483739912078e-09, 1.512647741166e-23, 4.597728212832e-13,\n",
            "        9.934660414586e-26, 6.524815716650e-28], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 1.6985535760689467, Const: (-5.0689936283987436e-06, 8.629043206642706e-07, 8.636226906613102, 2.9667619898859763, 0.6772443693470953, 4.791432525207447e-05), W00: -1.4689488803325172\n",
            "Epoch 10000, Loss_Tot: 2.154490447945979, Const: (0.00016072958762136658, -0.0001038131834460998, 4.606739927080528, 3.1301336665819433, 0.8338432533713086, 7.835378269061157e-05), W00: -1.5039477443590725\n",
            "Epoch 15000, Loss_Tot: 1.709446030494465, Const: (-3.839414670636643e-07, -1.320735701026976e-06, 8.458872251708343, 2.9570140360941943, 0.677558809312927, 4.887182732452968e-05), W00: -1.4705985881367585\n",
            "Epoch 20000, Loss_Tot: 1.7004451913707457, Const: (-4.438003451401329e-07, -1.2496343884116357e-06, 8.313753622902457, 2.9043626043748962, 0.6651481076549817, 4.846914206554653e-05), W00: -1.4655176595688815\n",
            "\n",
            "Minimum Cons: 0.000000000025945591655036 at epoch 3999\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4676704354\n",
            "W10 diff: -0.0000002525\n",
            "W01 diff: -0.0000004819\n",
            "Comp3: 8.7397393057069621\n",
            "Comp4: 2.9641663038983701\n",
            "Comp5: 0.6755832448663848\n",
            "Comp6: 0.0000476916060596\n",
            "Coeffs: tensor([9.916529008730e-01, 6.166394375973e-02, 5.467064174539e-03,\n",
            "        6.163675146528e-04, 1.086576704390e-04, 1.899776081949e-05,\n",
            "        3.137314774581e-06, 4.997194021281e-07, 8.158157040943e-08,\n",
            "        1.354092600049e-08, 5.922317710647e-02, 8.541524972251e-03,\n",
            "        1.212201033326e-03, 1.838355917966e-04, 3.335936053866e-05,\n",
            "        6.266953173524e-06, 1.102572600365e-06, 1.947646488194e-07,\n",
            "        3.387727706755e-08, 5.823124104923e-09, 6.667363208839e-03,\n",
            "        1.169297487421e-03, 2.305078220684e-04, 5.403955246839e-05,\n",
            "        1.142643179799e-05, 2.474268035676e-06, 4.526243873219e-07,\n",
            "        8.519698713058e-08, 1.548140416711e-08, 1.642339404376e-03,\n",
            "        2.021372787689e-04, 4.558349700631e-05, 1.579473725031e-05,\n",
            "        3.647348441101e-06, 8.157821829637e-07, 1.936222575613e-07,\n",
            "        3.724604577302e-08, 7.048354073370e-09, 5.854407583581e-04,\n",
            "        5.084175309433e-05, 1.185528629109e-05, 4.117836230104e-06,\n",
            "        1.419393915886e-06, 3.041709831780e-07, 6.531716919741e-08,\n",
            "        1.443218520318e-08, 2.958299692121e-04, 1.533809168311e-05,\n",
            "        4.003375797771e-06, 1.056282388033e-06, 3.824021173670e-07,\n",
            "        1.248081564714e-07, 2.695330499441e-08, 5.794888882208e-09,\n",
            "        8.213002735176e-05, 8.788920520334e-06, 1.366048065582e-06,\n",
            "        3.494208454194e-07, 9.644209702464e-08, 3.795106478277e-08,\n",
            "        1.137759362850e-08, 1.216164518909e-05, 4.593842079005e-06,\n",
            "        5.404176220827e-07, 1.192306492263e-07, 3.159184779982e-08,\n",
            "        9.359471809259e-09, 4.005525462502e-09, 4.070549024254e-06,\n",
            "        2.026942306239e-06, 2.804531816457e-07, 4.076002001498e-08,\n",
            "        1.096022363729e-08, 3.006356019880e-09, 2.762911378244e-05,\n",
            "        7.129726514200e-07, 1.244697214517e-07, 2.032597071251e-08,\n",
            "        3.831878933287e-09, 1.055956621492e-09, 3.770213258813e-08,\n",
            "        3.097658803594e-07, 6.808125223556e-08, 1.193508498997e-08,\n",
            "        1.801308011007e-09, 1.450998303327e-10, 2.281261062777e-07,\n",
            "        4.626280345204e-08, 6.504461883670e-09, 1.287590056568e-09,\n",
            "        1.142478324546e-12, 1.958384399239e-07, 3.089967961133e-08,\n",
            "        5.018553490203e-09, 1.018648777856e-14, 3.433265859451e-07,\n",
            "        2.696667622603e-08, 5.323105049460e-09, 9.450492236653e-17,\n",
            "        1.703602433106e-06, 2.373141315699e-08, 5.196209124754e-19,\n",
            "        1.166510149643e-05, 2.046373473386e-08, 3.378704978156e-21,\n",
            "        1.208760456781e-08, 2.087547581957e-23, 2.029528187655e-12,\n",
            "        1.379938071368e-25, 9.121847555805e-28], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.6927100277615645, Const: (1.3721096606023053e-05, -1.0277452456852743e-05, 8.257718374715134, 2.8626565439907137, 0.6547936159932768, 4.810559979083714e-05), W00: -1.461001260116871\n",
            "Epoch 30000, Loss_Tot: 1.6836883550280513, Const: (-7.763717246866975e-07, -7.159694783709369e-07, 8.386823652061128, 2.841758532540364, 0.6468488210538516, 4.750424908371774e-05), W00: -1.4580218715619127\n",
            "Epoch 35000, Loss_Tot: 2.026979104382588, Const: (7.370910388826424e-05, -4.741805095265228e-05, 10.609854682341698, 4.684050588764412, 1.2000147102583956, 7.150447544749551e-05), W00: -1.50800859992828\n",
            "Epoch 40000, Loss_Tot: 1.9923532723154898, Const: (-7.878000006966701e-05, 4.886166011042192e-05, 10.203566631429855, 4.527792166259088, 1.1445611001510454, 6.824987418109064e-05), W00: -1.5179549895022963\n",
            "\n",
            "Minimum Cons: 0.000000000042033244004243 at epoch 29965\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4583492480\n",
            "W10 diff: -0.0000002964\n",
            "W01 diff: -0.0000008276\n",
            "Comp3: 8.1947978621754771\n",
            "Comp4: 2.8292116410578863\n",
            "Comp5: 0.6465138054700729\n",
            "Comp6: 0.0000478165324839\n",
            "Coeffs: tensor([9.911964120403e-01, 6.469744901170e-02, 5.770569017376e-03,\n",
            "        6.660824490340e-04, 9.829697871531e-05, 1.290979167564e-05,\n",
            "        1.710872110554e-06, 2.211861119966e-07, 2.872444058533e-08,\n",
            "        3.644605429690e-09, 5.851256170402e-02, 9.096352272236e-03,\n",
            "        1.298386688318e-03, 2.001642970062e-04, 3.719371705225e-05,\n",
            "        5.087537981323e-06, 7.174109738476e-07, 1.012457746151e-07,\n",
            "        1.350110179198e-08, 1.819262602303e-09, 6.367466039849e-03,\n",
            "        1.137091941476e-03, 2.555165909954e-04, 6.049163814439e-05,\n",
            "        1.320408579548e-05, 2.514121613800e-06, 3.628937295905e-07,\n",
            "        5.274383668720e-08, 7.675508283388e-09, 1.554983930438e-03,\n",
            "        1.906255093863e-04, 5.262294165488e-05, 1.962950455177e-05,\n",
            "        4.442463727075e-06, 1.039838222960e-06, 1.800816904341e-07,\n",
            "        2.887598404712e-08, 4.214726610816e-09, 5.599759802316e-04,\n",
            "        4.757508467230e-05, 1.359370819155e-05, 5.126750560489e-06,\n",
            "        1.723936612065e-06, 3.869313975377e-07, 7.889612595658e-08,\n",
            "        1.312966027787e-08, 2.977455080703e-04, 1.502037614166e-05,\n",
            "        4.655541900746e-06, 1.304454379290e-06, 5.209495259225e-07,\n",
            "        1.617410678094e-07, 3.668817122877e-08, 6.142554688774e-09,\n",
            "        7.437733317218e-05, 9.084406137239e-06, 1.597611935151e-06,\n",
            "        4.377865234752e-07, 1.305583646320e-07, 5.800435677912e-08,\n",
            "        1.562618384741e-08, 1.185727130428e-05, 5.230618856934e-06,\n",
            "        6.329832918722e-07, 1.511903961650e-07, 4.341419161440e-08,\n",
            "        1.410199878674e-08, 5.857114504329e-09, 4.078014384952e-06,\n",
            "        2.170657425136e-06, 3.388553681396e-07, 5.310947970604e-08,\n",
            "        1.530749729843e-08, 4.581449116825e-09, 2.674886395619e-05,\n",
            "        7.016748362095e-07, 1.352020812585e-07, 2.453159377054e-08,\n",
            "        5.452098923728e-09, 1.651147472227e-09, 2.328478553963e-08,\n",
            "        3.321838694004e-07, 7.456034301805e-08, 9.593328583450e-09,\n",
            "        2.014212060808e-09, 8.101132496949e-11, 2.166775239579e-07,\n",
            "        4.443208937746e-08, 5.270373015282e-09, 8.338928104950e-10,\n",
            "        5.338275344636e-13, 1.847048710340e-07, 2.915627367859e-08,\n",
            "        3.835079270912e-09, 4.022949368521e-15, 3.272003935696e-07,\n",
            "        2.430456486212e-08, 3.590126258903e-09, 3.154837938021e-17,\n",
            "        1.626252916759e-06, 2.424986002668e-08, 1.805274532971e-19,\n",
            "        1.121792147021e-05, 2.412825294534e-08, 9.632010614197e-22,\n",
            "        5.674155529144e-09, 5.496710875365e-24, 7.794457523235e-13,\n",
            "        3.138645244646e-26, 1.792179758970e-28], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpJ00iAQXWI0",
        "outputId": "40934357-d4bf-4949-9828-e3f257c68bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.9854058743447487, Const: (-4.157440558216763e-05, 2.5137017697351993e-05, 10.098027117483936, 4.505412351524974, 1.1399776923252793, 6.821352602829763e-05), W00: -1.5177370601651972\n",
            "\n",
            "Minimum Cons: 0.000000003314012799210211 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.5177370602\n",
            "W10 diff: -0.0000415744\n",
            "W01 diff: 0.0000251370\n",
            "Comp3: 10.0980271174839356\n",
            "Comp4: 4.5054123515249742\n",
            "Comp5: 1.1399776923252793\n",
            "Comp6: 0.0000682135260283\n",
            "Coeffs: tensor([9.923845245287e-01, 5.866007483048e-02, 5.052034857374e-03,\n",
            "        5.614320401154e-04, 7.991808625847e-05, 9.982154815417e-06,\n",
            "        1.234896649416e-06, 1.528248371804e-07, 1.874479021877e-08,\n",
            "        2.239210755565e-09, 6.053164980146e-02, 8.369631809228e-03,\n",
            "        1.123639186016e-03, 1.678729395622e-04, 2.970906846680e-05,\n",
            "        3.847429109400e-06, 5.204492438867e-07, 6.837731892620e-08,\n",
            "        8.681609493676e-09, 1.107486458501e-09, 7.494581027023e-03,\n",
            "        1.170570946296e-03, 2.334195186890e-04, 5.115306816849e-05,\n",
            "        1.060976200380e-05, 1.872603564974e-06, 2.570421413761e-07,\n",
            "        3.550641735194e-08, 4.838394550712e-09, 2.029839348748e-03,\n",
            "        2.202615651029e-04, 5.080310578587e-05, 1.840877339162e-05,\n",
            "        3.757348868481e-06, 8.012319510275e-07, 1.239850398969e-07,\n",
            "        1.882231113476e-08, 2.606642006598e-09, 7.255063431300e-04,\n",
            "        5.813081556266e-05, 1.331857165739e-05, 4.889197665688e-06,\n",
            "        1.558376536070e-06, 3.299604323765e-07, 5.473112739988e-08,\n",
            "        8.325061673781e-09, 3.583296998019e-04, 1.907801284850e-05,\n",
            "        4.663228156135e-06, 1.258952078718e-06, 5.100008602205e-07,\n",
            "        1.400287891117e-07, 2.964876132449e-08, 4.089454453620e-09,\n",
            "        8.672923600304e-05, 1.065975603244e-05, 1.632730313537e-06,\n",
            "        4.402545791279e-07, 1.292883382830e-07, 5.654555469866e-08,\n",
            "        1.258235649102e-08, 1.335246124258e-05, 5.560748781144e-06,\n",
            "        6.611858149043e-07, 1.573257996944e-07, 4.510896694551e-08,\n",
            "        1.426467882781e-08, 5.272879464345e-09, 6.789416874234e-06,\n",
            "        1.920121733302e-06, 3.649783804729e-07, 5.622066452624e-08,\n",
            "        1.637874850686e-08, 4.818401426897e-09, 3.174339373483e-05,\n",
            "        6.437722836787e-07, 1.512804733978e-07, 2.689655494070e-08,\n",
            "        5.947008285973e-09, 1.749527632921e-09, 1.419012211458e-06,\n",
            "        2.931674332542e-07, 7.789355355842e-08, 1.123492540359e-08,\n",
            "        2.205210176958e-09, 1.202526810884e-09, 1.553028631006e-07,\n",
            "        3.806794912185e-08, 6.385567351004e-09, 8.819039313470e-10,\n",
            "        4.625338866634e-12, 9.247097620871e-08, 2.149322686290e-08,\n",
            "        4.528983916435e-09, 2.452792181010e-14, 1.502111620919e-07,\n",
            "        1.427287342921e-08, 2.617621445655e-09, 2.065727619376e-16,\n",
            "        8.394549607587e-07, 1.000453588764e-08, 1.799790820557e-18,\n",
            "        5.067142576669e-06, 6.720745544581e-09, 9.294815451169e-21,\n",
            "        9.206869791920e-06, 5.053988489067e-23, 2.136353988854e-09,\n",
            "        3.329915280272e-25, 2.328144229292e-27], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 1.7395237626905722, Const: (-3.1824493704757373e-06, 6.149797606713747e-07, 8.725142515554195, 3.115258844248201, 0.7161343899811419, 5.0357252496297375e-05), W00: -1.4859279686088858\n",
            "Epoch 10000, Loss_Tot: 1.984190270189753, Const: (-5.377944432005677e-07, -1.5395367576331864e-06, 11.768757867683096, 4.723010716252313, 1.1870192860513058, 6.95712033007636e-05), W00: -1.5001723779218439\n",
            "Epoch 15000, Loss_Tot: 1.8301324592804495, Const: (0.00010317463523201731, -6.708105335206938e-05, 9.310806092451937, 3.61430490064092, 0.878909018391393, 5.831726066001043e-05), W00: -1.4748972971176064\n",
            "Epoch 20000, Loss_Tot: 1.7229101346032059, Const: (-3.963203848167396e-05, 2.242080665970647e-05, 8.346324148995771, 2.9411488078858046, 0.6779276059952177, 5.089345203565356e-05), W00: -1.461822397547184\n",
            "\n",
            "Minimum Cons: 0.000000000006542271503109 at epoch 16462\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4639868338\n",
            "W10 diff: 0.0000000577\n",
            "W01 diff: 0.0000001174\n",
            "Comp3: 8.6602732503729438\n",
            "Comp4: 2.9667446028252753\n",
            "Comp5: 0.6776765954424474\n",
            "Comp6: 0.0000499969742437\n",
            "Coeffs: tensor([9.914054019472e-01, 6.167101251544e-02, 5.396848564298e-03,\n",
            "        6.036139629834e-04, 9.880984803917e-05, 1.333601724791e-05,\n",
            "        1.776965063113e-06, 2.380200173911e-07, 3.207771310767e-08,\n",
            "        4.323080041003e-09, 5.871350574985e-02, 8.772522195727e-03,\n",
            "        1.224817571303e-03, 1.835272362390e-04, 3.409837017348e-05,\n",
            "        5.349461662722e-06, 7.802553958296e-07, 1.098794757573e-07,\n",
            "        1.533910420094e-08, 2.201837857793e-09, 6.453268895826e-03,\n",
            "        1.161562533090e-03, 2.397604099615e-04, 5.682027230576e-05,\n",
            "        1.261671556441e-05, 2.724486273364e-06, 4.087067725109e-07,\n",
            "        6.159167524019e-08, 9.239658946103e-09, 1.617828687504e-03,\n",
            "        2.003568442510e-04, 4.991641037020e-05, 1.987905042073e-05,\n",
            "        4.700608852900e-06, 1.071605192987e-06, 2.059526133241e-07,\n",
            "        3.385718198590e-08, 5.134947376363e-09, 5.899576340843e-04,\n",
            "        5.150015854083e-05, 1.269688385735e-05, 5.156207846644e-06,\n",
            "        1.955787278743e-06, 4.606225340185e-07, 9.794938367714e-08,\n",
            "        1.554407734984e-08, 3.122694697938e-04, 1.606027447258e-05,\n",
            "        4.357026712510e-06, 1.294081305466e-06, 5.941159369925e-07,\n",
            "        2.008608579597e-07, 4.612100560057e-08, 8.452284036070e-09,\n",
            "        6.879931649440e-05, 8.904156692746e-06, 1.474761100049e-06,\n",
            "        4.453617538396e-07, 1.468663122540e-07, 7.332271382472e-08,\n",
            "        2.110598390819e-08, 1.472142417198e-05, 4.644708871092e-06,\n",
            "        5.683079015436e-07, 1.562362826869e-07, 5.000923177821e-08,\n",
            "        1.765979253078e-08, 8.711632260715e-09, 4.879084801519e-06,\n",
            "        1.437879730503e-06, 3.152907793682e-07, 5.458467029287e-08,\n",
            "        1.784446471684e-08, 5.847867770418e-09, 2.732646236967e-05,\n",
            "        5.306533930435e-07, 1.547527033716e-07, 2.499087040436e-08,\n",
            "        6.367372817299e-09, 2.210781586099e-09, 2.648650155370e-08,\n",
            "        2.735039346870e-07, 9.550496188779e-08, 1.529238735990e-08,\n",
            "        2.360697712091e-09, 6.793803144876e-11, 2.050684848860e-07,\n",
            "        5.042725650750e-08, 1.001885821295e-08, 1.225039269733e-09,\n",
            "        2.385832611430e-13, 2.043266568526e-07, 3.237731707576e-08,\n",
            "        7.987665443720e-09, 1.171032557752e-15, 2.851053502230e-07,\n",
            "        2.396532358556e-08, 5.267115797497e-09, 7.492638388915e-18,\n",
            "        1.810346739542e-06, 2.347614440468e-08, 5.402837523918e-20,\n",
            "        1.157603172067e-05, 2.339122027008e-08, 2.783579234255e-22,\n",
            "        5.524000071592e-08, 1.467958160878e-24, 6.113366991184e-12,\n",
            "        8.204117760027e-27, 4.586995418938e-29], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.695933711278159, Const: (1.013312003328437e-05, -8.985313252773963e-06, 8.064141511766243, 2.801502587085337, 0.6401887174535075, 4.8841830387360506e-05), W00: -1.4571978557435303\n",
            "Epoch 30000, Loss_Tot: 18.948028243051823, Const: (-0.003292695877165519, 0.0020502207216064328, 11.141503734405577, 3.2964794969945714, 0.7317596265945132, 0.00015620100683462644), W00: -1.462901642629513\n",
            "Epoch 35000, Loss_Tot: 1.9355430118908417, Const: (-3.100775525233246e-07, -1.0379018846595756e-06, 9.105854829260592, 3.923164721055597, 0.9992823626734728, 6.873445295357913e-05), W00: -1.4630993362196527\n",
            "Epoch 40000, Loss_Tot: 1.8500439855196555, Const: (-0.0001135473735103254, 7.066309489700551e-05, 9.587361921983922, 3.7461077615889398, 0.9254369593632618, 6.10108908841673e-05), W00: -1.459924825860162\n",
            "\n",
            "Minimum Cons: 0.000000000020866542663806 at epoch 33280\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4505129586\n",
            "W10 diff: -0.0000004452\n",
            "W01 diff: 0.0000000191\n",
            "Comp3: 7.4258245148731596\n",
            "Comp4: 2.6075702429890368\n",
            "Comp5: 0.5957680863950610\n",
            "Comp6: 0.0000468266206860\n",
            "Coeffs: tensor([9.913267707245e-01, 6.044005940293e-02, 5.234569896452e-03,\n",
            "        5.882397981415e-04, 8.656741807241e-05, 1.046735513507e-05,\n",
            "        1.261188424183e-06, 1.554369251014e-07, 1.934196603628e-08,\n",
            "        2.406838835850e-09, 5.841441637034e-02, 8.827951255993e-03,\n",
            "        1.196949127515e-03, 1.800132014019e-04, 3.333426622025e-05,\n",
            "        4.342992540693e-06, 5.515421199839e-07, 7.018912369768e-08,\n",
            "        9.027017192679e-09, 1.185651632811e-09, 6.251439558667e-03,\n",
            "        1.080268659001e-03, 2.394328877003e-04, 5.680557086600e-05,\n",
            "        1.215260297975e-05, 2.252774813810e-06, 3.048651141149e-07,\n",
            "        4.011161247850e-08, 5.277551271595e-09, 1.511360222925e-03,\n",
            "        1.805314475352e-04, 5.236445702446e-05, 2.116767791789e-05,\n",
            "        4.660982633196e-06, 1.005213680971e-06, 1.645113133221e-07,\n",
            "        2.416422165418e-08, 3.216560962761e-09, 5.409604015884e-04,\n",
            "        4.550276137438e-05, 1.330470227478e-05, 5.494969252366e-06,\n",
            "        2.035582340050e-06, 4.368033923121e-07, 8.017257445722e-08,\n",
            "        1.202821847956e-08, 2.925336741308e-04, 1.475125849192e-05,\n",
            "        4.536570022931e-06, 1.382185002663e-06, 6.362313558507e-07,\n",
            "        1.975245662612e-07, 4.266427741790e-08, 6.669806698805e-09,\n",
            "        7.440319713998e-05, 8.991040607414e-06, 1.546852209591e-06,\n",
            "        4.726879065366e-07, 1.587769882092e-07, 7.741005709224e-08,\n",
            "        1.982340579216e-08, 9.602510776583e-06, 5.413206143059e-06,\n",
            "        6.157230832854e-07, 1.649293894403e-07, 5.371334258129e-08,\n",
            "        1.929949501382e-08, 8.852885302224e-09, 5.439065806587e-06,\n",
            "        1.672784894674e-06, 3.423850959092e-07, 5.795091401417e-08,\n",
            "        1.905606740942e-08, 6.497434696393e-09, 2.554940846183e-05,\n",
            "        8.030839668200e-07, 1.958670685574e-07, 2.723623589679e-08,\n",
            "        6.782419980533e-09, 2.424544892235e-09, 9.112164472708e-09,\n",
            "        5.614550067020e-07, 1.383535848504e-07, 1.973065925915e-08,\n",
            "        2.631357504502e-09, 1.831577392540e-11, 4.328710045573e-07,\n",
            "        9.472252030163e-08, 1.428491698262e-08, 1.503911401462e-09,\n",
            "        5.125673326033e-14, 3.278101337865e-07, 7.302923624569e-08,\n",
            "        1.353907508220e-08, 2.189206332534e-16, 4.558765055335e-07,\n",
            "        5.630413249561e-08, 1.069285719760e-08, 1.125880982048e-18,\n",
            "        2.156299003965e-06, 4.421407410904e-08, 6.270478855232e-21,\n",
            "        1.015291202281e-05, 3.847969514865e-08, 2.499708723821e-23,\n",
            "        4.589510729521e-09, 1.110563455132e-25, 9.953534350437e-13,\n",
            "        5.263733544449e-28, 2.424544029474e-30], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GE-x6YfZtjs",
        "outputId": "be7c5166-658c-4c7e-9c50-394c5c3bbb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.8493465837068388, Const: (0.00011530217064303905, -7.660062091896513e-05, 9.333890743516745, 3.69611444869329, 0.9153508486277312, 6.089747210184725e-05), W00: -1.459334127187145\n",
            "\n",
            "Minimum Cons: 0.000000008429900859825647 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4593341272\n",
            "W10 diff: 0.0001153022\n",
            "W01 diff: -0.0000766006\n",
            "Comp3: 9.3338907435167453\n",
            "Comp4: 3.6961144486932902\n",
            "Comp5: 0.9153508486277312\n",
            "Comp6: 0.0000608974721018\n",
            "Coeffs: tensor([9.914129375297e-01, 5.876343826770e-02, 5.024982965718e-03,\n",
            "        5.626323926488e-04, 8.553573001048e-05, 1.038763589811e-05,\n",
            "        1.259199858311e-06, 1.582264209059e-07, 2.012798431731e-08,\n",
            "        2.560480897834e-09, 5.858827703854e-02, 8.441192731330e-03,\n",
            "        1.144367134300e-03, 1.726460640883e-04, 3.183083299413e-05,\n",
            "        4.402745163801e-06, 5.729440526590e-07, 7.455901454006e-08,\n",
            "        9.752738050106e-09, 1.299619747249e-09, 6.297411161765e-03,\n",
            "        1.102136968072e-03, 2.214529457719e-04, 5.492523394614e-05,\n",
            "        1.205747756568e-05, 2.368813113265e-06, 3.286097191650e-07,\n",
            "        4.408915463934e-08, 5.915385954866e-09, 1.543359491744e-03,\n",
            "        1.853084104081e-04, 4.928370349308e-05, 1.998239353491e-05,\n",
            "        4.544931282285e-06, 1.031047432984e-06, 1.814843483681e-07,\n",
            "        2.728698168630e-08, 3.691818423984e-09, 5.569059079274e-04,\n",
            "        4.774547045635e-05, 1.252593594627e-05, 5.205052632450e-06,\n",
            "        1.995414602287e-06, 4.472160381960e-07, 8.814280289378e-08,\n",
            "        1.364565390532e-08, 3.069295688593e-04, 1.596760307645e-05,\n",
            "        4.329943553947e-06, 1.304370161930e-06, 6.109537537824e-07,\n",
            "        1.970560616125e-07, 4.429635126112e-08, 7.544480841134e-09,\n",
            "        6.619437194593e-05, 9.224801184410e-06, 1.496763274909e-06,\n",
            "        4.552104028158e-07, 1.513822291354e-07, 7.645965515321e-08,\n",
            "        2.069857335656e-08, 1.354972282082e-05, 4.859418538178e-06,\n",
            "        5.968936834927e-07, 1.614327726802e-07, 5.222050900820e-08,\n",
            "        1.861168308612e-08, 9.335617680645e-09, 5.636886587267e-06,\n",
            "        1.478587986951e-06, 3.371369697890e-07, 5.814726285750e-08,\n",
            "        1.887429478759e-08, 6.226986345288e-09, 2.513085240477e-05,\n",
            "        5.401563737061e-07, 1.472241144246e-07, 2.702802841521e-08,\n",
            "        6.852322049829e-09, 2.364301452748e-09, 7.160002382070e-07,\n",
            "        3.046035215325e-07, 9.311902054411e-08, 1.391260677326e-08,\n",
            "        2.572349013677e-09, 6.334811845146e-10, 2.189521596404e-07,\n",
            "        5.359723637762e-08, 9.664119770658e-09, 1.080490414200e-09,\n",
            "        1.848637544179e-12, 1.786501147332e-07, 3.746200379610e-08,\n",
            "        8.115351182922e-09, 6.958364509846e-15, 2.249855109777e-07,\n",
            "        2.702571104482e-08, 5.622484740554e-09, 3.263495383432e-17,\n",
            "        1.037531449916e-06, 2.085045489156e-08, 1.851690807497e-19,\n",
            "        4.828277253059e-06, 1.701255697509e-08, 8.400331964916e-22,\n",
            "        6.707662533825e-06, 4.355847155835e-24, 4.320556522004e-09,\n",
            "        2.381733822993e-26, 1.365227464077e-28], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 1.68066604393298, Const: (-2.7121209722480444e-09, -1.6723595552825543e-06, 8.112865503640585, 2.701391231793729, 0.6129028644166751, 4.8150581299163866e-05), W00: -1.448815399194403\n",
            "Epoch 10000, Loss_Tot: 1.7330001601481935, Const: (1.6725513496407274e-06, -1.6966816247521166e-06, 7.679665532085909, 2.93939894416775, 0.7022563607091874, 5.3161629816858796e-05), W00: -1.4503785955131674\n",
            "Epoch 15000, Loss_Tot: 1.695867487406676, Const: (-1.2936122195661426e-07, -1.0894111488468639e-06, 7.73375871522532, 2.752947818537262, 0.6412115208677442, 5.01505625029883e-05), W00: -1.4443583919190854\n",
            "Epoch 20000, Loss_Tot: 1.7074424622348081, Const: (0.00017079488966031953, -0.00011484694732044609, 7.861217348464765, 2.5828589044136936, 0.5839964721727153, 4.762728260944365e-05), W00: -1.4382459417159208\n",
            "\n",
            "Minimum Cons: 0.000000000005718310964371 at epoch 924\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4571062722\n",
            "W10 diff: -0.0000000170\n",
            "W01 diff: -0.0000000973\n",
            "Comp3: 8.8201440960630304\n",
            "Comp4: 3.4509897441155726\n",
            "Comp5: 0.8448374965063802\n",
            "Comp6: 0.0000578753594062\n",
            "Coeffs: tensor([9.914347334832e-01, 5.881599856904e-02, 5.040485911497e-03,\n",
            "        5.658345254475e-04, 8.799940567943e-05, 1.078876035777e-05,\n",
            "        1.326025742259e-06, 1.693321482083e-07, 2.182088203938e-08,\n",
            "        2.811934221715e-09, 5.851637891805e-02, 8.456035528927e-03,\n",
            "        1.149559549824e-03, 1.739913002822e-04, 3.228692941498e-05,\n",
            "        4.612164705496e-06, 6.079627636053e-07, 8.013982653806e-08,\n",
            "        1.063535057077e-08, 1.435871466664e-09, 6.261025504233e-03,\n",
            "        1.100699757438e-03, 2.224466044792e-04, 5.565221697660e-05,\n",
            "        1.231668332080e-05, 2.504559938638e-06, 3.518764846279e-07,\n",
            "        4.786725807118e-08, 6.520851293381e-09, 1.534710471815e-03,\n",
            "        1.851524714034e-04, 4.964361012898e-05, 2.041277334083e-05,\n",
            "        4.704560803432e-06, 1.066950160787e-06, 1.958968879227e-07,\n",
            "        2.992510215676e-08, 4.108628666048e-09, 5.546454903331e-04,\n",
            "        4.779639427611e-05, 1.262493541106e-05, 5.329081014405e-06,\n",
            "        2.073587307092e-06, 4.689878437896e-07, 9.690373649386e-08,\n",
            "        1.509356833698e-08, 3.078934806215e-04, 1.602082153942e-05,\n",
            "        4.369274271870e-06, 1.337065001713e-06, 6.368397500874e-07,\n",
            "        2.090757517793e-07, 4.728975076491e-08, 8.489973915536e-09,\n",
            "        6.534544687529e-05, 9.227044568791e-06, 1.512127033596e-06,\n",
            "        4.670775232384e-07, 1.580735027364e-07, 8.180838959920e-08,\n",
            "        2.244257421249e-08, 1.335898223106e-05, 4.870053333594e-06,\n",
            "        6.056771091754e-07, 1.658604490834e-07, 5.456952692228e-08,\n",
            "        1.985241049735e-08, 1.024828112260e-08, 5.528434497661e-06,\n",
            "        1.494354071849e-06, 3.463918782114e-07, 5.992667171572e-08,\n",
            "        1.975120785810e-08, 6.654998367760e-09, 2.546531119132e-05,\n",
            "        5.902677127059e-07, 1.697407797281e-07, 2.804381428127e-08,\n",
            "        7.203111464530e-09, 2.555696442886e-09, 4.637825937373e-07,\n",
            "        3.451228445774e-07, 1.148864473161e-07, 1.760914796385e-08,\n",
            "        2.732972571864e-09, 4.610529005385e-10, 2.532979334599e-07,\n",
            "        6.930368644397e-08, 1.284180671232e-08, 1.183699747706e-09,\n",
            "        1.337997038518e-12, 2.068932820015e-07, 4.928783191418e-08,\n",
            "        1.158073312844e-08, 4.909218813989e-15, 2.705281695391e-07,\n",
            "        3.557173342981e-08, 8.386710149649e-09, 2.319710669931e-17,\n",
            "        1.279884993117e-06, 2.822890747320e-08, 1.280074159593e-19,\n",
            "        6.055201252711e-06, 2.305731876396e-08, 5.900168368348e-22,\n",
            "        5.212113067753e-06, 2.991560953040e-24, 2.650678950182e-09,\n",
            "        1.614944984671e-26, 9.058564527567e-29], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.6507785067935696, Const: (-6.742690088490377e-07, -1.189295504522292e-06, 7.582512942097392, 2.5037909212313116, 0.5663451209212125, 4.6643903445669743e-05), W00: -1.4332112648661801\n",
            "Epoch 30000, Loss_Tot: 1.7761615854857513, Const: (-3.508498334081622e-07, -1.0155709095904797e-06, 8.300956024518438, 3.2295933770892318, 0.7891133153675629, 5.720436715760176e-05), W00: -1.4489264688157026\n",
            "Epoch 35000, Loss_Tot: 1.6637086968931407, Const: (-9.0221214072983e-06, -1.0053490073769211e-06, 7.51793663954707, 2.5230890991680557, 0.5716770276957726, 4.725769707995247e-05), W00: -1.440297294161771\n",
            "Epoch 40000, Loss_Tot: 1.6549984944223577, Const: (-4.045715833966845e-05, 2.5188104226137042e-05, 7.010028143575317, 2.43747446217609, 0.5552963244666149, 4.631030692920827e-05), W00: -1.4382628193791824\n",
            "\n",
            "Minimum Cons: 0.000000000031287917199751 at epoch 23184\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4341390603\n",
            "W10 diff: 0.0000004285\n",
            "W01 diff: -0.0000005032\n",
            "Comp3: 6.6236308548245075\n",
            "Comp4: 2.3844206513705815\n",
            "Comp5: 0.5484386344162695\n",
            "Comp6: 0.0000473383900931\n",
            "Coeffs: tensor([9.907894915294e-01, 6.214359249438e-02, 5.446761528766e-03,\n",
            "        6.416727183641e-04, 9.509536073356e-05, 1.206576490479e-05,\n",
            "        1.587473611227e-06, 2.099845556105e-07, 2.790735537820e-08,\n",
            "        3.720317977323e-09, 5.745388689480e-02, 9.036573940130e-03,\n",
            "        1.253549329137e-03, 2.000485750439e-04, 3.894882799780e-05,\n",
            "        5.547356900777e-06, 7.476132712536e-07, 1.023498523298e-07,\n",
            "        1.449464322615e-08, 2.059656697486e-09, 5.791982479994e-03,\n",
            "        1.022315459809e-03, 2.512576429410e-04, 6.579268810433e-05,\n",
            "        1.540848243342e-05, 3.137904214460e-06, 4.637455787385e-07,\n",
            "        6.632355406683e-08, 9.485401644018e-09, 1.384514784285e-03,\n",
            "        1.660922188406e-04, 5.793548933968e-05, 2.685951031207e-05,\n",
            "        6.185408893791e-06, 1.440966673442e-06, 2.754553320185e-07,\n",
            "        4.456743308658e-08, 6.380667236323e-09, 5.030763188636e-04,\n",
            "        4.242561801847e-05, 1.448776717659e-05, 7.066469085032e-06,\n",
            "        2.841457575283e-06, 6.572337137250e-07, 1.485630810220e-07,\n",
            "        2.354232313987e-08, 2.862899107797e-04, 1.498854584399e-05,\n",
            "        5.035543033799e-06, 1.773564286306e-06, 9.219526003121e-07,\n",
            "        3.125581779008e-07, 7.337955452847e-08, 1.449664038895e-08,\n",
            "        6.121525907731e-05, 9.797597679589e-06, 1.744099867785e-06,\n",
            "        6.098588413652e-07, 2.310576894822e-07, 1.330925591456e-07,\n",
            "        3.561394805277e-08, 1.223750078745e-05, 5.297964066967e-06,\n",
            "        7.211518209881e-07, 2.153503729139e-07, 7.787050320579e-08,\n",
            "        3.248855775059e-08, 1.766659930559e-08, 5.105271378591e-06,\n",
            "        1.726362926629e-06, 4.065004559233e-07, 7.653058527784e-08,\n",
            "        2.794174571728e-08, 1.082329360918e-08, 2.337663044430e-05,\n",
            "        8.781287398657e-07, 2.421468887615e-07, 3.639129459815e-08,\n",
            "        1.011675259529e-08, 4.105230749405e-09, 2.741479856886e-08,\n",
            "        6.026500832202e-07, 2.300816200323e-07, 2.936115279374e-08,\n",
            "        4.038482829867e-09, 3.912855404426e-11, 4.723110682916e-07,\n",
            "        1.836520845051e-07, 2.563270961359e-08, 1.730929563519e-09,\n",
            "        8.185818983696e-14, 3.867341635486e-07, 1.343338359309e-07,\n",
            "        3.348880537580e-08, 1.930157562269e-16, 4.214325004570e-07,\n",
            "        1.053091511580e-07, 3.384946578963e-08, 7.001751454693e-19,\n",
            "        1.977457143702e-06, 8.622843686545e-08, 2.885035457123e-21,\n",
            "        9.278652021775e-06, 7.060491163059e-08, 9.795974461381e-24,\n",
            "        9.950369192919e-08, 3.493888981520e-26, 1.099574059507e-11,\n",
            "        1.292453635155e-28, 4.802583309998e-31], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YmDSkoFbd6O",
        "outputId": "dbfe8368-d149-4cf8-c2bd-354391fb4290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.6572151158741937, Const: (5.705541755229859e-05, -3.704934656423298e-05, 7.007735442422155, 2.4369632115892825, 0.5551893291149875, 4.630557325872445e-05), W00: -1.4381665296393802\n",
            "\n",
            "Minimum Cons: 0.000000003150132023329291 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4381665296\n",
            "W10 diff: 0.0000570554\n",
            "W01 diff: -0.0000370493\n",
            "Comp3: 7.0077354424221552\n",
            "Comp4: 2.4369632115892825\n",
            "Comp5: 0.5551893291149875\n",
            "Comp6: 0.0000463055732587\n",
            "Coeffs: tensor([9.909807028743e-01, 6.056333055026e-02, 5.170909291525e-03,\n",
            "        5.907293325524e-04, 8.270973389535e-05, 1.020816899092e-05,\n",
            "        1.300411886647e-06, 1.656579651154e-07, 2.110296599189e-08,\n",
            "        2.688280912163e-09, 5.777064526825e-02, 8.800058365869e-03,\n",
            "        1.181625161954e-03, 1.834371514988e-04, 3.488581575113e-05,\n",
            "        4.701920113252e-06, 6.099132984801e-07, 7.976571175673e-08,\n",
            "        1.076172818198e-08, 1.451937022256e-09, 5.912616591234e-03,\n",
            "        9.934467683782e-04, 2.422067335393e-04, 6.013982565859e-05,\n",
            "        1.353968645587e-05, 2.687086812203e-06, 3.727346557624e-07,\n",
            "        5.028814348034e-08, 6.816001332642e-09, 1.408500546654e-03,\n",
            "        1.648966389809e-04, 5.543324445674e-05, 2.492603345187e-05,\n",
            "        5.462576174852e-06, 1.199152169793e-06, 2.159011253894e-07,\n",
            "        3.318989842211e-08, 4.539971119991e-09, 5.124979937410e-04,\n",
            "        4.182566051106e-05, 1.380820936595e-05, 6.502993958647e-06,\n",
            "        2.468816564531e-06, 5.443565403368e-07, 1.174852210589e-07,\n",
            "        1.763628682189e-08, 2.914306219199e-04, 1.496294543780e-05,\n",
            "        4.723309468271e-06, 1.612339641435e-06, 8.048046610250e-07,\n",
            "        2.553162112288e-07, 5.737623848396e-08, 1.079770501737e-08,\n",
            "        6.432643209742e-05, 9.285277831263e-06, 1.616491299895e-06,\n",
            "        5.498131317013e-07, 1.989555139790e-07, 1.092284063922e-07,\n",
            "        2.746123165336e-08, 1.082583091017e-05, 5.577955517204e-06,\n",
            "        6.621693766667e-07, 1.894286017472e-07, 6.662336749224e-08,\n",
            "        2.638171304851e-08, 1.321810531191e-08, 5.398121567423e-06,\n",
            "        1.960350632703e-06, 3.752292549486e-07, 6.617083467727e-08,\n",
            "        2.349054863590e-08, 8.648456903911e-09, 2.395889959848e-05,\n",
            "        1.138389183088e-06, 2.741137526043e-07, 3.137399724084e-08,\n",
            "        8.282892818343e-09, 3.166152583432e-09, 1.690618644977e-08,\n",
            "        8.353121491270e-07, 2.799574569393e-07, 3.472007734227e-08,\n",
            "        3.186297360772e-09, 1.868656993458e-11, 6.528839678949e-07,\n",
            "        2.382640309803e-07, 3.127067040406e-08, 1.798494470820e-09,\n",
            "        3.107467680645e-14, 5.419259645222e-07, 1.751586972499e-07,\n",
            "        4.373547650467e-08, 5.478006718926e-17, 4.779574646776e-07,\n",
            "        1.344143385901e-07, 4.176283336855e-08, 1.158764719356e-19,\n",
            "        2.114648403242e-06, 1.115705400328e-07, 3.428880531158e-22,\n",
            "        9.355906474385e-06, 9.260905874791e-08, 8.415206388180e-25,\n",
            "        2.966795313490e-08, 2.464395393233e-27, 2.930087108759e-12,\n",
            "        7.355622777394e-30, 2.195475068322e-32], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 1.7247164003350315, Const: (-1.1106917177095e-06, -1.3430229601762989e-06, 6.905175867185717, 2.639403598923955, 0.610185808304577, 5.156555028878814e-05), W00: -1.4588127653297143\n",
            "Epoch 10000, Loss_Tot: 1.6646526526364576, Const: (-7.204315652931115e-07, -1.3889442020964538e-06, 7.243978323647571, 2.5427912571597537, 0.5821430238703327, 4.743373773602454e-05), W00: -1.4396542568878248\n",
            "Epoch 15000, Loss_Tot: 11.293894755163237, Const: (0.0025455141616621546, -0.00163289534980815, 4.953189625470847, 2.050798924652451, 0.4831483014981408, 8.486449179902118e-05), W00: -1.4277069876849606\n",
            "Epoch 20000, Loss_Tot: 1.6384135257667845, Const: (-1.885050138206168e-08, -1.936856147910504e-06, 7.125904057404848, 2.4335900466904614, 0.554559962281769, 4.551043424610923e-05), W00: -1.4312898114727624\n",
            "\n",
            "Minimum Cons: 0.000000000037713941043210 at epoch 1471\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4363094196\n",
            "W10 diff: -0.0000007814\n",
            "W01 diff: -0.0000002667\n",
            "Comp3: 7.1470675022023684\n",
            "Comp4: 2.4366427281671101\n",
            "Comp5: 0.5530673321880217\n",
            "Comp6: 0.0000456792478495\n",
            "Coeffs: tensor([9.909937751573e-01, 6.041094260401e-02, 5.149471211249e-03,\n",
            "        5.868676880307e-04, 8.169265285190e-05, 1.007700881521e-05,\n",
            "        1.280860261050e-06, 1.628059218837e-07, 2.069371349270e-08,\n",
            "        2.630308201339e-09, 5.772423762260e-02, 8.770692439439e-03,\n",
            "        1.178467091884e-03, 1.824245135039e-04, 3.467762994492e-05,\n",
            "        4.651037157566e-06, 6.019505477534e-07, 7.897362360171e-08,\n",
            "        1.063732760277e-08, 1.432791481797e-09, 5.881157730074e-03,\n",
            "        9.847561688969e-04, 2.402291225959e-04, 5.996344890263e-05,\n",
            "        1.353136164265e-05, 2.678740661833e-06, 3.707110149601e-07,\n",
            "        4.993281085480e-08, 6.754756530515e-09, 1.394797732601e-03,\n",
            "        1.630121078178e-04, 5.482481282847e-05, 2.487173316175e-05,\n",
            "        5.467767922341e-06, 1.202149191629e-06, 2.161987815801e-07,\n",
            "        3.315123504997e-08, 4.527952235049e-09, 5.064123208822e-04,\n",
            "        4.121634221350e-05, 1.355144792600e-05, 6.481068643379e-06,\n",
            "        2.484471070751e-06, 5.470501805900e-07, 1.179590208493e-07,\n",
            "        1.766952731296e-08, 2.875905216832e-04, 1.467567524762e-05,\n",
            "        4.656340429587e-06, 1.591959482883e-06, 8.055941105569e-07,\n",
            "        2.576479420254e-07, 5.780799093156e-08, 1.087139406130e-08,\n",
            "        6.480442908246e-05, 9.168404146116e-06, 1.588948220124e-06,\n",
            "        5.487132708314e-07, 1.992505178839e-07, 1.099881215154e-07,\n",
            "        2.776055258040e-08, 1.077235766494e-05, 5.576070171176e-06,\n",
            "        6.548013172325e-07, 1.897942151338e-07, 6.674679278604e-08,\n",
            "        2.647563772543e-08, 1.342677039723e-08, 5.190991439848e-06,\n",
            "        1.950797460225e-06, 3.771201124176e-07, 6.633831622535e-08,\n",
            "        2.357075317465e-08, 8.682758949697e-09, 2.373644847886e-05,\n",
            "        1.152859683011e-06, 2.718042465122e-07, 3.158539744860e-08,\n",
            "        8.313746350750e-09, 3.185895763969e-09, 7.876068807733e-09,\n",
            "        8.484952459822e-07, 2.792845966847e-07, 3.386170861583e-08,\n",
            "        3.217195605831e-09, 9.754043984966e-12, 6.676599361089e-07,\n",
            "        2.397859770825e-07, 3.046878115112e-08, 1.870598990022e-09,\n",
            "        1.535277011901e-14, 5.564562500015e-07, 1.769433871190e-07,\n",
            "        4.253679176892e-08, 2.564227578975e-17, 4.767174911996e-07,\n",
            "        1.367036909924e-07, 4.194021025831e-08, 5.909879411990e-20,\n",
            "        2.107343931935e-06, 1.139346780799e-07, 1.681958822260e-22,\n",
            "        9.315550917915e-06, 9.495801284017e-08, 4.148740131100e-25,\n",
            "        5.585132116194e-09, 1.181886021396e-27, 5.567744621632e-13,\n",
            "        3.366936764972e-30, 9.591672102130e-33], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.6241798587357879, Const: (-6.47939195408398e-07, -2.1333273423351784e-06, 6.655288991841007, 2.289119236449726, 0.5193596297240494, 4.40365414587929e-05), W00: -1.4302531894598387\n",
            "Epoch 30000, Loss_Tot: 1.6465745023135654, Const: (-0.00013487082544938467, 8.790362619448544e-05, 6.396482747470529, 2.1938171316626947, 0.4962385727089734, 4.379333692911109e-05), W00: -1.4288716793193628\n",
            "Epoch 35000, Loss_Tot: 1.7165174839225001, Const: (-9.600289923028527e-07, -2.422813199620677e-06, 5.810793074250764, 2.326939879768028, 0.5384940880749, 5.174901208067818e-05), W00: -1.448714667110416\n",
            "Epoch 40000, Loss_Tot: 1.6125757160242862, Const: (1.092051078455114e-07, -2.6742425229819844e-06, 6.072958418898871, 2.1517103853888866, 0.4880679481515818, 4.2703967180234726e-05), W00: -1.4302056712324025\n",
            "\n",
            "Minimum Cons: 0.000000000035135990593339 at epoch 29234\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4263267074\n",
            "W10 diff: -0.0000006722\n",
            "W01 diff: -0.0000004850\n",
            "Comp3: 6.3288020616735725\n",
            "Comp4: 2.1690481153664658\n",
            "Comp5: 0.4911645418862397\n",
            "Comp6: 0.0000423898468000\n",
            "Coeffs: tensor([9.909786424221e-01, 5.831245027849e-02, 4.889267298079e-03,\n",
            "        5.459525523058e-04, 7.593072707354e-05, 9.321459187367e-06,\n",
            "        1.162991750703e-06, 1.451001346493e-07, 1.810334396960e-08,\n",
            "        2.258654311105e-09, 5.748642405114e-02, 8.750967027870e-03,\n",
            "        1.158325725456e-03, 1.749280456759e-04, 3.291607892181e-05,\n",
            "        4.487746789229e-06, 5.696339346754e-07, 7.366809646699e-08,\n",
            "        9.632722699769e-09, 1.270821138893e-09, 5.703666499225e-03,\n",
            "        9.540975587035e-04, 2.418818952620e-04, 6.024208301060e-05,\n",
            "        1.333964006948e-05, 2.597150479530e-06, 3.689154389894e-07,\n",
            "        4.868078105349e-08, 6.423743395211e-09, 1.313225904534e-03,\n",
            "        1.540323646400e-04, 5.426515731104e-05, 2.541478009370e-05,\n",
            "        5.616198396680e-06, 1.208046373108e-06, 2.138869438717e-07,\n",
            "        3.244855750223e-08, 4.418152339657e-09, 4.730936446912e-04,\n",
            "        3.776847782864e-05, 1.289456172509e-05, 6.744676298659e-06,\n",
            "        2.662608200681e-06, 5.671549439221e-07, 1.196524558326e-07,\n",
            "        1.768406323756e-08, 2.772577289964e-04, 1.322165457429e-05,\n",
            "        4.372001715032e-06, 1.696296704276e-06, 8.939518637113e-07,\n",
            "        2.783543735535e-07, 5.995514075858e-08, 1.119374591912e-08,\n",
            "        6.287479707099e-05, 8.509516405496e-06, 1.531947401763e-06,\n",
            "        5.902393227702e-07, 2.261391939946e-07, 1.220450112634e-07,\n",
            "        3.040949521787e-08, 1.081829773687e-05, 5.708273765410e-06,\n",
            "        6.644348473947e-07, 2.128987262080e-07, 7.672168633527e-08,\n",
            "        3.080026857889e-08, 1.582001832512e-08, 4.702724541761e-06,\n",
            "        2.153756712558e-06, 4.152004024705e-07, 7.704335521876e-08,\n",
            "        2.821722233626e-08, 1.028183663892e-08, 2.230778728978e-05,\n",
            "        1.468627752813e-06, 2.454751627146e-07, 3.768817771549e-08,\n",
            "        1.039114552266e-08, 3.952251440213e-09, 9.840051601179e-09,\n",
            "        1.210724652591e-06, 3.198412851901e-07, 2.378291075923e-08,\n",
            "        4.005741604013e-09, 7.469748308654e-12, 1.037408681475e-06,\n",
            "        3.064699554966e-07, 2.533652662752e-08, 1.736214293074e-09,\n",
            "        7.229245369600e-15, 8.889029861131e-07, 2.525247522921e-07,\n",
            "        3.779821458993e-08, 8.507648481868e-18, 8.017381095941e-07,\n",
            "        2.163756654229e-07, 5.084937638661e-08, 1.116005001209e-20,\n",
            "        1.984647858315e-06, 1.854013439914e-07, 1.862558250892e-23,\n",
            "        8.278591615765e-06, 1.588610170669e-07, 2.650675309404e-26,\n",
            "        8.276486607088e-09, 5.124776563364e-29, 3.951016788873e-13,\n",
            "        9.920129856583e-32, 1.929130555388e-34], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epoch_part = 20000\n",
        "num_epochs = 2*epoch_part\n",
        "min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "min_cons_epoch = 0\n",
        "max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "min_cons_D1 = 0\n",
        "min_cons_D2 = 0\n",
        "min_cons_D3 = 0\n",
        "min_cons_L6 = 0\n",
        "\n",
        "\n",
        "zero_vec = torch.zeros(len(NullPoints),dtype = torch.float64)\n",
        "one_vec = torch.ones(len(NullPoints),dtype = torch.float64)\n",
        "for epoch in range(num_epochs+1):\n",
        "\n",
        "    w00vals = w00full()\n",
        "    w01vals = w01full()\n",
        "    w10vals = w10full()\n",
        "\n",
        "\n",
        "    losscomp1 = (w01- w01vals)\n",
        "    losscomp2 = (w10- w10vals)\n",
        "    losscomp3 =  F.mse_loss(ampD1full(),zero_vec)\n",
        "    losscomp4 =  F.mse_loss(ampD2full(),zero_vec)\n",
        "    losscomp5 =  F.mse_loss(ampD3full(),zero_vec)\n",
        "    losscomp6 =  F.mse_loss(ampD1full()/ampfull(),zero_vec)\n",
        "    beta1 = 10**3\n",
        "    beta4 = 10**4\n",
        "\n",
        "\n",
        "    cons1 = (losscomp1**2 + losscomp2**2)\n",
        "    cons2 =  losscomp3\n",
        "    cons3 =  (losscomp4 + losscomp5)\n",
        "    cons4 = losscomp6\n",
        "    cons = (cons1**(1/2))*(cons4**(1/2))\n",
        "\n",
        "    loss = - w00vals + (beta1**2)*cons1 + (beta4**2)*cons4\n",
        "\n",
        "    if cons < min_cons:\n",
        "      if w00vals > max_w00:\n",
        "        min_cons = cons.item()\n",
        "        min_cons_epoch = epoch\n",
        "        max_w00 = w00vals.item()\n",
        "        min_cons_w10 = (losscomp1).item()\n",
        "        min_cons_w01 = (losscomp2).item()\n",
        "        min_cons_D1 = (losscomp3**(1/2)).item()\n",
        "        min_cons_D2 = (losscomp4**(1/2)).item()\n",
        "        min_cons_D3 = (losscomp5**(1/2)).item()\n",
        "        min_cons_L6 = (losscomp6**(1/2)).item()\n",
        "        min_cons_coeffs = ImPW(ell_n_tensor)\n",
        "\n",
        "    # Print loss every 5000 epochs\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss_Tot: {loss.item()}, Const: {(losscomp1).item(), (losscomp2).item(), (losscomp3**(1/2)).item(), (losscomp4**(1/2)).item(),(losscomp5**(1/2)).item(),(losscomp6**(1/2)).item()}, W00: {w00vals.item()}\")\n",
        "     # Print best solution every 100000 epochs\n",
        "    if (epoch) % (epoch_part) == 0:\n",
        "      print(f\"\\nMinimum Cons: {min_cons:.24f} at epoch {min_cons_epoch}\")\n",
        "      print(\"Values at minimum cons epoch:\")\n",
        "      print(f\"W00: {max_w00:.10f}\")\n",
        "      print(f\"W10 diff: {min_cons_w10:.10f}\")\n",
        "      print(f\"W01 diff: {min_cons_w01:.10f}\")\n",
        "      print(f\"Comp3: {min_cons_D1:.16f}\")\n",
        "      print(f\"Comp4: {min_cons_D2:.16f}\")\n",
        "      print(f\"Comp5: {min_cons_D3:.16f}\")\n",
        "      print(f\"Comp6: {min_cons_L6:.16f}\")\n",
        "      print(f\"Coeffs: {min_cons_coeffs}\")\n",
        "\n",
        "      min_cons = torch.tensor([10**4], dtype = torch.float64)  # Initialize minimum cons to positive infinity\n",
        "      min_cons_epoch = 0\n",
        "      max_w00 = -torch.tensor([10**10], dtype = torch.float64)  # Track the max w00\n",
        "      min_cons_w00 = 0  # Save w00 for minimum loss epoch\n",
        "      min_cons_w10 = 0  # Save w10 difference for minimum loss epoch\n",
        "      min_cons_w01 = 0  # Save w01 difference for minimum loss epoch\n",
        "      min_cons_D1 = 0\n",
        "      min_cons_D2 = 0\n",
        "      min_cons_D3 = 0\n",
        "      min_cons_L6 = 0\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pUkHkzqc_bd",
        "outputId": "e6c76dcf-b059-4856-fd22-5450170bb770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss_Tot: 1.6125134896286974, Const: (-1.969648344646302e-06, -1.344197457120444e-06, 6.075127663629711, 2.151925230868283, 0.48809781588118467, 4.2698390861210656e-05), W00: -1.4301925450336204\n",
            "\n",
            "Minimum Cons: 0.000000000101819158030704 at epoch 0\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4301925450\n",
            "W10 diff: -0.0000019696\n",
            "W01 diff: -0.0000013442\n",
            "Comp3: 6.0751276636297113\n",
            "Comp4: 2.1519252308682830\n",
            "Comp5: 0.4880978158811847\n",
            "Comp6: 0.0000426983908612\n",
            "Coeffs: tensor([9.912837042025e-01, 5.693513205136e-02, 4.666114757029e-03,\n",
            "        5.105432498970e-04, 6.806108410160e-05, 8.123119470860e-06,\n",
            "        9.826936189586e-07, 1.188808956445e-07, 1.394350681328e-08,\n",
            "        1.607299681898e-09, 5.797680195961e-02, 8.433485072339e-03,\n",
            "        1.103528725062e-03, 1.633001707048e-04, 3.048572445697e-05,\n",
            "        3.871535267489e-06, 4.767872522993e-07, 6.001449399152e-08,\n",
            "        7.616071402708e-09, 9.683526877160e-10, 5.870073273538e-03,\n",
            "        9.738814722497e-04, 2.285596886865e-04, 5.696080900812e-05,\n",
            "        1.241161280436e-05, 2.350995031493e-06, 3.047063191508e-07,\n",
            "        3.896766725145e-08, 4.983417867061e-09, 1.333983297765e-03,\n",
            "        1.559240288339e-04, 5.236537022697e-05, 2.337431144514e-05,\n",
            "        5.144555013307e-06, 1.111223631390e-06, 1.896014372310e-07,\n",
            "        2.641322230978e-08, 3.397267112541e-09, 4.809179828614e-04,\n",
            "        3.768097490794e-05, 1.321758885612e-05, 6.075017616793e-06,\n",
            "        2.352272329480e-06, 5.143374233649e-07, 1.038994875572e-07,\n",
            "        1.540542636789e-08, 2.856707482944e-04, 1.364842029931e-05,\n",
            "        4.452176725361e-06, 1.507703765652e-06, 7.839948847103e-07,\n",
            "        2.520228345051e-07, 5.308306548162e-08, 9.344318502929e-09,\n",
            "        6.595780810072e-05, 8.988787776073e-06, 1.530470298312e-06,\n",
            "        5.151750249969e-07, 1.926454874974e-07, 1.065806209276e-07,\n",
            "        2.705516450170e-08, 8.509977442314e-06, 5.489142648771e-06,\n",
            "        6.177841861921e-07, 1.829644234005e-07, 6.509206369168e-08,\n",
            "        2.669134121110e-08, 1.409049048140e-08, 5.360453833209e-06,\n",
            "        1.896438796945e-06, 3.636970776745e-07, 6.606266982309e-08,\n",
            "        2.369724757379e-08, 8.970983782636e-09, 2.319368565657e-05,\n",
            "        1.258268776978e-06, 1.851699088253e-07, 3.190310759924e-08,\n",
            "        8.821361985754e-09, 3.444637942958e-09, 2.067616614424e-09,\n",
            "        9.516497082061e-07, 2.165291437766e-07, 1.753636592962e-08,\n",
            "        3.545998358692e-09, 1.191315360274e-12, 7.593653646305e-07,\n",
            "        2.065385665789e-07, 1.799440048852e-08, 1.530402002556e-09,\n",
            "        8.724272025507e-16, 7.867086646451e-07, 1.629142065879e-07,\n",
            "        2.835149111377e-08, 7.945191731368e-19, 7.890732093810e-07,\n",
            "        1.387157917289e-07, 3.183049435307e-08, 8.127239651398e-22,\n",
            "        2.757315582422e-06, 1.229126890822e-07, 1.032591509969e-24,\n",
            "        7.970015301005e-06, 1.206040906246e-07, 1.127265331421e-27,\n",
            "        4.577862679122e-10, 1.704397318084e-30, 2.355660595716e-14,\n",
            "        2.580189853889e-33, 3.923641341861e-36], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 5000, Loss_Tot: 1.6177628454428699, Const: (-3.870588112242501e-07, -2.795171622826942e-06, 6.273525066965331, 2.202462113136783, 0.4998277853493326, 4.315565145900375e-05), W00: -1.4315138573588442\n",
            "Epoch 10000, Loss_Tot: 2.288047623987094, Const: (-0.0006625523114498488, 0.00043182725537294075, 6.295159889314432, 2.1577323189116115, 0.48666520535236585, 4.8174493975399535e-05), W00: -1.4305190931180491\n",
            "Epoch 15000, Loss_Tot: 1.6460834534555038, Const: (1.3992038438992438e-07, -1.3056810392875207e-06, 6.977853729594609, 2.4425762459043585, 0.5566642252415744, 4.5298449195131306e-05), W00: -1.4408867791264244\n",
            "Epoch 20000, Loss_Tot: 2.951413215493825, Const: (2.9427816847782395e-06, 4.691742540696708e-06, 21.31919064678424, 0.5926192007613645, 0.12022162588335727, 0.00012297577188330472), W00: -1.4390784960522525\n",
            "\n",
            "Minimum Cons: 0.000000000050249198345361 at epoch 7603\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4289162305\n",
            "W10 diff: -0.0000004579\n",
            "W01 diff: -0.0000011082\n",
            "Comp3: 6.1221378701378288\n",
            "Comp4: 2.1385363438451814\n",
            "Comp5: 0.4845898860675258\n",
            "Comp6: 0.0000419052701628\n",
            "Coeffs: tensor([9.912446082916e-01, 5.695911054581e-02, 4.691383825941e-03,\n",
            "        5.143061158777e-04, 6.765818352452e-05, 8.140416494310e-06,\n",
            "        9.882260265246e-07, 1.169032071328e-07, 1.362740313660e-08,\n",
            "        1.483414038104e-09, 5.788998639314e-02, 8.605778082506e-03,\n",
            "        1.123361314044e-03, 1.662848108640e-04, 3.142127351146e-05,\n",
            "        3.902059202065e-06, 4.821773332187e-07, 6.054818414122e-08,\n",
            "        7.696463431166e-09, 9.783208005503e-10, 5.841080705574e-03,\n",
            "        9.657160868079e-04, 2.398730154980e-04, 5.901021855646e-05,\n",
            "        1.279831101910e-05, 2.375957803339e-06, 3.035882423647e-07,\n",
            "        3.879098251424e-08, 4.956516559831e-09, 1.320635110030e-03,\n",
            "        1.550916176491e-04, 5.531073289364e-05, 2.518692590424e-05,\n",
            "        5.510846395621e-06, 1.151389791948e-06, 1.932366217055e-07,\n",
            "        2.600965695133e-08, 3.321107364859e-09, 4.729973438814e-04,\n",
            "        3.733659627977e-05, 1.315838432235e-05, 6.600896291673e-06,\n",
            "        2.599026568487e-06, 5.501045727399e-07, 1.095633219912e-07,\n",
            "        1.584559380057e-08, 2.807176293681e-04, 1.340874680024e-05,\n",
            "        4.413998484163e-06, 1.652259636214e-06, 8.752212630084e-07,\n",
            "        2.731230795737e-07, 5.762144878066e-08, 9.940048462964e-09,\n",
            "        6.727197007161e-05, 8.794598868137e-06, 1.544574990827e-06,\n",
            "        5.682743115692e-07, 2.168194898155e-07, 1.185992326157e-07,\n",
            "        2.950396565271e-08, 8.018697524639e-06, 5.858109829130e-06,\n",
            "        6.593545605077e-07, 2.033667244194e-07, 7.379877683581e-08,\n",
            "        3.098330785244e-08, 1.552755496774e-08, 5.301031757588e-06,\n",
            "        1.958653340207e-06, 3.943250069232e-07, 7.392741025425e-08,\n",
            "        2.713449495135e-08, 1.054841999604e-08, 2.281043907408e-05,\n",
            "        1.303006648798e-06, 1.807105261319e-07, 3.592576812545e-08,\n",
            "        1.017217562062e-08, 4.006454559918e-09, 4.907505949847e-09,\n",
            "        1.027442576770e-06, 2.000981214645e-07, 1.552456076740e-08,\n",
            "        4.170715114542e-09, 2.140897379201e-12, 1.016944971600e-06,\n",
            "        2.053079218899e-07, 1.657415384230e-08, 1.710976730040e-09,\n",
            "        1.227666200160e-15, 1.046161572457e-06, 1.625934723001e-07,\n",
            "        2.600963950794e-08, 9.355882839573e-19, 9.547898636584e-07,\n",
            "        1.350403789300e-07, 2.744312743432e-08, 8.512909063118e-22,\n",
            "        2.095372324869e-06, 1.222077651514e-07, 1.035821967873e-24,\n",
            "        8.157475516496e-06, 1.470747554480e-07, 1.049147081834e-27,\n",
            "        2.374779998051e-09, 1.510201819971e-30, 9.890854755705e-14,\n",
            "        2.173870162282e-33, 3.147157212768e-36], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Epoch 25000, Loss_Tot: 1.838656166951147, Const: (-3.424629779757993e-07, -1.2704038543365215e-06, 9.18529957370158, 3.7226503576199583, 0.9332103863113077, 6.200619010815177e-05), W00: -1.4541776745714767\n",
            "Epoch 30000, Loss_Tot: 9.828336841995194, Const: (-0.002225580852999842, 0.0014609708968496005, 8.854454090950592, 2.6976182199754595, 0.5978995370191499, 0.00011361992443482754), W00: -1.4497420244565704\n",
            "Epoch 35000, Loss_Tot: 5.645023562335567, Const: (0.0013324060465893517, -0.0008809926848996064, 17.54618773473005, 3.7274812348844004, 0.7712667690274462, 0.0001283702472360112), W00: -1.4456775409576201\n",
            "Epoch 40000, Loss_Tot: 1.6049197878870187, Const: (-3.8773486221499454e-05, 2.3757135847457533e-05, 6.214813323169293, 2.151360675454558, 0.4865210834320191, 4.067410518517253e-05), W00: -1.4374137198881276\n",
            "\n",
            "Minimum Cons: 0.000000000030051568130033 at epoch 35104\n",
            "Values at minimum cons epoch:\n",
            "W00: -1.4366565898\n",
            "W10 diff: -0.0000000685\n",
            "W01 diff: -0.0000007126\n",
            "Comp3: 6.6081447679653929\n",
            "Comp4: 2.2709231027363455\n",
            "Comp5: 0.5184856986582775\n",
            "Comp6: 0.0000419793774899\n",
            "Coeffs: tensor([9.915569677253e-01, 5.583543927026e-02, 4.571795828772e-03,\n",
            "        4.930749751940e-04, 6.967039385432e-05, 9.444206292835e-06,\n",
            "        1.247954011598e-06, 1.582173221155e-07, 1.995979502832e-08,\n",
            "        2.493144161076e-09, 5.840444146490e-02, 8.879627777992e-03,\n",
            "        1.109457050773e-03, 1.632242523182e-04, 3.145123389494e-05,\n",
            "        4.350455934148e-06, 6.029964956030e-07, 8.480689744525e-08,\n",
            "        1.144052247671e-08, 1.522053192823e-09, 6.105438245878e-03,\n",
            "        9.984207759273e-04, 2.557503357676e-04, 6.168478480268e-05,\n",
            "        1.313719339125e-05, 2.457070055452e-06, 3.455685990424e-07,\n",
            "        4.860160500326e-08, 6.835446606062e-09, 1.373173973596e-03,\n",
            "        1.630577540363e-04, 5.920766745054e-05, 2.668815457057e-05,\n",
            "        5.541083455151e-06, 1.133795464422e-06, 1.884616489125e-07,\n",
            "        2.785070422841e-08, 3.917295414019e-09, 4.756488516083e-04,\n",
            "        3.917733037995e-05, 1.404800410810e-05, 6.864352732195e-06,\n",
            "        2.532236067061e-06, 5.241942568390e-07, 1.068855435418e-07,\n",
            "        1.509153911566e-08, 2.789593124072e-04, 1.343466741594e-05,\n",
            "        4.566149510452e-06, 1.694263251575e-06, 8.211502361962e-07,\n",
            "        2.488042832391e-07, 5.202666308101e-08, 9.387036919190e-09,\n",
            "        6.982439680391e-05, 8.372013454448e-06, 1.533785898685e-06,\n",
            "        5.398770314005e-07, 2.020863528667e-07, 1.020110610385e-07,\n",
            "        2.596280711939e-08, 1.031030754351e-05, 5.625066400056e-06,\n",
            "        5.684348338339e-07, 1.771921765645e-07, 6.376507306268e-08,\n",
            "        2.561275714647e-08, 1.270804968167e-08, 5.116499133843e-06,\n",
            "        1.749098657640e-06, 3.067132062835e-07, 5.859602896397e-08,\n",
            "        2.108073686721e-08, 7.983371622097e-09, 2.258740582720e-05,\n",
            "        1.207041316996e-06, 1.486892221219e-07, 2.355183214786e-08,\n",
            "        7.069664437892e-09, 2.677906205981e-09, 1.691962390985e-07,\n",
            "        9.099728833839e-07, 1.467007279500e-07, 1.144733004505e-08,\n",
            "        2.371345114974e-09, 4.749074181729e-11, 9.511059530002e-07,\n",
            "        1.552843992015e-07, 1.061934314613e-08, 1.076032030624e-09,\n",
            "        2.892319065792e-14, 1.121684956492e-06, 1.210682350341e-07,\n",
            "        1.600240867130e-08, 2.284823266008e-17, 1.322856952378e-06,\n",
            "        1.053719278553e-07, 1.931826108005e-08, 1.940824119597e-20,\n",
            "        1.780978772698e-06, 1.061952012360e-07, 1.787536891946e-23,\n",
            "        7.867941280086e-06, 1.281867424166e-07, 1.515542490821e-26,\n",
            "        5.764446457282e-07, 1.729718297479e-29, 5.229787818354e-11,\n",
            "        2.645440396987e-32, 4.050584713651e-35], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Minimum Cons: 0.000000000035135990593339 at epoch 29234\n",
        "Values at minimum cons epoch:\n",
        "W00: -1.4263267074\n",
        "W10 diff: -0.0000006722\n",
        "W01 diff: -0.0000004850\n",
        "Comp3: 6.3288020616735725\n",
        "Comp4: 2.1690481153664658\n",
        "Comp5: 0.4911645418862397\n",
        "Comp6: 0.0000423898468000\n",
        "\n",
        "solTensor = torch.tensor([9.909786424221e-01, 5.831245027849e-02, 4.889267298079e-03,\n",
        "        5.459525523058e-04, 7.593072707354e-05, 9.321459187367e-06,\n",
        "        1.162991750703e-06, 1.451001346493e-07, 1.810334396960e-08,\n",
        "        2.258654311105e-09, 5.748642405114e-02, 8.750967027870e-03,\n",
        "        1.158325725456e-03, 1.749280456759e-04, 3.291607892181e-05,\n",
        "        4.487746789229e-06, 5.696339346754e-07, 7.366809646699e-08,\n",
        "        9.632722699769e-09, 1.270821138893e-09, 5.703666499225e-03,\n",
        "        9.540975587035e-04, 2.418818952620e-04, 6.024208301060e-05,\n",
        "        1.333964006948e-05, 2.597150479530e-06, 3.689154389894e-07,\n",
        "        4.868078105349e-08, 6.423743395211e-09, 1.313225904534e-03,\n",
        "        1.540323646400e-04, 5.426515731104e-05, 2.541478009370e-05,\n",
        "        5.616198396680e-06, 1.208046373108e-06, 2.138869438717e-07,\n",
        "        3.244855750223e-08, 4.418152339657e-09, 4.730936446912e-04,\n",
        "        3.776847782864e-05, 1.289456172509e-05, 6.744676298659e-06,\n",
        "        2.662608200681e-06, 5.671549439221e-07, 1.196524558326e-07,\n",
        "        1.768406323756e-08, 2.772577289964e-04, 1.322165457429e-05,\n",
        "        4.372001715032e-06, 1.696296704276e-06, 8.939518637113e-07,\n",
        "        2.783543735535e-07, 5.995514075858e-08, 1.119374591912e-08,\n",
        "        6.287479707099e-05, 8.509516405496e-06, 1.531947401763e-06,\n",
        "        5.902393227702e-07, 2.261391939946e-07, 1.220450112634e-07,\n",
        "        3.040949521787e-08, 1.081829773687e-05, 5.708273765410e-06,\n",
        "        6.644348473947e-07, 2.128987262080e-07, 7.672168633527e-08,\n",
        "        3.080026857889e-08, 1.582001832512e-08, 4.702724541761e-06,\n",
        "        2.153756712558e-06, 4.152004024705e-07, 7.704335521876e-08,\n",
        "        2.821722233626e-08, 1.028183663892e-08, 2.230778728978e-05,\n",
        "        1.468627752813e-06, 2.454751627146e-07, 3.768817771549e-08,\n",
        "        1.039114552266e-08, 3.952251440213e-09, 9.840051601179e-09,\n",
        "        1.210724652591e-06, 3.198412851901e-07, 2.378291075923e-08,\n",
        "        4.005741604013e-09, 7.469748308654e-12, 1.037408681475e-06,\n",
        "        3.064699554966e-07, 2.533652662752e-08, 1.736214293074e-09,\n",
        "        7.229245369600e-15, 8.889029861131e-07, 2.525247522921e-07,\n",
        "        3.779821458993e-08, 8.507648481868e-18, 8.017381095941e-07,\n",
        "        2.163756654229e-07, 5.084937638661e-08, 1.116005001209e-20,\n",
        "        1.984647858315e-06, 1.854013439914e-07, 1.862558250892e-23,\n",
        "        8.278591615765e-06, 1.588610170669e-07, 2.650675309404e-26,\n",
        "        8.276486607088e-09, 5.124776563364e-29, 3.951016788873e-13,\n",
        "        9.920129856583e-32, 1.929130555388e-34], dtype=torch.float64)\n",
        "\n",
        "def CoeffFuncSol(solTensor, ell, n):\n",
        "    solTensor = solTensor.unsqueeze(1)  # Shape [49, 1]\n",
        "    combined_tensor = torch.cat((ell_n_tensor, solTensor), dim=1)\n",
        "    mask = (combined_tensor[:, 0] == ell) & (combined_tensor[:, 1] == n)\n",
        "    # Check if there is an entry\n",
        "    if mask.any():\n",
        "        return combined_tensor[mask, 2].item()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "CoeffFuncSol(solTensor,0,1),CoeffFuncSol(solTensor,1,2),CoeffFuncSol(solTensor,2,3),CoeffFuncSol(solTensor,3,4),CoeffFuncSol(solTensor,4,5),CoeffFuncSol(solTensor,5,6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6K4SAvKcnVn",
        "outputId": "dc24b360-2ce7-45d1-b204-8b8094c4c369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9909786424221,\n",
              " 0.05748642405114,\n",
              " 0.005703666499225,\n",
              " 0.001313225904534,\n",
              " 0.0004730936446912,\n",
              " 0.0002772577289964)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lUYeH0rRhIsJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "E4OXSRB1Aa0I",
        "Cvn_WVJT9zso",
        "b1zEe6CejZc8",
        "unfpcb5i2YU5"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}